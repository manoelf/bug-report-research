{"comments": {}, "bugs": {"1630336": {"comments": [{"attachment_id": null, "creator": "jklukas@mozilla.com", "creation_time": "2020-04-15T17:25:32Z", "text": "The copy_deduplicate query for `main_v4` failed this morning with a timeout after 6 hours of running. That query is the root for all desktop ETL, so derived tables won't have new data until this is resolved.\n\nIt appears that the problem is related to [yesterday's pipeline incident](https://bugzilla.mozilla.org/show_bug.cgi?id=1630096) where we had Pub/Sub latency and we were rejecting new connections and we also had to deal with performance issues in the sinks into BigQuery.\n\nAround 16:15 UTC, we see many thousands of documents that are submitted ~12 times each. Generally, the submission_timestamp is the same per document_id, which indicates these weren't duplicate submissions from clients but rather duplicate deliveries from PubSub.\n\nThe copy_deduplicate query becomes much more expensive in the case where we have a significant number of rows with identical submission_dates as we have to window over them in order to deduplicate, which requires shuffling of entire rows.", "id": 14757288, "time": "2020-04-15T17:25:32Z", "raw_text": "The copy_deduplicate query for `main_v4` failed this morning with a timeout after 6 hours of running. That query is the root for all desktop ETL, so derived tables won't have new data until this is resolved.\n\nIt appears that the problem is related to [yesterday's pipeline incident](https://bugzilla.mozilla.org/show_bug.cgi?id=1630096) where we had Pub/Sub latency and we were rejecting new connections and we also had to deal with performance issues in the sinks into BigQuery.\n\nAround 16:15 UTC, we see many thousands of documents that are submitted ~12 times each. Generally, the submission_timestamp is the same per document_id, which indicates these weren't duplicate submissions from clients but rather duplicate deliveries from PubSub.\n\nThe copy_deduplicate query becomes much more expensive in the case where we have a significant number of rows with identical submission_dates as we have to window over them in order to deduplicate, which requires shuffling of entire rows.", "tags": [], "bug_id": 1630336, "count": 0, "is_private": false, "author": "jklukas@mozilla.com"}, {"id": 14757291, "text": "I am working around this performance issue by deduplicating at the `payload_bytes_decoded` level where the data is gzipped and smaller. That query ran in about 10 minutes and I'm now running a Dataflow batch job to load the data from the bytes form into the production stable table.", "creator": "jklukas@mozilla.com", "attachment_id": null, "creation_time": "2020-04-15T17:27:02Z", "bug_id": 1630336, "tags": [], "count": 1, "is_private": false, "author": "jklukas@mozilla.com", "time": "2020-04-15T17:27:02Z", "raw_text": "I am working around this performance issue by deduplicating at the `payload_bytes_decoded` level where the data is gzipped and smaller. That query ran in about 10 minutes and I'm now running a Dataflow batch job to load the data from the bytes form into the production stable table."}, {"author": "jklukas@mozilla.com", "bug_id": 1630336, "tags": [], "count": 2, "is_private": false, "raw_text": "`telemetry_stable.main_v4` is now properly populated for 2020-04-14. I've cleared the downstream tasks in Airflow, so main_summary is now running and other downstream tasks will follow.\n\nI expect the main_summary DAG to complete in about 2 hours, so ~4:00 PM Pacific. There may need to be some additional intervention in other DAGs that depend on main_summary. I will plan to address those in the morning.", "time": "2020-04-15T21:09:15Z", "id": 14757814, "text": "`telemetry_stable.main_v4` is now properly populated for 2020-04-14. I've cleared the downstream tasks in Airflow, so main_summary is now running and other downstream tasks will follow.\n\nI expect the main_summary DAG to complete in about 2 hours, so ~4:00 PM Pacific. There may need to be some additional intervention in other DAGs that depend on main_summary. I will plan to address those in the morning.", "creation_time": "2020-04-15T21:09:15Z", "attachment_id": null, "creator": "jklukas@mozilla.com"}, {"raw_text": "There was very little fixup I needed to do this morning, as it looks like DAGs downstream of main_summary were generally able to do the right thing and wait on main_summary tasks finishing, so they ran automatically as data was available. We're all clear here.", "time": "2020-04-16T12:47:04Z", "author": "jklukas@mozilla.com", "count": 3, "is_private": false, "tags": [], "bug_id": 1630336, "creation_time": "2020-04-16T12:47:04Z", "attachment_id": null, "creator": "jklukas@mozilla.com", "id": 14758965, "text": "There was very little fixup I needed to do this morning, as it looks like DAGs downstream of main_summary were generally able to do the right thing and wait on main_summary tasks finishing, so they ran automatically as data was available. We're all clear here."}, {"count": 4, "is_private": false, "bug_id": 1630336, "tags": [], "author": "jklukas@mozilla.com", "text": "The Dataflow steps are recorded in https://github.com/mozilla/bigquery-backfill/pull/9", "id": 14758966, "attachment_id": null, "creator": "jklukas@mozilla.com", "time": "2020-04-16T12:47:39Z", "raw_text": "The Dataflow steps are recorded in https://github.com/mozilla/bigquery-backfill/pull/9", "creation_time": "2020-04-16T12:47:39Z"}]}}}