{"comments": {}, "bugs": {"681967": {"comments": [{"id": 5677854, "raw_text": "As the mobile web is becoming larger and larger, we must rethink how we handle requests to multiple needed assets on a page. The common way to do it today is to combine many images into large sprite sheets and serve them as one file. This is problematic in several ways - it's a pain to maintain and work with, consumes much more memory on the client and hurts performance in many cases (background images are way slower than <img>'s on i.e. WebKit).\n\nThe only viable alternative we could come up with is a web package format. A way to zip multiple files into a single file, transfer them to the client, and reuse them.\n\nOne proposal is to call it WebPF, use tar files as the underlying format, serve a manifest.json on the top level to handle delta downloads of the same package to the client, and add a virtual file system on the client to deal with the locally unpacked files. This could be done either in the specific way:\n\nwindow.loadPackage('package.webpf', function() {\n    var img = new Image();\n    img.src = \"package.webpf/myImage.png\";\n})\n\nor alternatively, through a generic local file system (this would be awesome, would allow us to to fancy things with Canvas and the File APIs):\n\nwindow.loadPackage('package.webpf', function(files) {\n    files[0].saveTo('myImage.png');\n    var img = new Image();\n    img.src = \"local://<absolute path of url of site>/myImage.png\";\n})\n\nI realize this is a big deal, but we need to start working on it (if someone hasn't already). There's certainly plenty of security and design concerns, keep them coming :)\n\nThanks!", "attachment_id": null, "text": "As the mobile web is becoming larger and larger, we must rethink how we handle requests to multiple needed assets on a page. The common way to do it today is to combine many images into large sprite sheets and serve them as one file. This is problematic in several ways - it's a pain to maintain and work with, consumes much more memory on the client and hurts performance in many cases (background images are way slower than <img>'s on i.e. WebKit).\n\nThe only viable alternative we could come up with is a web package format. A way to zip multiple files into a single file, transfer them to the client, and reuse them.\n\nOne proposal is to call it WebPF, use tar files as the underlying format, serve a manifest.json on the top level to handle delta downloads of the same package to the client, and add a virtual file system on the client to deal with the locally unpacked files. This could be done either in the specific way:\n\nwindow.loadPackage('package.webpf', function() {\n    var img = new Image();\n    img.src = \"package.webpf/myImage.png\";\n})\n\nor alternatively, through a generic local file system (this would be awesome, would allow us to to fancy things with Canvas and the File APIs):\n\nwindow.loadPackage('package.webpf', function(files) {\n    files[0].saveTo('myImage.png');\n    var img = new Image();\n    img.src = \"local://<absolute path of url of site>/myImage.png\";\n})\n\nI realize this is a big deal, but we need to start working on it (if someone hasn't already). There's certainly plenty of security and design concerns, keep them coming :)\n\nThanks!", "time": "2011-08-25T15:47:21Z", "bug_id": 681967, "tags": [], "creation_time": "2011-08-25T15:47:21Z", "author": "pbakaus@zynga.com", "count": 0, "is_private": false, "creator": "pbakaus@zynga.com"}, {"raw_text": "There have been existing proposals for this in the past, including some implementation work, based on zip instead of tar.  Worth looking it up.", "attachment_id": null, "id": 5677868, "count": 1, "is_private": false, "creator": "bzbarsky@mit.edu", "text": "There have been existing proposals for this in the past, including some implementation work, based on zip instead of tar.  Worth looking it up.", "bug_id": 681967, "time": "2011-08-25T15:52:28Z", "tags": [], "creation_time": "2011-08-25T15:52:28Z", "author": "bzbarsky@mit.edu"}, {"attachment_id": null, "raw_text": "Definitely, I had a feeling people had similar ideas already. If you happen to have any links, post them here. We need to make this happen!", "id": 5677905, "creator": "pbakaus@zynga.com", "is_private": false, "count": 2, "author": "pbakaus@zynga.com", "creation_time": "2011-08-25T16:09:50Z", "bug_id": 681967, "tags": [], "time": "2011-08-25T16:09:50Z", "text": "Definitely, I had a feeling people had similar ideas already. If you happen to have any links, post them here. We need to make this happen!"}, {"is_private": false, "creator": "monteslu@gmail.com", "count": 3, "creation_time": "2011-08-25T16:26:09Z", "author": "monteslu@gmail.com", "text": "Is there anything really stopping us from writing an API to do this now?  Seems like we could pull down a zip read the contents, then base64 encode things into dataURLs.  At least for images.  Text files like css and other javascript resources could be inserted or evaled.\n\nI don't know how useful this would be, but it's a javascript zip parser:\nhttp://cheeso.members.winisp.net/Unzip-Example.htm\n\nThe new binary ajax transfers seem like they might be helpful as well", "bug_id": 681967, "tags": [], "time": "2011-08-25T16:26:09Z", "attachment_id": null, "raw_text": "Is there anything really stopping us from writing an API to do this now?  Seems like we could pull down a zip read the contents, then base64 encode things into dataURLs.  At least for images.  Text files like css and other javascript resources could be inserted or evaled.\n\nI don't know how useful this would be, but it's a javascript zip parser:\nhttp://cheeso.members.winisp.net/Unzip-Example.htm\n\nThe new binary ajax transfers seem like they might be helpful as well", "id": 5677972}, {"id": 5678150, "attachment_id": null, "raw_text": "I worked on this about a year ago.  For reference, see [1, 2].\n\nWe ultimately decided to scrap resource packages in favor of HTTP pipelining and spdy, each of which gets you most of the speedup you'd get with packaging but doesn't require changes to web content.\n\n[1] http://limi.net/articles/resource-packages/\n[2] http://people.mozilla.com/~jlebar/respkg/", "creation_time": "2011-08-25T17:23:30Z", "author": "justin.lebar+bug@gmail.com", "text": "I worked on this about a year ago.  For reference, see [1, 2].\n\nWe ultimately decided to scrap resource packages in favor of HTTP pipelining and spdy, each of which gets you most of the speedup you'd get with packaging but doesn't require changes to web content.\n\n[1] http://limi.net/articles/resource-packages/\n[2] http://people.mozilla.com/~jlebar/respkg/", "tags": [], "time": "2011-08-25T17:23:30Z", "bug_id": 681967, "is_private": false, "creator": "justin.lebar+bug@gmail.com", "count": 4}, {"bug_id": 681967, "count": 5, "creation_time": "2011-08-25T19:09:05Z", "author": "cjones.bugs@gmail.com", "text": "Sure, but worth pointing out that HTTP pipelining and SPDY require server-side changes, which might be more of an obstacle than content changes for some sites.\n\nIt looks to me like there are a few separable goals here\n (1) Tar/zip an app that comprises multiple files into one file, to optimize network traffic: many requests --> one request, in theory.\n (2) Serve a (separate?) manifest of the files in the app tar, to allow delta updates of the package cached by the browser.\n (3) Allow content to directly map local packages as a kind of fs.  Reminiscent of jar:// and chrome:// protocols.\n\n (1)\nIf the server-side support is present, then (1) could be satisfied by pipelining or SPDY, but *only if* the browser knows all the files an app wants to request.  For the use case of wanting to stuff an image in a package, but only dynamically load it later to use as say a canvas sprite (i.e. not insert into the main DOM), then it's not clear pipelining/SPDY could help.  However, if all an app's files were declared in a cache or manifest, then the UA would which all requests to make in parallel.\n\n(There's a (1.1) here which is optimizing bandwidth usage, but I think that's worth discussing here.  The most interesting question is whether better compression could be achieved with a .tar or compressing all files individually.  There's also a (1.2) which is optimizing disk usage for locally-cached sites, e.g. by bzip'ing a .tar package stored on disk.  This seems pretty simple, except wrt (2) below.)\n\n (2)\nThere's already the app cache, and there are various app-manifest proposals flying around.  Delta-updating feels like it's pretty well in hand.\n\n (3)\nFor reading files, with app-cache, a site should be able to address component files using http:// or https:// and expect them to come off of disk if at all possible.  So I'm not sure a new local:// schema is necessary.  *Writing* files back sounds really scary.  If foo.png is in the app/cache manifest, and foo.html writes to foo.png locally, what's the UA supposed to do next time foo.html is loaded?  Grab the original version off the server or use the local copy?  This seems like a use case better satisfied by FileWriter or IndexedDB.\n\n\nTo me, it appears the biggest question is what to do about (1) above.  If we have the right server support, we could leverage app/cache manifests to optimize traffic.  But for the cases when server support is absent, is it worth reviving one of the packaging proposals?", "time": "2011-08-25T19:09:05Z", "tags": [], "is_private": false, "creator": "cjones.bugs@gmail.com", "id": 5678489, "attachment_id": null, "raw_text": "Sure, but worth pointing out that HTTP pipelining and SPDY require server-side changes, which might be more of an obstacle than content changes for some sites.\n\nIt looks to me like there are a few separable goals here\n (1) Tar/zip an app that comprises multiple files into one file, to optimize network traffic: many requests --> one request, in theory.\n (2) Serve a (separate?) manifest of the files in the app tar, to allow delta updates of the package cached by the browser.\n (3) Allow content to directly map local packages as a kind of fs.  Reminiscent of jar:// and chrome:// protocols.\n\n (1)\nIf the server-side support is present, then (1) could be satisfied by pipelining or SPDY, but *only if* the browser knows all the files an app wants to request.  For the use case of wanting to stuff an image in a package, but only dynamically load it later to use as say a canvas sprite (i.e. not insert into the main DOM), then it's not clear pipelining/SPDY could help.  However, if all an app's files were declared in a cache or manifest, then the UA would which all requests to make in parallel.\n\n(There's a (1.1) here which is optimizing bandwidth usage, but I think that's worth discussing here.  The most interesting question is whether better compression could be achieved with a .tar or compressing all files individually.  There's also a (1.2) which is optimizing disk usage for locally-cached sites, e.g. by bzip'ing a .tar package stored on disk.  This seems pretty simple, except wrt (2) below.)\n\n (2)\nThere's already the app cache, and there are various app-manifest proposals flying around.  Delta-updating feels like it's pretty well in hand.\n\n (3)\nFor reading files, with app-cache, a site should be able to address component files using http:// or https:// and expect them to come off of disk if at all possible.  So I'm not sure a new local:// schema is necessary.  *Writing* files back sounds really scary.  If foo.png is in the app/cache manifest, and foo.html writes to foo.png locally, what's the UA supposed to do next time foo.html is loaded?  Grab the original version off the server or use the local copy?  This seems like a use case better satisfied by FileWriter or IndexedDB.\n\n\nTo me, it appears the biggest question is what to do about (1) above.  If we have the right server support, we could leverage app/cache manifests to optimize traffic.  But for the cases when server support is absent, is it worth reviving one of the packaging proposals?"}, {"id": 5678497, "raw_text": "(In reply to Chris Jones [:cjones] [:warhammer] from comment #5)\n> (There's a (1.1) here which is optimizing bandwidth usage, but I think\n> that's worth discussing here.\n\nSorry, *don't* think that's worth discussing here.", "attachment_id": null, "text": "(In reply to Chris Jones [:cjones] [:warhammer] from comment #5)\n> (There's a (1.1) here which is optimizing bandwidth usage, but I think\n> that's worth discussing here.\n\nSorry, *don't* think that's worth discussing here.", "time": "2011-08-25T19:11:25Z", "tags": [], "creation_time": "2011-08-25T19:11:25Z", "author": "cjones.bugs@gmail.com", "is_private": false, "creator": "cjones.bugs@gmail.com", "bug_id": 681967, "count": 6}, {"id": 5678535, "raw_text": "> Sure, but worth pointing out that HTTP pipelining and SPDY require server-side changes\n\nSPDY does, but pipelining is part of the HTTP 1.1 spec.  My understanding is that it's handled well by most servers; the problem is proxies in the way.\n\nSPDY lets the page give the browser a list of resources to prefetch, addressing (1).\n\nThe theory I used when deciding to drop resource packages is:\n\n * both RP and pipelining are large changes\n\n * RP has the potential to speed up sites which opt in by providing a package\n\n * pipelining has the potential to speed up sites which run a compatible webserver (most) without any changes on the site's end\n\n * the potential speedups are on the same order (RP can theoretically download a whole page in two requests, which is better than pipelining, but in practice, you're not going to put your whole site in a package)\n\n * SPDY subsumes almost all the advantages of resource packages over pipelining.\n\nNote that RP requires back-end changes for most users, though perhaps only at the level above the HTTP server -- you're not going to want to keep the packages up-to-date by hand; you'll want software to do that for you.", "attachment_id": null, "text": "> Sure, but worth pointing out that HTTP pipelining and SPDY require server-side changes\n\nSPDY does, but pipelining is part of the HTTP 1.1 spec.  My understanding is that it's handled well by most servers; the problem is proxies in the way.\n\nSPDY lets the page give the browser a list of resources to prefetch, addressing (1).\n\nThe theory I used when deciding to drop resource packages is:\n\n * both RP and pipelining are large changes\n\n * RP has the potential to speed up sites which opt in by providing a package\n\n * pipelining has the potential to speed up sites which run a compatible webserver (most) without any changes on the site's end\n\n * the potential speedups are on the same order (RP can theoretically download a whole page in two requests, which is better than pipelining, but in practice, you're not going to put your whole site in a package)\n\n * SPDY subsumes almost all the advantages of resource packages over pipelining.\n\nNote that RP requires back-end changes for most users, though perhaps only at the level above the HTTP server -- you're not going to want to keep the packages up-to-date by hand; you'll want software to do that for you.", "time": "2011-08-25T19:30:13Z", "tags": [], "bug_id": 681967, "creation_time": "2011-08-25T19:30:13Z", "author": "justin.lebar+bug@gmail.com", "count": 7, "is_private": false, "creator": "justin.lebar+bug@gmail.com"}, {"author": "cjones.bugs@gmail.com", "creation_time": "2011-08-25T19:42:52Z", "tags": [], "time": "2011-08-25T19:42:52Z", "text": "(In reply to Justin Lebar [:jlebar] from comment #7)\n> > Sure, but worth pointing out that HTTP pipelining and SPDY require server-side changes\n> \n> SPDY does, but pipelining is part of the HTTP 1.1 spec.  My understanding is\n> that it's handled well by most servers; the problem is proxies in the way.\n> \n\nI confess to not knowing a lot about server-side support for pipelining.\n\n> SPDY lets the page give the browser a list of resources to prefetch,\n> addressing (1).\n\nGreat, another manifest :/.\n\n> Note that RP requires back-end changes for most users, though perhaps only\n> at the level above the HTTP server -- you're not going to want to keep the\n> packages up-to-date by hand; you'll want software to do that for you.\n\nAgreed.  My concern there was about having to change server SW or configuration; changing the way site content is published seems simpler in the sense that an upload script or ten would need to be changed, rather than new possibly ungrantable permissions being granted to site admins.\n\nBased on your analysis, how does this approach sound to all interested parties ---\n\n - We find a manifest to Rule Them All.  It should subsume capabilities of SPDY manifest and probably app-cache.  I wonder if the manifest work for Open Web Apps is getting towards here.  If a manifest to rule them all isn't feasible, we should start understanding why not.  Do we need manifest references, so that several pages on the same domain can share a core manifest, but with each page having its own resources?\n\n - When fetching a page, we choose SPDY if it's available.  We use the Grand Unified Manifest as the SPDY prefetch hint thing.\n\n - If SPDY isn't available, we try HTTP pipelining.  We use the GUM as the list of parallel requests to make.\n\nThis leaves servers and/or client networks that can't support SPDY or pipelining in the cold.  It would be interesting to know what % of our userbase this is.", "creator": "cjones.bugs@gmail.com", "is_private": false, "id": 5678559, "attachment_id": null, "raw_text": "(In reply to Justin Lebar [:jlebar] from comment #7)\n> > Sure, but worth pointing out that HTTP pipelining and SPDY require server-side changes\n> \n> SPDY does, but pipelining is part of the HTTP 1.1 spec.  My understanding is\n> that it's handled well by most servers; the problem is proxies in the way.\n> \n\nI confess to not knowing a lot about server-side support for pipelining.\n\n> SPDY lets the page give the browser a list of resources to prefetch,\n> addressing (1).\n\nGreat, another manifest :/.\n\n> Note that RP requires back-end changes for most users, though perhaps only\n> at the level above the HTTP server -- you're not going to want to keep the\n> packages up-to-date by hand; you'll want software to do that for you.\n\nAgreed.  My concern there was about having to change server SW or configuration; changing the way site content is published seems simpler in the sense that an upload script or ten would need to be changed, rather than new possibly ungrantable permissions being granted to site admins.\n\nBased on your analysis, how does this approach sound to all interested parties ---\n\n - We find a manifest to Rule Them All.  It should subsume capabilities of SPDY manifest and probably app-cache.  I wonder if the manifest work for Open Web Apps is getting towards here.  If a manifest to rule them all isn't feasible, we should start understanding why not.  Do we need manifest references, so that several pages on the same domain can share a core manifest, but with each page having its own resources?\n\n - When fetching a page, we choose SPDY if it's available.  We use the Grand Unified Manifest as the SPDY prefetch hint thing.\n\n - If SPDY isn't available, we try HTTP pipelining.  We use the GUM as the list of parallel requests to make.\n\nThis leaves servers and/or client networks that can't support SPDY or pipelining in the cold.  It would be interesting to know what % of our userbase this is.", "bug_id": 681967, "count": 8}, {"bug_id": 681967, "count": 9, "author": "pbakaus@zynga.com", "creation_time": "2011-08-26T08:21:34Z", "time": "2011-08-26T08:21:34Z", "tags": [], "text": "This is starting to become a great discussion and I have to admit I'm not very knowledgeable about pipelining, SPDY and protocols in general. I am knowledgeable about load and runtime performance of web apps though, so anything that satisfies the following will work for me:\n\n1) greatly reduced http round trips for 500+ images that need to be loaded\n2) no base64, sprite sheet or canvas slicing hacks to split the images again\n3) delta updates\n\nAt Zynga, we will implement any server or client site change to make this happen. But we should still try to design it so indie devs and individuals can implement it easily on their own.", "creator": "pbakaus@zynga.com", "is_private": false, "id": 5680035, "attachment_id": null, "raw_text": "This is starting to become a great discussion and I have to admit I'm not very knowledgeable about pipelining, SPDY and protocols in general. I am knowledgeable about load and runtime performance of web apps though, so anything that satisfies the following will work for me:\n\n1) greatly reduced http round trips for 500+ images that need to be loaded\n2) no base64, sprite sheet or canvas slicing hacks to split the images again\n3) delta updates\n\nAt Zynga, we will implement any server or client site change to make this happen. But we should still try to design it so indie devs and individuals can implement it easily on their own."}, {"count": 10, "creator": "mice@mice.sk", "is_private": false, "tags": [], "bug_id": 681967, "time": "2011-08-26T16:57:38Z", "text": "If we are talking about web applications, it might be worth considering the Web Storage draft [1] as a container for any content that we don't need to pull regularly.\n\nThis covers all 3 points that Paul mentioned above:\n1.) no need for http round trips, content would be stored in localStorage - CSS and JS files normally, images as dataURIs\n2.) no need for sprites, the only caveat is base64 for working with images /but we can prepare them on the server to reduce load on the user agent/\n3.) certainly achievable /it might be worthwhile to agree on structure of keys in key-value pairs, e.g. in order to accommodate for JS/CSS revision numbers, this can be however app-specific/\n\nRegarding ZIP packages - can we use the existing recommendation for Widget Packaging [2]? If we could use it as means of transporting the data in a single package, we could rely on existing support for widgets in some browsers. /However please note that I'm not familiar with this recommendation myself yet, so I'm not sure if there is an API which would allow us to access widget package contents in the context that we'd need for what we're discussing here./\n\n[1] http://www.w3.org/TR/webstorage/\n[2] http://www.w3.org/TR/widgets/", "author": "mice@mice.sk", "creation_time": "2011-08-26T16:57:38Z", "raw_text": "If we are talking about web applications, it might be worth considering the Web Storage draft [1] as a container for any content that we don't need to pull regularly.\n\nThis covers all 3 points that Paul mentioned above:\n1.) no need for http round trips, content would be stored in localStorage - CSS and JS files normally, images as dataURIs\n2.) no need for sprites, the only caveat is base64 for working with images /but we can prepare them on the server to reduce load on the user agent/\n3.) certainly achievable /it might be worthwhile to agree on structure of keys in key-value pairs, e.g. in order to accommodate for JS/CSS revision numbers, this can be however app-specific/\n\nRegarding ZIP packages - can we use the existing recommendation for Widget Packaging [2]? If we could use it as means of transporting the data in a single package, we could rely on existing support for widgets in some browsers. /However please note that I'm not familiar with this recommendation myself yet, so I'm not sure if there is an API which would allow us to access widget package contents in the context that we'd need for what we're discussing here./\n\n[1] http://www.w3.org/TR/webstorage/\n[2] http://www.w3.org/TR/widgets/", "attachment_id": null, "id": 5680925}, {"bug_id": 681967, "count": 11, "creation_time": "2012-10-15T09:08:52Z", "author": "pbakaus@zynga.com", "text": "The ticket at https://bugzilla.mozilla.org/show_bug.cgi?id=772434 seems to be a follow-up on this one (whether they knew about it or not). Unarchiving on the client seems like a great feature on its own, but I wonder if it solves the use-case of referring to a certain unpacked file in your CSS.", "time": "2012-10-15T09:08:52Z", "tags": [], "is_private": false, "creator": "pbakaus@zynga.com", "id": 6728097, "attachment_id": null, "raw_text": "The ticket at https://bugzilla.mozilla.org/show_bug.cgi?id=772434 seems to be a follow-up on this one (whether they knew about it or not). Unarchiving on the client seems like a great feature on its own, but I wonder if it solves the use-case of referring to a certain unpacked file in your CSS."}, {"bug_id": 681967, "count": 12, "author": "jonas@sicking.cc", "creation_time": "2012-10-15T09:49:35Z", "time": "2012-10-15T09:49:35Z", "tags": [], "text": "I think they are fairly independent, though bug 772434 does give you the ability to work around the fact that the web still doesn't have a good packaging solution.\n\nSPDY is now implemented in Chrome and in Firefox and is looking to get standardized as HTTP 2.0 (possibly with some modifications).\n\nOne advantage that SPDY has over a packaging format is that SPDY allows downloading resources in an arbitrary order. One problem that we ran in to with resource packages was that it was hard to ensure that it wouldn't produce slowdowns in some situations.\n\nWhen downloading a .zip file with 500+ images in it, you are effectively downloading the images in the order determined in the .zip file. So if you have a page which happens to refer to the 500th image, you're going to see reduced performance.\n\nObviously this can be worked around by being smart about which order you put images in in the .zip file. But it's easy to mess up, and it can be hard if you have multiple pages all referring to the same .zip file, but using different images from it.\n\nSince SPDY works on a protocol level this isn't a problem at all. Files can be downloaded in whichever order they are needed.\n\nAnother nice thing with SPDY, as Justin has pointed out, is that it doesn't require content changes. I.e. \"all\" you need to do to get the basic speedups is to deploy a webserver which supports SPDY. No need to rewrite any of HTML/JS logic.\n\nWith all that said, I definitely think that we should try to look at finding a packaging format for the web. The point of the above is that it's not an easy task. In the meantime I'd recommend that people try out SPDY since it could help quite a bit.", "creator": "jonas@sicking.cc", "is_private": false, "id": 6728149, "attachment_id": null, "raw_text": "I think they are fairly independent, though bug 772434 does give you the ability to work around the fact that the web still doesn't have a good packaging solution.\n\nSPDY is now implemented in Chrome and in Firefox and is looking to get standardized as HTTP 2.0 (possibly with some modifications).\n\nOne advantage that SPDY has over a packaging format is that SPDY allows downloading resources in an arbitrary order. One problem that we ran in to with resource packages was that it was hard to ensure that it wouldn't produce slowdowns in some situations.\n\nWhen downloading a .zip file with 500+ images in it, you are effectively downloading the images in the order determined in the .zip file. So if you have a page which happens to refer to the 500th image, you're going to see reduced performance.\n\nObviously this can be worked around by being smart about which order you put images in in the .zip file. But it's easy to mess up, and it can be hard if you have multiple pages all referring to the same .zip file, but using different images from it.\n\nSince SPDY works on a protocol level this isn't a problem at all. Files can be downloaded in whichever order they are needed.\n\nAnother nice thing with SPDY, as Justin has pointed out, is that it doesn't require content changes. I.e. \"all\" you need to do to get the basic speedups is to deploy a webserver which supports SPDY. No need to rewrite any of HTML/JS logic.\n\nWith all that said, I definitely think that we should try to look at finding a packaging format for the web. The point of the above is that it's not an easy task. In the meantime I'd recommend that people try out SPDY since it could help quite a bit."}]}}}