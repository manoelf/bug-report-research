{"comments": {}, "bugs": {"959925": {"comments": [{"tags": [], "id": 8291161, "time": "2014-01-15T04:28:03Z", "is_private": false, "raw_text": "Decoding and painting an image in pdf.js involves lots of copies of the data.\n\n(a) pdf.worker.js is a JS worker. It first decodes the colour data from the PDF\n    into an RGB array (24 bits per pixel), and the opacity data into an A array\n    (8 bits per pixel). These two combined are effectively copy 1.\n\n(b) pdf.worker.js combines the RGB and A arrays into a new RGBA typed array,\n    which has 32 bits per pixel. This is copy 2. Copy 1 is now dead, and will\n    be reclaimed by the next GC.\n\n(c) pdf.worker.js transfers copy 2 to the main pdf.js thread. (This is a\n    transfer, not a copy, possible because it's a typed array.)\n\n(d) pdf.js does |d = createImageData()|, which ends up in\n    mozilla::dom::CreateImageData(), which allocates a Uint8ClampedArray. This\n    is copy 3. pdf.js copies the typed arrays (copy 2) into the ImageData (copy\n    3). Copy 2 is now dead, and will be reclaimed by the next GC.\n\n(e) pdf.js does |putImageData(d, 0, 0)|, which ends up in\n    mozilla::dom::CanvasRenderingContext2D::PutImageData_explicit.\n    \n(f) PutImageData_explicit() creates a new gfxImageSurface() and copies in the\n    data. This is copy 4, and it is short-lived.\n\n(g) PutImageData_explicit() then calls CreateSourceSurfaceFromData(), creating\n    a cairo surface. This is copy 5, and it is short-lived. Copy 3 is now dead,\n    and will be reclaimed by the next GC(?)\n\nEach copy can easily be multiple MB, and in the worst case (which is probably\ncommon) we have five copies of the data alive at once.\n\nSo, ways to improve it.\n\n- In steps (d) and (e) we can split the image up into smaller chunks, and then\n  create/put those chunks one at a time. This will make copies 3, 4 and 5 much\n  smaller.\n\n- In steps (a) and (b) it is possible to decode the data directly into the RGBA\n  array, which would merge copies 1 and 2. However, it's not easy, due to the\n  various kinds of colour encodings and the possibility of images being\n  resized.\n\nIf we did both of those, we end up with one full-size copy, and three copies of\neach chunk, but two of the copies of each chunk would be very short-lived.", "bug_id": 959925, "author": "n.nethercote@gmail.com", "attachment_id": null, "count": 0, "text": "Decoding and painting an image in pdf.js involves lots of copies of the data.\n\n(a) pdf.worker.js is a JS worker. It first decodes the colour data from the PDF\n    into an RGB array (24 bits per pixel), and the opacity data into an A array\n    (8 bits per pixel). These two combined are effectively copy 1.\n\n(b) pdf.worker.js combines the RGB and A arrays into a new RGBA typed array,\n    which has 32 bits per pixel. This is copy 2. Copy 1 is now dead, and will\n    be reclaimed by the next GC.\n\n(c) pdf.worker.js transfers copy 2 to the main pdf.js thread. (This is a\n    transfer, not a copy, possible because it's a typed array.)\n\n(d) pdf.js does |d = createImageData()|, which ends up in\n    mozilla::dom::CreateImageData(), which allocates a Uint8ClampedArray. This\n    is copy 3. pdf.js copies the typed arrays (copy 2) into the ImageData (copy\n    3). Copy 2 is now dead, and will be reclaimed by the next GC.\n\n(e) pdf.js does |putImageData(d, 0, 0)|, which ends up in\n    mozilla::dom::CanvasRenderingContext2D::PutImageData_explicit.\n    \n(f) PutImageData_explicit() creates a new gfxImageSurface() and copies in the\n    data. This is copy 4, and it is short-lived.\n\n(g) PutImageData_explicit() then calls CreateSourceSurfaceFromData(), creating\n    a cairo surface. This is copy 5, and it is short-lived. Copy 3 is now dead,\n    and will be reclaimed by the next GC(?)\n\nEach copy can easily be multiple MB, and in the worst case (which is probably\ncommon) we have five copies of the data alive at once.\n\nSo, ways to improve it.\n\n- In steps (d) and (e) we can split the image up into smaller chunks, and then\n  create/put those chunks one at a time. This will make copies 3, 4 and 5 much\n  smaller.\n\n- In steps (a) and (b) it is possible to decode the data directly into the RGBA\n  array, which would merge copies 1 and 2. However, it's not easy, due to the\n  various kinds of colour encodings and the possibility of images being\n  resized.\n\nIf we did both of those, we end up with one full-size copy, and three copies of\neach chunk, but two of the copies of each chunk would be very short-lived.", "creation_time": "2014-01-15T04:28:03Z", "creator": "n.nethercote@gmail.com"}, {"tags": [], "raw_text": "Hmm, typically you get bad memory consumption when scrolling quickly through a scanned document, i.e. when we're dealing with many pages in quick succession. No individual page is a problem, it's the handling of many pages that adds up; in particular, the longer-lived copies add up quickly. So chunking actually won't help much -- copies 4 and 5 are so short-lived they don't matter much, and copy 3 just gets split up into lots of smaller pieces, all of which hang around just as long as a single larger array would. (And in some ways they're worse.)\n\nHowever, since an ImageData is basically just a wrapper around a Uint8ClampedArray, I wonder if it's possible to avoid making copy 3 by making copy 2 an ImageData object. Is an ImageData transferable from a worker? Alternatively, can you construct an ImageData from an existing Uint8ClampedArray?", "bug_id": 959925, "id": 8291239, "time": "2014-01-15T05:01:14Z", "is_private": false, "creation_time": "2014-01-15T05:01:14Z", "author": "n.nethercote@gmail.com", "text": "Hmm, typically you get bad memory consumption when scrolling quickly through a scanned document, i.e. when we're dealing with many pages in quick succession. No individual page is a problem, it's the handling of many pages that adds up; in particular, the longer-lived copies add up quickly. So chunking actually won't help much -- copies 4 and 5 are so short-lived they don't matter much, and copy 3 just gets split up into lots of smaller pieces, all of which hang around just as long as a single larger array would. (And in some ways they're worse.)\n\nHowever, since an ImageData is basically just a wrapper around a Uint8ClampedArray, I wonder if it's possible to avoid making copy 3 by making copy 2 an ImageData object. Is an ImageData transferable from a worker? Alternatively, can you construct an ImageData from an existing Uint8ClampedArray?", "attachment_id": null, "count": 1, "creator": "n.nethercote@gmail.com"}, {"creator": "n.nethercote@gmail.com", "creation_time": "2014-01-15T05:12:47Z", "attachment_id": null, "count": 2, "text": "> copy 3 just gets split up into lots of smaller pieces\n\nActually, that's not right, because a single ImageData object can be reused for each piece.\n\n> Is an ImageData transferable from a\n> worker? Alternatively, can you construct an ImageData from an existing\n> Uint8ClampedArray?\n\nAFAICT, the answer is \"no\" in both cases :(", "author": "n.nethercote@gmail.com", "raw_text": "> copy 3 just gets split up into lots of smaller pieces\n\nActually, that's not right, because a single ImageData object can be reused for each piece.\n\n> Is an ImageData transferable from a\n> worker? Alternatively, can you construct an ImageData from an existing\n> Uint8ClampedArray?\n\nAFAICT, the answer is \"no\" in both cases :(", "bug_id": 959925, "time": "2014-01-15T05:12:47Z", "id": 8291254, "is_private": false, "tags": []}, {"creation_time": "2014-01-15T06:50:05Z", "tags": [], "text": "(In reply to Nicholas Nethercote [:njn] from comment #2)\n> > copy 3 just gets split up into lots of smaller pieces\n> \n> Actually, that's not right, because a single ImageData object can be reused\n> for each piece.\n> \n> > Is an ImageData transferable from a\n> > worker? Alternatively, can you construct an ImageData from an existing\n> > Uint8ClampedArray?\n> \n> AFAICT, the answer is \"no\" in both cases :(\n\nworker.postMessage(imageData, [imageData.data.buffer]); should work.", "attachment_id": null, "count": 3, "author": "VYV03354@nifty.ne.jp", "bug_id": 959925, "raw_text": "(In reply to Nicholas Nethercote [:njn] from comment #2)\n> > copy 3 just gets split up into lots of smaller pieces\n> \n> Actually, that's not right, because a single ImageData object can be reused\n> for each piece.\n> \n> > Is an ImageData transferable from a\n> > worker? Alternatively, can you construct an ImageData from an existing\n> > Uint8ClampedArray?\n> \n> AFAICT, the answer is \"no\" in both cases :(\n\nworker.postMessage(imageData, [imageData.data.buffer]); should work.", "creator": "VYV03354@nifty.ne.jp", "is_private": false, "time": "2014-01-15T06:50:05Z", "id": 8291443}, {"tags": [], "raw_text": "> worker.postMessage(imageData, [imageData.data.buffer]); should work.\n\nThat will transfer the array buffer, but then we need to be able to create a new ImageData from an existing ArrayBuffer without doing copying, and that isn't possible.", "bug_id": 959925, "time": "2014-01-15T06:55:35Z", "id": 8291449, "is_private": false, "creation_time": "2014-01-15T06:55:35Z", "text": "> worker.postMessage(imageData, [imageData.data.buffer]); should work.\n\nThat will transfer the array buffer, but then we need to be able to create a new ImageData from an existing ArrayBuffer without doing copying, and that isn't possible.", "attachment_id": null, "count": 4, "author": "n.nethercote@gmail.com", "creator": "n.nethercote@gmail.com"}, {"id": 8291486, "time": "2014-01-15T07:18:04Z", "is_private": false, "raw_text": "Oh, we don't support ImageData constructor yet :(", "bug_id": 959925, "creator": "VYV03354@nifty.ne.jp", "author": "VYV03354@nifty.ne.jp", "text": "Oh, we don't support ImageData constructor yet :(", "attachment_id": null, "count": 5, "tags": [], "creation_time": "2014-01-15T07:18:04Z"}, {"id": 8291554, "time": "2014-01-15T08:02:58Z", "is_private": false, "bug_id": 959925, "raw_text": "This code works on Chrome but doesn't on Firefox :(\nLooks like we break imageData.data.length when we passed around the imageData.", "tags": [], "creator": "VYV03354@nifty.ne.jp", "author": "VYV03354@nifty.ne.jp", "attachment_id": 8360242, "text": "Created attachment 8360242\nimagedata_worker.html\n\nThis code works on Chrome but doesn't on Firefox :(\nLooks like we break imageData.data.length when we passed around the imageData.", "count": 6, "creation_time": "2014-01-15T08:02:58Z"}, {"tags": [], "is_private": false, "id": 8292617, "time": "2014-01-15T14:42:32Z", "bug_id": 959925, "raw_text": "The imagedata_worker testcase seems to work just fine for me in a Mac 2014-01-11 nightly....", "author": "bzbarsky@mit.edu", "text": "The imagedata_worker testcase seems to work just fine for me in a Mac 2014-01-11 nightly....", "attachment_id": null, "count": 7, "creation_time": "2014-01-15T14:42:32Z", "creator": "bzbarsky@mit.edu"}, {"tags": [], "raw_text": "Ah, I tested it with Firefox 26. Indeed it worked in Nightly. Sorry for the confusion.", "bug_id": 959925, "is_private": false, "time": "2014-01-15T15:15:36Z", "id": 8292806, "creation_time": "2014-01-15T15:15:36Z", "attachment_id": null, "count": 8, "text": "Ah, I tested it with Firefox 26. Indeed it worked in Nightly. Sorry for the confusion.", "author": "VYV03354@nifty.ne.jp", "creator": "VYV03354@nifty.ne.jp"}, {"bug_id": 959925, "raw_text": "(In reply to Nicholas Nethercote [:njn] from comment #4)\n> > worker.postMessage(imageData, [imageData.data.buffer]); should work.\n> \n> That will transfer the array buffer, but then we need to be able to create a\n> new ImageData from an existing ArrayBuffer without doing copying, and that\n> isn't possible.\n\nSo we could pass around ImageData objects to workaround the absence of the ImageData constructor.", "id": 8292824, "time": "2014-01-15T15:19:07Z", "is_private": false, "tags": [], "creator": "VYV03354@nifty.ne.jp", "creation_time": "2014-01-15T15:19:07Z", "author": "VYV03354@nifty.ne.jp", "count": 9, "text": "(In reply to Nicholas Nethercote [:njn] from comment #4)\n> > worker.postMessage(imageData, [imageData.data.buffer]); should work.\n> \n> That will transfer the array buffer, but then we need to be able to create a\n> new ImageData from an existing ArrayBuffer without doing copying, and that\n> isn't possible.\n\nSo we could pass around ImageData objects to workaround the absence of the ImageData constructor.", "attachment_id": null}, {"bug_id": 959925, "creator": "n.nethercote@gmail.com", "raw_text": "> So we could pass around ImageData objects to workaround the absence of the\n> ImageData constructor.\n\nAh, so we can transfer ImageData objects:\n\n  worker.postMessage(imageData, [imageData.data.buffer]);\n\nThis means \"copy the |imageData|, except transfer the |imageData.data.buffer| property\". Nice.\n\nHowever, if I instead take the approach of calling createImageData/putImageData on chunks of the pixel data, then I don't need to transfer the ImageData. And the chunk approach (a) also causes copy 4 and copy 5 to be smaller, and (b) doesn't rely on features that browsers might not implement reliably. So I will pursue the chunk approach. But thanks for the information!", "id": 8295055, "time": "2014-01-15T22:00:36Z", "is_private": false, "creation_time": "2014-01-15T22:00:36Z", "author": "n.nethercote@gmail.com", "text": "> So we could pass around ImageData objects to workaround the absence of the\n> ImageData constructor.\n\nAh, so we can transfer ImageData objects:\n\n  worker.postMessage(imageData, [imageData.data.buffer]);\n\nThis means \"copy the |imageData|, except transfer the |imageData.data.buffer| property\". Nice.\n\nHowever, if I instead take the approach of calling createImageData/putImageData on chunks of the pixel data, then I don't need to transfer the ImageData. And the chunk approach (a) also causes copy 4 and copy 5 to be smaller, and (b) doesn't rely on features that browsers might not implement reliably. So I will pursue the chunk approach. But thanks for the information!", "attachment_id": null, "count": 10, "tags": []}, {"author": "n.nethercote@gmail.com", "count": 11, "attachment_id": null, "text": "*** Bug 931468 has been marked as a duplicate of this bug. ***", "tags": [], "creation_time": "2014-01-16T01:20:10Z", "id": 8295949, "time": "2014-01-16T01:20:10Z", "is_private": false, "creator": "n.nethercote@gmail.com", "bug_id": 959925, "raw_text": ""}, {"time": "2014-01-16T01:25:01Z", "id": 8295967, "is_private": false, "bug_id": 959925, "raw_text": "Here are three PDFs I've been using for testing. They're all scanned documents,\nand so rely entirely on images.\n- http://www.pdf-archive.com/2013/09/30/file2/file2.pdf\n- http://pbadupws.nrc.gov/docs/ML0909/ML090920467.pdf\n- http://ridl.cfd.rit.edu/products/manuals/Agilent/signal%20analyzer/Supplemental.pdf", "tags": [], "creator": "n.nethercote@gmail.com", "text": "Here are three PDFs I've been using for testing. They're all scanned documents,\nand so rely entirely on images.\n- http://www.pdf-archive.com/2013/09/30/file2/file2.pdf\n- http://pbadupws.nrc.gov/docs/ML0909/ML090920467.pdf\n- http://ridl.cfd.rit.edu/products/manuals/Agilent/signal%20analyzer/Supplemental.pdf", "count": 12, "attachment_id": null, "author": "n.nethercote@gmail.com", "creation_time": "2014-01-16T01:25:01Z"}, {"is_private": false, "time": "2014-01-16T01:45:27Z", "id": 8296017, "bug_id": 959925, "raw_text": "This patch implements the chunking of the createImageData/putImageData. Each\nchunk is 16 rows of the image. Referring to comment 0, it greatly reduces the\nsize of copies 3, 4 and 5. Of these, copy 3 is the most important, because it's\na JS allocation and so lives until the next GC (i.e. quite some time).\n\nFor example, if we have a 2000x2000 image (not untypical, and smaller than\nmany), a full copy will take 2000*2000*4 = 16,000,000 bytes. In comparison,\neach chunk is only 2000*16*4 = 128,000 bytes, which is 125x smaller.\n\nOn a 3 page document (file2.pdf from comment 12), this reduces peak physical\nmemory when scrolling through the entire document from ~315 MiB to ~265 MiB.\nThe savings on bigger documents will be larger.\n\nIt also, believe it or not, is marginally faster than the old code. I measured\nthe time taken for the putBinaryImageData() function to execute. In a browser\nthat supports set() and subarray(), it improved the time from ~70ms to ~50ms.\nIn a browser that doesn't support those functions, the time was about the same.\n\nYury: if you are happy with this patch, would you mind again handling the pull\nrequest on the github side, and testing? And please feel free to do any\nadditional performance testing.", "tags": [], "creator": "n.nethercote@gmail.com", "count": 13, "text": "Created attachment 8360785\nDo createImageData/putImageData in chunks, to save memory.\n\nThis patch implements the chunking of the createImageData/putImageData. Each\nchunk is 16 rows of the image. Referring to comment 0, it greatly reduces the\nsize of copies 3, 4 and 5. Of these, copy 3 is the most important, because it's\na JS allocation and so lives until the next GC (i.e. quite some time).\n\nFor example, if we have a 2000x2000 image (not untypical, and smaller than\nmany), a full copy will take 2000*2000*4 = 16,000,000 bytes. In comparison,\neach chunk is only 2000*16*4 = 128,000 bytes, which is 125x smaller.\n\nOn a 3 page document (file2.pdf from comment 12), this reduces peak physical\nmemory when scrolling through the entire document from ~315 MiB to ~265 MiB.\nThe savings on bigger documents will be larger.\n\nIt also, believe it or not, is marginally faster than the old code. I measured\nthe time taken for the putBinaryImageData() function to execute. In a browser\nthat supports set() and subarray(), it improved the time from ~70ms to ~50ms.\nIn a browser that doesn't support those functions, the time was about the same.\n\nYury: if you are happy with this patch, would you mind again handling the pull\nrequest on the github side, and testing? And please feel free to do any\nadditional performance testing.", "attachment_id": 8360785, "author": "n.nethercote@gmail.com", "creation_time": "2014-01-16T01:45:27Z"}, {"raw_text": "This patch avoids creating the opacity (A) array for images that don't have a\nmask (which is the common case for scanned documents) by writing the opacity\nvalues directly into the RGBA array. This avoids one quarter of copy 1 (from\ncomment 0).\n\nI renamed |buf| to make it clear which buffer was the A buffer and which was\nthe RGBA one.", "creator": "n.nethercote@gmail.com", "bug_id": 959925, "id": 8296572, "time": "2014-01-16T05:46:59Z", "is_private": false, "creation_time": "2014-01-16T05:46:59Z", "author": "n.nethercote@gmail.com", "count": 14, "text": "Created attachment 8360874\nDon't create the opacity buffer for images that lack a mask.\n\nThis patch avoids creating the opacity (A) array for images that don't have a\nmask (which is the common case for scanned documents) by writing the opacity\nvalues directly into the RGBA array. This avoids one quarter of copy 1 (from\ncomment 0).\n\nI renamed |buf| to make it clear which buffer was the A buffer and which was\nthe RGBA one.", "attachment_id": 8360874, "tags": []}, {"creator": "ydelendik@mozilla.com", "author": "ydelendik@mozilla.com", "count": 15, "text": "Comment on attachment 8360874\nDon't create the opacity buffer for images that lack a mask.\n\nReview of attachment 8360874:\n-----------------------------------------------------------------\n\nUnable to run tests. Please submit as a pull request to the https://github.com/mozilla/pdf.js with review items addressed.\n\n::: browser/extensions/pdfjs/content/build/pdf.worker.js\n@@ +28178,3 @@\n>        var smask = this.smask;\n>        var mask = this.mask;\n> +      var aBuf;\n\nUnneeded renaming of the buf variable\n\n@@ +28224,5 @@\n> +      }\n> +\n> +      if (aBuf) {\n> +        for (var i = 0, ii = width * actualHeight; i < ii; ++i) {\n> +          rgbaBuf[4*i + 3] = aBuf[i];\n\nNo need to multiply by 4 here. Introduce e.g. j that will be incremented by 4 and started from 3.", "attachment_id": 8360874, "creation_time": "2014-01-16T13:19:03Z", "id": 8297730, "time": "2014-01-16T13:19:03Z", "is_private": false, "bug_id": 959925, "raw_text": "Review of attachment 8360874:\n-----------------------------------------------------------------\n\nUnable to run tests. Please submit as a pull request to the https://github.com/mozilla/pdf.js with review items addressed.\n\n::: browser/extensions/pdfjs/content/build/pdf.worker.js\n@@ +28178,3 @@\n>        var smask = this.smask;\n>        var mask = this.mask;\n> +      var aBuf;\n\nUnneeded renaming of the buf variable\n\n@@ +28224,5 @@\n> +      }\n> +\n> +      if (aBuf) {\n> +        for (var i = 0, ii = width * actualHeight; i < ii; ++i) {\n> +          rgbaBuf[4*i + 3] = aBuf[i];\n\nNo need to multiply by 4 here. Introduce e.g. j that will be incremented by 4 and started from 3.", "tags": []}, {"attachment_id": 8360785, "text": "Comment on attachment 8360785\nDo createImageData/putImageData in chunks, to save memory.\n\nReview of attachment 8360785:\n-----------------------------------------------------------------\n\nLooks good. Unable to run tests. Please submit as a pull request to the https://github.com/mozilla/pdf.js for testing.", "count": 16, "author": "ydelendik@mozilla.com", "creation_time": "2014-01-16T13:21:24Z", "creator": "ydelendik@mozilla.com", "tags": [], "is_private": false, "time": "2014-01-16T13:21:24Z", "id": 8297741, "raw_text": "Review of attachment 8360785:\n-----------------------------------------------------------------\n\nLooks good. Unable to run tests. Please submit as a pull request to the https://github.com/mozilla/pdf.js for testing.", "bug_id": 959925}, {"count": 17, "attachment_id": null, "text": "> Unable to run tests.\n\nWhat does that mean?\n\nI setup a git clone and did the testing as described at https://github.com/mozilla/pdf.js/wiki/Contributing. It seems like the testing coverage for images isn't very good -- for the RGB changes I'm having to change getRgbBuffer for every sub-class of ColorSpace, and I have |throw| statements in the ones I haven't managed to test manually (which is most of them) and none of the test are hitting them. Am I missing something?", "author": "n.nethercote@gmail.com", "creation_time": "2014-01-16T19:33:31Z", "creator": "n.nethercote@gmail.com", "tags": [], "time": "2014-01-16T19:33:31Z", "id": 8299486, "is_private": false, "raw_text": "> Unable to run tests.\n\nWhat does that mean?\n\nI setup a git clone and did the testing as described at https://github.com/mozilla/pdf.js/wiki/Contributing. It seems like the testing coverage for images isn't very good -- for the RGB changes I'm having to change getRgbBuffer for every sub-class of ColorSpace, and I have |throw| statements in the ones I haven't managed to test manually (which is most of them) and none of the test are hitting them. Am I missing something?", "bug_id": 959925}, {"creator": "n.nethercote@gmail.com", "creation_time": "2014-01-16T19:38:03Z", "author": "n.nethercote@gmail.com", "count": 18, "attachment_id": null, "text": "\n> > +      var aBuf;\n> \n> Unneeded renaming of the buf variable\n\nAs I mentioned in comment 14, we now have two buffers in that function: the RGBA buffer, and the A buffer. I'd like it to be clear which is which.", "raw_text": "\n> > +      var aBuf;\n> \n> Unneeded renaming of the buf variable\n\nAs I mentioned in comment 14, we now have two buffers in that function: the RGBA buffer, and the A buffer. I'd like it to be clear which is which.", "bug_id": 959925, "is_private": false, "id": 8299514, "time": "2014-01-16T19:38:03Z", "tags": []}, {"creation_time": "2014-01-16T19:53:39Z", "attachment_id": null, "count": 19, "text": "\n(In reply to Nicholas Nethercote [:njn] from comment #18)\n > > Unneeded renaming of the buf variable\n> \n> As I mentioned in comment 14, we now have two buffers in that function: the\n> RGBA buffer, and the A buffer. I'd like it to be clear which is which.\n\nOh, I see 'a' stands for 'alpha' (thought it's an article)\n\n(In reply to Nicholas Nethercote [:njn] from comment #17)\n> I setup a git clone and did the testing as described at\n> https://github.com/mozilla/pdf.js/wiki/Contributing. It seems like the\n> testing coverage for images isn't very good -- for the RGB changes I'm\n> having to change getRgbBuffer for every sub-class of ColorSpace, and I have\n> |throw| statements in the ones I haven't managed to test manually (which is\n> most of them) and none of the test are hitting them. Am I missing something?\n\nNot sure. We have tests cases for the most of color spaces. Probably external PDF files or ref images are missing. Could you submit a pull request(s)?", "author": "ydelendik@mozilla.com", "creator": "ydelendik@mozilla.com", "tags": [], "raw_text": "\n(In reply to Nicholas Nethercote [:njn] from comment #18)\n > > Unneeded renaming of the buf variable\n> \n> As I mentioned in comment 14, we now have two buffers in that function: the\n> RGBA buffer, and the A buffer. I'd like it to be clear which is which.\n\nOh, I see 'a' stands for 'alpha' (thought it's an article)\n\n(In reply to Nicholas Nethercote [:njn] from comment #17)\n> I setup a git clone and did the testing as described at\n> https://github.com/mozilla/pdf.js/wiki/Contributing. It seems like the\n> testing coverage for images isn't very good -- for the RGB changes I'm\n> having to change getRgbBuffer for every sub-class of ColorSpace, and I have\n> |throw| statements in the ones I haven't managed to test manually (which is\n> most of them) and none of the test are hitting them. Am I missing something?\n\nNot sure. We have tests cases for the most of color spaces. Probably external PDF files or ref images are missing. Could you submit a pull request(s)?", "bug_id": 959925, "is_private": false, "time": "2014-01-16T19:53:39Z", "id": 8299594}, {"is_private": false, "time": "2014-01-17T06:04:40Z", "id": 8301906, "creator": "n.nethercote@gmail.com", "raw_text": "I did the chunking and skip-RGB-and-A approaches as pull requests:\nhttps://github.com/mozilla/pdf.js/pull/4138\nhttps://github.com/mozilla/pdf.js/pull/4139", "bug_id": 959925, "tags": [], "count": 20, "attachment_id": null, "text": "I did the chunking and skip-RGB-and-A approaches as pull requests:\nhttps://github.com/mozilla/pdf.js/pull/4138\nhttps://github.com/mozilla/pdf.js/pull/4139", "author": "n.nethercote@gmail.com", "creation_time": "2014-01-17T06:04:40Z"}, {"tags": [], "id": 8301911, "time": "2014-01-17T06:07:42Z", "is_private": false, "bug_id": 959925, "raw_text": "(In reply to Nicholas Nethercote [:njn] from comment #20)\n> I did the chunking and skip-RGB-and-A approaches as pull requests:\n> https://github.com/mozilla/pdf.js/pull/4138\n> https://github.com/mozilla/pdf.js/pull/4139\n\nAnd with those in place, in the common cases (no resizing, no opacity mask) we drop from 5 copies to approximately 1.03 copies of the data. (The 0.3 comes from the three slices that replace copies 3, 4 and 5.)", "author": "n.nethercote@gmail.com", "count": 21, "text": "(In reply to Nicholas Nethercote [:njn] from comment #20)\n> I did the chunking and skip-RGB-and-A approaches as pull requests:\n> https://github.com/mozilla/pdf.js/pull/4138\n> https://github.com/mozilla/pdf.js/pull/4139\n\nAnd with those in place, in the common cases (no resizing, no opacity mask) we drop from 5 copies to approximately 1.03 copies of the data. (The 0.3 comes from the three slices that replace copies 3, 4 and 5.)", "attachment_id": null, "creation_time": "2014-01-17T06:07:42Z", "creator": "n.nethercote@gmail.com"}, {"creation_time": "2014-01-17T21:55:41Z", "attachment_id": null, "count": 22, "text": "The chunking has been merged to pdf.js master: https://github.com/mozilla/pdf.js/pull/4138", "author": "n.nethercote@gmail.com", "tags": [], "bug_id": 959925, "creator": "n.nethercote@gmail.com", "raw_text": "The chunking has been merged to pdf.js master: https://github.com/mozilla/pdf.js/pull/4138", "time": "2014-01-17T21:55:41Z", "id": 8305322, "is_private": false}, {"attachment_id": null, "text": "Hmm. Further measurements show that the above changes only make a small difference.\n\nConsider http://pbadupws.nrc.gov/docs/ML0909/ML090920467.pdf, a scanned PDF with 226 pages, and each page is approximately 34,000,000 bytes when represented in 32-bit RGBA form.\n\nFollow these steps:\n- Start measuring resident memory consumption (RSS), e.g. via top.\n- Open the PDF.\n- Scroll to the document's end as fast as possible by holding down the PgDn key.\n- Wait for all the pages to be decoded and rendered.\n\nOn my fast desktop Linux machine, the waiting takes approximately one minute. More relevantly for this bug, RSS gradually climbs in an ascending sawtooth pattern to about 7,800 MiB, whether or not the patches I've written for this bug are applied. And guess what? 226 * 34,000,000 is over 7,328 MiB.\n\nIn other words, even though I reduced five copies of each page's date down to 1, the four copies I removed were all short-lived, whereas the remaining copy is very long-lived.\n\nI haven't yet worked out what is holding onto the arrays for each page, and whether it's deliberate or accidental. The data does get released eventually... if I wait a while, and then trigger \"minimize memory usage\" in about:memory, then RSS drops back to ~200 MB (but minimizing immediately doesn't do it). bdahl, do you know what controls this, and whether it's deliberate? I'm hoping that it's accidental, and that with some judicious nulling of some references that it'll become collectable much earlier.\n\nFinally: evince is the native PDF viewer on Linux. When I use it to view the document, I can't get its RSS to exceed 150 MB. I don't expect pdf.js should be as memory-efficient as evince, but currently it's not even close -- 7,800 MiB for a 226 page document is ridiculous.", "count": 23, "author": "n.nethercote@gmail.com", "tags": [], "creation_time": "2014-01-20T01:44:03Z", "time": "2014-01-20T01:44:03Z", "id": 8308403, "is_private": false, "raw_text": "Hmm. Further measurements show that the above changes only make a small difference.\n\nConsider http://pbadupws.nrc.gov/docs/ML0909/ML090920467.pdf, a scanned PDF with 226 pages, and each page is approximately 34,000,000 bytes when represented in 32-bit RGBA form.\n\nFollow these steps:\n- Start measuring resident memory consumption (RSS), e.g. via top.\n- Open the PDF.\n- Scroll to the document's end as fast as possible by holding down the PgDn key.\n- Wait for all the pages to be decoded and rendered.\n\nOn my fast desktop Linux machine, the waiting takes approximately one minute. More relevantly for this bug, RSS gradually climbs in an ascending sawtooth pattern to about 7,800 MiB, whether or not the patches I've written for this bug are applied. And guess what? 226 * 34,000,000 is over 7,328 MiB.\n\nIn other words, even though I reduced five copies of each page's date down to 1, the four copies I removed were all short-lived, whereas the remaining copy is very long-lived.\n\nI haven't yet worked out what is holding onto the arrays for each page, and whether it's deliberate or accidental. The data does get released eventually... if I wait a while, and then trigger \"minimize memory usage\" in about:memory, then RSS drops back to ~200 MB (but minimizing immediately doesn't do it). bdahl, do you know what controls this, and whether it's deliberate? I'm hoping that it's accidental, and that with some judicious nulling of some references that it'll become collectable much earlier.\n\nFinally: evince is the native PDF viewer on Linux. When I use it to view the document, I can't get its RSS to exceed 150 MB. I don't expect pdf.js should be as memory-efficient as evince, but currently it's not even close -- 7,800 MiB for a 226 page document is ridiculous.", "creator": "n.nethercote@gmail.com", "bug_id": 959925}, {"creation_time": "2014-01-20T04:19:31Z", "tags": [], "text": "So there is a mechanism to discard the imageData: PDFPageProxy.complete(), which calls _tryDestroy(). However, somehow when you scroll quickly the intra-thread messaging ordering occurs in such a way that none of the pages get to complete() until all the subsequent pages have been displayed. In fact, if you scroll slowly through the document -- e.g. one page per second, which is enough to render each page fully before moving to the next one -- the peak RSS is about 1 GB.", "count": 24, "attachment_id": null, "author": "n.nethercote@gmail.com", "raw_text": "So there is a mechanism to discard the imageData: PDFPageProxy.complete(), which calls _tryDestroy(). However, somehow when you scroll quickly the intra-thread messaging ordering occurs in such a way that none of the pages get to complete() until all the subsequent pages have been displayed. In fact, if you scroll slowly through the document -- e.g. one page per second, which is enough to render each page fully before moving to the next one -- the peak RSS is about 1 GB.", "creator": "n.nethercote@gmail.com", "bug_id": 959925, "is_private": false, "time": "2014-01-20T04:19:31Z", "id": 8308625}, {"author": "n.nethercote@gmail.com", "text": "Created attachment 8362353\nIgnore continueCallback.\n\nI worked it out. The problem is with continueCallback. If defined, it gets\ncalled when a page's image is received by the main thread. At this point, the\npage is almost done, and we'd really like to finish processing the last few ops\nin order to reach complete(), whereupon the page's pixel data can be freed.\n\nHowever, viewer.js tries to be clever. It makes continueCallback (which is\ncalled after every 15ms of operation processing) return early if the page being\nrendered isn't the \"most important\" page, which I guess is the page that is\ncurrently in the viewer. So we end up postponing the completion of every page\nprior to the final page until the final page is rendered, but that doesn't\nhappen until all the previous pages have sent their image data to the main\nthread, because the image data sending is in sequential page order.\n\nThe attached patch just ignores continueCallback. With it applied, I get a\npeak RSS of about 880 MB instead of 7,800. As it stands the patch probably\nisn't reasonable, but something needs to be done.", "count": 25, "attachment_id": 8362353, "creation_time": "2014-01-20T05:03:27Z", "creator": "n.nethercote@gmail.com", "tags": [], "id": 8308686, "time": "2014-01-20T05:03:27Z", "is_private": false, "bug_id": 959925, "raw_text": "I worked it out. The problem is with continueCallback. If defined, it gets\ncalled when a page's image is received by the main thread. At this point, the\npage is almost done, and we'd really like to finish processing the last few ops\nin order to reach complete(), whereupon the page's pixel data can be freed.\n\nHowever, viewer.js tries to be clever. It makes continueCallback (which is\ncalled after every 15ms of operation processing) return early if the page being\nrendered isn't the \"most important\" page, which I guess is the page that is\ncurrently in the viewer. So we end up postponing the completion of every page\nprior to the final page until the final page is rendered, but that doesn't\nhappen until all the previous pages have sent their image data to the main\nthread, because the image data sending is in sequential page order.\n\nThe attached patch just ignores continueCallback. With it applied, I get a\npeak RSS of about 880 MB instead of 7,800. As it stands the patch probably\nisn't reasonable, but something needs to be done."}, {"creation_time": "2014-01-20T22:27:19Z", "author": "n.nethercote@gmail.com", "text": "> I worked it out. The problem is with continueCallback. \n\nYury pointed me at https://github.com/mozilla/pdf.js/issues/1887, which covers this.", "count": 26, "attachment_id": null, "tags": [], "bug_id": 959925, "raw_text": "> I worked it out. The problem is with continueCallback. \n\nYury pointed me at https://github.com/mozilla/pdf.js/issues/1887, which covers this.", "creator": "n.nethercote@gmail.com", "id": 8311373, "time": "2014-01-20T22:27:19Z", "is_private": false}, {"is_private": false, "time": "2014-01-22T00:59:46Z", "id": 8316780, "bug_id": 959925, "raw_text": "The three patches landed (squashed into one):\nhttps://github.com/mozilla/pdf.js/commit/f7e354dfe54b215c2539735701923c02908293f3\n\nI filed bug 962351 for the scrolling issue in comment 25.", "tags": [], "creator": "n.nethercote@gmail.com", "count": 27, "attachment_id": null, "text": "The three patches landed (squashed into one):\nhttps://github.com/mozilla/pdf.js/commit/f7e354dfe54b215c2539735701923c02908293f3\n\nI filed bug 962351 for the scrolling issue in comment 25.", "author": "n.nethercote@gmail.com", "creation_time": "2014-01-22T00:59:46Z"}, {"id": 8317046, "time": "2014-01-22T02:01:59Z", "is_private": false, "creator": "ryanvm@gmail.com", "raw_text": "Let's leave this open for bug 960051.", "bug_id": 959925, "author": "ryanvm@gmail.com", "attachment_id": null, "text": "Let's leave this open for bug 960051.", "count": 28, "tags": [], "creation_time": "2014-01-22T02:01:59Z"}, {"creation_time": "2014-01-22T14:25:06Z", "author": "ydelendik@mozilla.com", "text": "Comment on attachment 8362353\nIgnore continueCallback.\n\nSee IRC discussion at \nhttp://logbot.glob.com.au/?c=mozilla%23pdfjs&s=20+Jan+2014&e=20+Jan+2014&h=continueCallback#c3303", "attachment_id": 8362353, "count": 29, "creator": "ydelendik@mozilla.com", "tags": [], "raw_text": "See IRC discussion at \nhttp://logbot.glob.com.au/?c=mozilla%23pdfjs&s=20+Jan+2014&e=20+Jan+2014&h=continueCallback#c3303", "bug_id": 959925, "is_private": false, "id": 8319693, "time": "2014-01-22T14:25:06Z"}, {"creator": "n.nethercote@gmail.com", "creation_time": "2014-01-25T10:22:16Z", "author": "n.nethercote@gmail.com", "text": "This has now been imported (bug 960051).", "count": 30, "attachment_id": null, "raw_text": "This has now been imported (bug 960051).", "bug_id": 959925, "id": 8333652, "time": "2014-01-25T10:22:16Z", "is_private": false, "tags": []}]}}}