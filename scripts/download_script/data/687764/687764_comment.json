{"comments": {}, "bugs": {"687764": {"comments": [{"creator": "matt@mattmccutchen.net", "author": "matt@mattmccutchen.net", "tags": [], "is_private": false, "count": 0, "id": 5724833, "bug_id": 687764, "raw_text": "I've seen claims in many places that users can protect themselves from given classes of fraudulent certificates by manually inspecting the certificate in the page info.  Of course, that is not true: the page info only shows the certificate for the final top-level HTML page (not previous redirects, embedded objects, or XMLHttpRequests), and confidential data (such as cookies, a form post, or data scraped from an open window of the same origin or retrieved via an XMLHttpRequest or window.postMessage by virtue of the origin) may already have been irrevocably leaked.  Not to mention that manual inspection is a PITA that no user will ever do consistently.  In essence, users, web sites, and the browser itself all assume that any successful SSL connection is server-authenticated; I have yet to see a viable design that would avoid or weaken this assumption.  (C.f. bug 327181 comment 14 and bug 566008 comment 10.)\n\nObviously, suddenly ripping out the \"View Certificate\" button and the site identity button \"Verified by\" text (for DV certs; EV certs are different) would give a poor user experience.  So this bug is to start a discussion on what information or functionality these controls can usefully provide without giving rise to a false sense of security.", "creation_time": "2011-09-20T04:14:44Z", "text": "I've seen claims in many places that users can protect themselves from given classes of fraudulent certificates by manually inspecting the certificate in the page info.  Of course, that is not true: the page info only shows the certificate for the final top-level HTML page (not previous redirects, embedded objects, or XMLHttpRequests), and confidential data (such as cookies, a form post, or data scraped from an open window of the same origin or retrieved via an XMLHttpRequest or window.postMessage by virtue of the origin) may already have been irrevocably leaked.  Not to mention that manual inspection is a PITA that no user will ever do consistently.  In essence, users, web sites, and the browser itself all assume that any successful SSL connection is server-authenticated; I have yet to see a viable design that would avoid or weaken this assumption.  (C.f. bug 327181 comment 14 and bug 566008 comment 10.)\n\nObviously, suddenly ripping out the \"View Certificate\" button and the site identity button \"Verified by\" text (for DV certs; EV certs are different) would give a poor user experience.  So this bug is to start a discussion on what information or functionality these controls can usefully provide without giving rise to a false sense of security.", "time": "2011-09-20T04:14:44Z", "attachment_id": null}, {"author": "mozilla@misc.lka.org.lu", "creator": "mozilla@misc.lka.org.lu", "bug_id": 687764, "id": 5724920, "tags": [], "count": 1, "is_private": false, "attachment_id": null, "time": "2011-09-20T06:13:30Z", "text": "> I've seen claims in many places that users can protect themselves from given \n> classes of fraudulent certificates by manually inspecting the certificate in \n> the page info.\n\nThe claim is that a user can verify the authenticity of a self-signed certificate by comparing its fingerprint against a reference from a trusted source (such as a business card handed to him by website operator).\n\n> Of course, that is not true: the page info only shows the certificate for the \n> final top-level HTML page (not previous redirects, embedded objects, or \n> XMLHttpRequests)\n\nThe same is true of CA-validates certificates.\n\nHowever, all this are web server design issues under the control of the webmaster. If the webmaster is security-conscious, he should avoid excessive redirects, and including elements from untrusted third-party sites (such as googleapis) into his secure pages. Certificates prevent man-in-the-middle attacks \"on the wire\", but if you let in the bad guy through a different avenue, than all bets are off.\n\n> and confidential data (such as cookies, a form post, or data scraped from an \n> open window of the same origin or retrieved via an XMLHttpRequest or \n> window.postMessage by virtue of the origin) may already have been irrevocably > leaked.\n\nTrue enough. So use the \"secure\" marker on your cookies, and keep the entire session on SSL, rather than just the login dialog. For performance and scalability reasons, many web servers (such as Facebook) don't do this, but this is a bug in these web servers, rather than a design fault in SSL.\n\n> Not to mention that manual inspection is a PITA that no user will ever do \n> consistently.\n\nHe will only need to do it once per server. After that, the browser can make sure that the certificate didn't change since last time.\n\nAnd if \"lazy\" users are a concern, maybe we could make the fingerprint verification interface such that the user has to enter the fingerprint he \"expects\" (i.e. the one on the business card handed he got by the website operator). Certificate exception is then only added if the certificate matches.\n\nBut then, the biggest \"lazy user and accomodating webmaster\" issue are the transparent redirects from http to https so that the users doesn't need to type the https manually. An interloper could catch the original http, and just remove the redirect or replace it with a redirect to his own \"secure\" site. Most users don't verify presence of the lock icon, nor whether the site is still what they expect (and then, lets hope the attacker didn't substitute an alpha for an a, which would look exactly the same). Of course, this affects CA-validated certs more than fingerprint-verified certs.\n\n> So this bug is to start a discussion on what information or functionality \n> these controls can usefully provide without giving rise to a false sense of \n> security.\n\nSome browsers already warn about pages which mix encrypted and unencrypted elements. Maybe the same could be extended to pages which mix encrypted contents from different bailiwicks?\n\nCookie leaking can be prevented by the \"secure\" tag. Maybe the browser could warn about cookies that it got from an SSL page that are not marked secure? (... and pre-existing cookies that would be sent to SSL page).\n\nWhen following a link from an SSL page to a non-SSL page, GET parameters should be scrubbed from the referer.", "raw_text": "> I've seen claims in many places that users can protect themselves from given \n> classes of fraudulent certificates by manually inspecting the certificate in \n> the page info.\n\nThe claim is that a user can verify the authenticity of a self-signed certificate by comparing its fingerprint against a reference from a trusted source (such as a business card handed to him by website operator).\n\n> Of course, that is not true: the page info only shows the certificate for the \n> final top-level HTML page (not previous redirects, embedded objects, or \n> XMLHttpRequests)\n\nThe same is true of CA-validates certificates.\n\nHowever, all this are web server design issues under the control of the webmaster. If the webmaster is security-conscious, he should avoid excessive redirects, and including elements from untrusted third-party sites (such as googleapis) into his secure pages. Certificates prevent man-in-the-middle attacks \"on the wire\", but if you let in the bad guy through a different avenue, than all bets are off.\n\n> and confidential data (such as cookies, a form post, or data scraped from an \n> open window of the same origin or retrieved via an XMLHttpRequest or \n> window.postMessage by virtue of the origin) may already have been irrevocably > leaked.\n\nTrue enough. So use the \"secure\" marker on your cookies, and keep the entire session on SSL, rather than just the login dialog. For performance and scalability reasons, many web servers (such as Facebook) don't do this, but this is a bug in these web servers, rather than a design fault in SSL.\n\n> Not to mention that manual inspection is a PITA that no user will ever do \n> consistently.\n\nHe will only need to do it once per server. After that, the browser can make sure that the certificate didn't change since last time.\n\nAnd if \"lazy\" users are a concern, maybe we could make the fingerprint verification interface such that the user has to enter the fingerprint he \"expects\" (i.e. the one on the business card handed he got by the website operator). Certificate exception is then only added if the certificate matches.\n\nBut then, the biggest \"lazy user and accomodating webmaster\" issue are the transparent redirects from http to https so that the users doesn't need to type the https manually. An interloper could catch the original http, and just remove the redirect or replace it with a redirect to his own \"secure\" site. Most users don't verify presence of the lock icon, nor whether the site is still what they expect (and then, lets hope the attacker didn't substitute an alpha for an a, which would look exactly the same). Of course, this affects CA-validated certs more than fingerprint-verified certs.\n\n> So this bug is to start a discussion on what information or functionality \n> these controls can usefully provide without giving rise to a false sense of \n> security.\n\nSome browsers already warn about pages which mix encrypted and unencrypted elements. Maybe the same could be extended to pages which mix encrypted contents from different bailiwicks?\n\nCookie leaking can be prevented by the \"secure\" tag. Maybe the browser could warn about cookies that it got from an SSL page that are not marked secure? (... and pre-existing cookies that would be sent to SSL page).\n\nWhen following a link from an SSL page to a non-SSL page, GET parameters should be scrubbed from the referer.", "creation_time": "2011-09-20T06:13:30Z"}, {"tags": [], "is_private": false, "count": 2, "id": 5725781, "bug_id": 687764, "raw_text": "(In reply to Alain Knaff from comment #1)\n> The claim is that a user can verify the authenticity of a self-signed\n> certificate by comparing its fingerprint against a reference from a trusted\n> source (such as a business card handed to him by website operator).\n\nThat is a different claim, which is true.  This bug is about the claim in comment #0.  I don't think any of the rest of your comment applies to this bug.", "creation_time": "2011-09-20T16:14:15Z", "text": "(In reply to Alain Knaff from comment #1)\n> The claim is that a user can verify the authenticity of a self-signed\n> certificate by comparing its fingerprint against a reference from a trusted\n> source (such as a business card handed to him by website operator).\n\nThat is a different claim, which is true.  This bug is about the claim in comment #0.  I don't think any of the rest of your comment applies to this bug.", "time": "2011-09-20T16:14:15Z", "attachment_id": null, "creator": "matt@mattmccutchen.net", "author": "matt@mattmccutchen.net"}]}}}