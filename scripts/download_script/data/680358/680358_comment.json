{"comments": {}, "bugs": {"680358": {"comments": [{"tags": [], "is_private": false, "count": 0, "id": 5664710, "bug_id": 680358, "creation_time": "2011-08-19T06:45:20Z", "raw_text": "Running DMD on a local test video revealed larger than expected internal fragmentation on our video frame allocations:\n\n==5965== Unreported: 7,708,672 (cumulative: 7,708,672) bytes in 11 heap block(s) in record 1 of 11426:\n==5965==  Requested bytes unreported: 1,536,000 / 16,896,000\n==5965==  Slop      bytes unreported: 6,172,672 / 6,172,672\n==5965==    at 0x4A06065: malloc (vg_replace_malloc.c:261)\n==5965==    by 0x5678F26: moz_xmalloc (mozalloc.cpp:110)\n==5965==    by 0x7A49C75: mozilla::layers::BasicPlanarYCbCrImage::SetData(mozilla::layers::PlanarYCbCrImage::Data const&) (mozalloc.h:241)\n==5965==    by 0x73BCEEB: VideoData::Create(nsVideoInfo&, mozilla::layers::ImageContainer*, long, long, long, VideoData::YCbCrBuffer const&, int, long, nsIntRect) (nsBuiltinDecoderReader.cpp:\n186)\n\nThe test video is 800x480, so each frame allocation is 1536000 bytes.  The decoded frame queue holds 10 frames.  The allocator appears to be allocating 2097152 bytes per frame, resulting in 25% slop.\n\n(Note that one entire frame is classified as unreported because it has been allocated but is not currently present in the decode queue used to calculate the allocated memory size.)\n\nLooking at jemalloc's implementation, these allocations will be taking the huge_malloc path, which rounds allocations up to the next chunksize (~1MB).", "text": "Running DMD on a local test video revealed larger than expected internal fragmentation on our video frame allocations:\n\n==5965== Unreported: 7,708,672 (cumulative: 7,708,672) bytes in 11 heap block(s) in record 1 of 11426:\n==5965==  Requested bytes unreported: 1,536,000 / 16,896,000\n==5965==  Slop      bytes unreported: 6,172,672 / 6,172,672\n==5965==    at 0x4A06065: malloc (vg_replace_malloc.c:261)\n==5965==    by 0x5678F26: moz_xmalloc (mozalloc.cpp:110)\n==5965==    by 0x7A49C75: mozilla::layers::BasicPlanarYCbCrImage::SetData(mozilla::layers::PlanarYCbCrImage::Data const&) (mozalloc.h:241)\n==5965==    by 0x73BCEEB: VideoData::Create(nsVideoInfo&, mozilla::layers::ImageContainer*, long, long, long, VideoData::YCbCrBuffer const&, int, long, nsIntRect) (nsBuiltinDecoderReader.cpp:\n186)\n\nThe test video is 800x480, so each frame allocation is 1536000 bytes.  The decoded frame queue holds 10 frames.  The allocator appears to be allocating 2097152 bytes per frame, resulting in 25% slop.\n\n(Note that one entire frame is classified as unreported because it has been allocated but is not currently present in the decode queue used to calculate the allocated memory size.)\n\nLooking at jemalloc's implementation, these allocations will be taking the huge_malloc path, which rounds allocations up to the next chunksize (~1MB).", "time": "2011-08-19T06:45:20Z", "attachment_id": null, "creator": "kinetik@flim.org", "author": "kinetik@flim.org"}, {"creator": "n.nethercote@gmail.com", "author": "n.nethercote@gmail.com", "creation_time": "2011-08-19T06:54:16Z", "raw_text": "Nice find.  I'm surprised but glad that someone other than me is running DMD :)\n\nThe jemalloc size requests you have to watch out for are in the ranges 512--8192 bytes and 1--2MB, because in those ranges all the sizes are powers-of-two so you can end up with up to 50% slop if you're unlucky.", "text": "Nice find.  I'm surprised but glad that someone other than me is running DMD :)\n\nThe jemalloc size requests you have to watch out for are in the ranges 512--8192 bytes and 1--2MB, because in those ranges all the sizes are powers-of-two so you can end up with up to 50% slop if you're unlucky.", "attachment_id": null, "time": "2011-08-19T06:54:16Z", "tags": [], "count": 1, "is_private": false, "id": 5664715, "bug_id": 680358}, {"creator": "rjesup@jesup.org", "author": "rjesup@jesup.org", "creation_time": "2011-08-23T20:40:07Z", "raw_text": "Taking bug to do investigation per memshrink meeting", "text": "Taking bug to do investigation per memshrink meeting", "attachment_id": null, "time": "2011-08-23T20:40:07Z", "tags": [], "is_private": false, "count": 2, "id": 5672918, "bug_id": 680358}, {"text": "Here's how this works in jemalloc:\n\nIn huge_malloc, let csize be size rounded up to the nearest megabyte, and let psize be size rounded up to the nearest page.\n\nAllocate csize bytes.  Then, if MALLOC_DECOMMIT is defined, do\n\n  if (csize - psize > 0)\n    pages_decommit((void *)((uintptr_t)ret + psize), csize - psize);\n\nSo on Windows, where DECOMMIT is defined, we're not wasting anything.  On Linux, I think the assumption is that the mmapped memory beyond psize is never touched, so the OS never gives it a physical page.", "attachment_id": null, "time": "2011-08-23T20:44:33Z", "creation_time": "2011-08-23T20:44:33Z", "raw_text": "Here's how this works in jemalloc:\n\nIn huge_malloc, let csize be size rounded up to the nearest megabyte, and let psize be size rounded up to the nearest page.\n\nAllocate csize bytes.  Then, if MALLOC_DECOMMIT is defined, do\n\n  if (csize - psize > 0)\n    pages_decommit((void *)((uintptr_t)ret + psize), csize - psize);\n\nSo on Windows, where DECOMMIT is defined, we're not wasting anything.  On Linux, I think the assumption is that the mmapped memory beyond psize is never touched, so the OS never gives it a physical page.", "bug_id": 680358, "count": 3, "is_private": false, "tags": [], "id": 5672938, "creator": "justin.lebar+bug@gmail.com", "author": "justin.lebar+bug@gmail.com"}, {"text": "IOW, I don't think the round-up-to-nearest-mb behavior is actually causing us to use more memory.", "time": "2011-08-23T20:45:44Z", "attachment_id": null, "creation_time": "2011-08-23T20:45:44Z", "raw_text": "IOW, I don't think the round-up-to-nearest-mb behavior is actually causing us to use more memory.", "bug_id": 680358, "tags": [], "count": 4, "is_private": false, "id": 5672944, "creator": "justin.lebar+bug@gmail.com", "author": "justin.lebar+bug@gmail.com"}, {"tags": [], "count": 5, "is_private": false, "id": 5673092, "bug_id": 680358, "raw_text": "I think I agree with Justin - it will take memory space in the map, but won't take actual physical memory.  Taking space in the map still matters, since 32-bit processes will start to run out of usable VM space (and start having major perf problems) around 1.7-2.1GB VSS.  And in Win32 (though the logic is obtuse) MALLOC_DECOMMIT is set.\n\nIt may be useful to use MALLOC_DECOMMIT on Linux if it frees up VM space, but it's not a huge issue.  It would apply to all >1MB allocs (large images mostly).  It's easy to test (jlebar?).\n\nThere may be a use in grouping allocs of the video buffers when they're *smaller* than 1MB to reduce total slop, and above 1MB it may save address space if we don't do MALLOC_DECOMMIT.  I'll look into that.", "creation_time": "2011-08-23T21:31:39Z", "text": "I think I agree with Justin - it will take memory space in the map, but won't take actual physical memory.  Taking space in the map still matters, since 32-bit processes will start to run out of usable VM space (and start having major perf problems) around 1.7-2.1GB VSS.  And in Win32 (though the logic is obtuse) MALLOC_DECOMMIT is set.\n\nIt may be useful to use MALLOC_DECOMMIT on Linux if it frees up VM space, but it's not a huge issue.  It would apply to all >1MB allocs (large images mostly).  It's easy to test (jlebar?).\n\nThere may be a use in grouping allocs of the video buffers when they're *smaller* than 1MB to reduce total slop, and above 1MB it may save address space if we don't do MALLOC_DECOMMIT.  I'll look into that.", "time": "2011-08-23T21:31:39Z", "attachment_id": null, "creator": "rjesup@jesup.org", "author": "rjesup@jesup.org"}, {"author": "paul.biggar@gmail.com", "creator": "paul.biggar@gmail.com", "attachment_id": null, "time": "2011-08-23T21:40:11Z", "text": "(In reply to Randell Jesup [:jesup] from comment #5)\n\n> It may be useful to use MALLOC_DECOMMIT on Linux if it frees up VM space,\n> but it's not a huge issue.  It would apply to all >1MB allocs (large images\n> mostly).  It's easy to test (jlebar?).\n\nThis owuld be worthwhile anyway. I tried turning it on on Mac and it reduced memory footprint by 43%(!), but totally killed performance of the browser.", "raw_text": "(In reply to Randell Jesup [:jesup] from comment #5)\n\n> It may be useful to use MALLOC_DECOMMIT on Linux if it frees up VM space,\n> but it's not a huge issue.  It would apply to all >1MB allocs (large images\n> mostly).  It's easy to test (jlebar?).\n\nThis owuld be worthwhile anyway. I tried turning it on on Mac and it reduced memory footprint by 43%(!), but totally killed performance of the browser.", "creation_time": "2011-08-23T21:40:11Z", "bug_id": 680358, "id": 5673118, "count": 6, "is_private": false, "tags": []}, {"bug_id": 680358, "id": 5673401, "is_private": false, "count": 7, "tags": [], "attachment_id": null, "time": "2011-08-23T23:08:35Z", "text": "I was under the impression that the DECOMMIT wins on mac were due to madvise bugs -- if madvise FREE/DONT_NEED doesn't actually release memory, then of course decommit would help.\n\nI did an informal experiment with DECOMMIT in bug 636220 comment 16 and 17 -- compare anonymous maps' RSS between comment 16 and 17 and they're close to the same.", "raw_text": "I was under the impression that the DECOMMIT wins on mac were due to madvise bugs -- if madvise FREE/DONT_NEED doesn't actually release memory, then of course decommit would help.\n\nI did an informal experiment with DECOMMIT in bug 636220 comment 16 and 17 -- compare anonymous maps' RSS between comment 16 and 17 and they're close to the same.", "creation_time": "2011-08-23T23:08:35Z", "author": "justin.lebar+bug@gmail.com", "creator": "justin.lebar+bug@gmail.com"}, {"author": "paul.biggar@gmail.com", "creator": "paul.biggar@gmail.com", "attachment_id": null, "time": "2011-08-23T23:42:09Z", "text": "(In reply to Justin Lebar [:jlebar] from comment #7)\n> I was under the impression that the DECOMMIT wins on mac were due to madvise\n> bugs -- if madvise FREE/DONT_NEED doesn't actually release memory, then of\n> course decommit would help.\n\nNo, this is two separate issues. The madvise issue is that madvise doesnt work on Mac 10.5. The DECOMMIT thing is that enabling MALLOC_DECOMMIT on mac (that is, doing a decommit using mmap instead of using madvise) causes a huge slowdown.", "raw_text": "(In reply to Justin Lebar [:jlebar] from comment #7)\n> I was under the impression that the DECOMMIT wins on mac were due to madvise\n> bugs -- if madvise FREE/DONT_NEED doesn't actually release memory, then of\n> course decommit would help.\n\nNo, this is two separate issues. The madvise issue is that madvise doesnt work on Mac 10.5. The DECOMMIT thing is that enabling MALLOC_DECOMMIT on mac (that is, doing a decommit using mmap instead of using madvise) causes a huge slowdown.", "creation_time": "2011-08-23T23:42:09Z", "bug_id": 680358, "id": 5673487, "tags": [], "count": 8, "is_private": false}, {"author": "justin.lebar+bug@gmail.com", "creator": "justin.lebar+bug@gmail.com", "attachment_id": null, "time": "2011-08-23T23:58:10Z", "text": "But did DECOMMIT give a massive footprint win on 10.6, where madvise works properly, or only on 10.5?  My thesis is that non-decommit plus a working madvise should use approximately as much memory as decommit.", "raw_text": "But did DECOMMIT give a massive footprint win on 10.6, where madvise works properly, or only on 10.5?  My thesis is that non-decommit plus a working madvise should use approximately as much memory as decommit.", "creation_time": "2011-08-23T23:58:10Z", "bug_id": 680358, "id": 5673540, "count": 9, "is_private": false, "tags": []}, {"author": "paul.biggar@gmail.com", "creator": "paul.biggar@gmail.com", "attachment_id": null, "time": "2011-08-24T00:08:22Z", "text": "DECOMMIT gave a much larger win than madvise, on 10.6 (where madvise works).", "raw_text": "DECOMMIT gave a much larger win than madvise, on 10.6 (where madvise works).", "creation_time": "2011-08-24T00:08:22Z", "bug_id": 680358, "id": 5673582, "tags": [], "is_private": false, "count": 10}, {"bug_id": 680358, "id": 5679704, "tags": [], "is_private": false, "count": 11, "author": "n.nethercote@gmail.com", "time": "2011-08-26T02:49:52Z", "attachment_id": null, "text": "So there are two things to do here:\n\n(1) Account for the slop in the media/decoded-{audio,video} memory reporters.  This can be done by using moz_malloc_usable_size, and if it returns 0 (which it can on platforms that don't support it), then fall back to the old calculation which doesn't account for slop.  You could fold in the slop to the existing reporters, or report it separately, depending on which you think is better.\n\n(2) Determine if the slop can be reduced.  That's the investigation that Randall agreed to do.\n\nWe can do both in this bug or file a separate bug for (1).  Any preferences?", "creator": "n.nethercote@gmail.com", "raw_text": "So there are two things to do here:\n\n(1) Account for the slop in the media/decoded-{audio,video} memory reporters.  This can be done by using moz_malloc_usable_size, and if it returns 0 (which it can on platforms that don't support it), then fall back to the old calculation which doesn't account for slop.  You could fold in the slop to the existing reporters, or report it separately, depending on which you think is better.\n\n(2) Determine if the slop can be reduced.  That's the investigation that Randall agreed to do.\n\nWe can do both in this bug or file a separate bug for (1).  Any preferences?", "creation_time": "2011-08-26T02:49:52Z"}, {"creator": "kinetik@flim.org", "author": "kinetik@flim.org", "creation_time": "2011-08-26T03:00:10Z", "raw_text": "I think it makes sense to have a separate bug, and to track the slop separately since in this case it's \"only\" wasted address space.", "text": "I think it makes sense to have a separate bug, and to track the slop separately since in this case it's \"only\" wasted address space.", "time": "2011-08-26T03:00:10Z", "attachment_id": null, "count": 12, "is_private": false, "tags": [], "id": 5679721, "bug_id": 680358}, {"count": 13, "is_private": false, "tags": [], "id": 5679728, "bug_id": 680358, "creation_time": "2011-08-26T03:04:24Z", "raw_text": "Filed bug 682195.", "text": "Filed bug 682195.", "time": "2011-08-26T03:04:24Z", "attachment_id": null, "creator": "kinetik@flim.org", "author": "kinetik@flim.org"}, {"tags": [], "is_private": false, "count": 14, "id": 5974855, "bug_id": 680358, "raw_text": "Randell, any progress on (2) from comment 11?", "creation_time": "2012-01-12T10:52:52Z", "text": "Randell, any progress on (2) from comment 11?", "time": "2012-01-12T10:52:52Z", "attachment_id": null, "creator": "n.nethercote@gmail.com", "author": "n.nethercote@gmail.com"}, {"author": "rjesup@jesup.org", "creator": "rjesup@jesup.org", "bug_id": 680358, "id": 5976534, "is_private": false, "count": 15, "tags": [], "time": "2012-01-12T20:57:58Z", "attachment_id": null, "text": "Yes, sorry.\n\nThe problem is that the frames are allocated very separately - it's not creating a queue of 10 frames to (re)use, it's allocating and pushing frames into a queue until there are 10, and deallocating when they're consumed.\n\nIn order to fix this, we would need to create a recycling allocator (aka buffer pool) for use for the video frames for a stream and pre-allocate 10 (AMPLE_VIDEO_FRAMES) buffers, in which case we could remove all the slop (except <1MB at the end) and also avoid trips through the allocator on every frame (not a huge win, but a win especially if due to other things interfering (fragmenting/re-using) with the video's allocations it might have to obtain new memory from the OS at times).\n\nAs an optimization, the buffer pool could either be informed when it's not in operation (and drop the memory), or whenever all frames are consumed drop the memory (even if this churns a little it's better than current, and it may never churn).\n\nWith the current algorithm, it's largely impossible to reduce the slop.  The change would not need to be very intrusive; we would need a bufferpool object for each queue and the create-a-frame code would allocate through there.  Minor side thing to check or deal with - if the video changes resolution in mid-stream, the buffers may have to be dropped and re-allocated (and hold both temporarily).  (Yes, h.264 and probably others can change resolutions in mid-stream.)\n\nWork is contained and not particularly complex though definitely non-trivial.  Win is potentially significant on video depending on the resolution; small win perhaps on performance.\n\nSince the investigation is done, dropping the bug back in the pool for the video people to fight over (or really almost any person comfortable with allocators/etc could work on it).", "raw_text": "Yes, sorry.\n\nThe problem is that the frames are allocated very separately - it's not creating a queue of 10 frames to (re)use, it's allocating and pushing frames into a queue until there are 10, and deallocating when they're consumed.\n\nIn order to fix this, we would need to create a recycling allocator (aka buffer pool) for use for the video frames for a stream and pre-allocate 10 (AMPLE_VIDEO_FRAMES) buffers, in which case we could remove all the slop (except <1MB at the end) and also avoid trips through the allocator on every frame (not a huge win, but a win especially if due to other things interfering (fragmenting/re-using) with the video's allocations it might have to obtain new memory from the OS at times).\n\nAs an optimization, the buffer pool could either be informed when it's not in operation (and drop the memory), or whenever all frames are consumed drop the memory (even if this churns a little it's better than current, and it may never churn).\n\nWith the current algorithm, it's largely impossible to reduce the slop.  The change would not need to be very intrusive; we would need a bufferpool object for each queue and the create-a-frame code would allocate through there.  Minor side thing to check or deal with - if the video changes resolution in mid-stream, the buffers may have to be dropped and re-allocated (and hold both temporarily).  (Yes, h.264 and probably others can change resolutions in mid-stream.)\n\nWork is contained and not particularly complex though definitely non-trivial.  Win is potentially significant on video depending on the resolution; small win perhaps on performance.\n\nSince the investigation is done, dropping the bug back in the pool for the video people to fight over (or really almost any person comfortable with allocators/etc could work on it).", "creation_time": "2012-01-12T20:57:58Z"}, {"creation_time": "2012-01-12T23:22:56Z", "raw_text": "So in the best case this would save us just under 10MB per video, i.e. just under 1MB per decoded frame.  And on Windows and Linux at least, this 10MB is wasted address space but probably has no other cost.\n\nI guess this wouldn't hurt to fix but I think there are plenty of other MemShrink bugs with better effort:benefit ratios.", "attachment_id": null, "time": "2012-01-12T23:22:56Z", "text": "So in the best case this would save us just under 10MB per video, i.e. just under 1MB per decoded frame.  And on Windows and Linux at least, this 10MB is wasted address space but probably has no other cost.\n\nI guess this wouldn't hurt to fix but I think there are plenty of other MemShrink bugs with better effort:benefit ratios.", "id": 5977041, "tags": [], "is_private": false, "count": 16, "bug_id": 680358, "author": "n.nethercote@gmail.com", "creator": "n.nethercote@gmail.com"}, {"bug_id": 680358, "id": 6232108, "is_private": false, "count": 17, "tags": [], "time": "2012-04-18T01:07:26Z", "attachment_id": null, "text": "> So in the best case this would save us just under 10MB per video, i.e. just under 1MB per \n> decoded frame.\n\nActually, it's no more than 4KB per decoded frame.  Although we round the virtual size up to the next MB, the committed size is rounded up to the next page.\n\nAs such, this is not a particularly important bug.", "creation_time": "2012-04-18T01:07:26Z", "raw_text": "> So in the best case this would save us just under 10MB per video, i.e. just under 1MB per \n> decoded frame.\n\nActually, it's no more than 4KB per decoded frame.  Although we round the virtual size up to the next MB, the committed size is rounded up to the next page.\n\nAs such, this is not a particularly important bug.", "author": "justin.lebar+bug@gmail.com", "creator": "justin.lebar+bug@gmail.com"}, {"author": "kinetik@flim.org", "creator": "kinetik@flim.org", "bug_id": 680358, "id": 6753728, "tags": [], "is_private": false, "count": 18, "time": "2012-10-23T02:43:25Z", "attachment_id": null, "text": "We do actually care about how much address space is wasted, so this is still worth fixing at some point.", "creation_time": "2012-10-23T02:43:25Z", "raw_text": "We do actually care about how much address space is wasted, so this is still worth fixing at some point."}, {"tags": [], "is_private": false, "count": 19, "id": 8347141, "bug_id": 680358, "raw_text": "Also, batched allocation and avoiding churn may help reduce VM fragmentation, so perhaps bump up on the radar?", "creation_time": "2014-01-29T15:51:07Z", "text": "Also, batched allocation and avoiding churn may help reduce VM fragmentation, so perhaps bump up on the radar?", "time": "2014-01-29T15:51:07Z", "attachment_id": null, "creator": "rjesup@jesup.org", "author": "rjesup@jesup.org"}, {"creator": "benjamin@smedbergs.us", "author": "benjamin@smedbergs.us", "text": "dmajor, does this bug match up with your previous measurements of HTML <video> decoding causing additional memory usage or fragmentation?", "time": "2014-01-29T15:58:32Z", "attachment_id": null, "raw_text": "dmajor, does this bug match up with your previous measurements of HTML <video> decoding causing additional memory usage or fragmentation?", "creation_time": "2014-01-29T15:58:32Z", "bug_id": 680358, "tags": [], "is_private": false, "count": 20, "id": 8347167}, {"creator": "away@bugmail.cc", "author": "away@bugmail.cc", "bug_id": 680358, "count": 21, "is_private": false, "tags": [], "id": 8348007, "text": "(In reply to Benjamin Smedberg  [:bsmedberg] from comment #20)\n> dmajor, does this bug match up with your previous measurements of HTML\n> <video> decoding causing additional memory usage or fragmentation?\n\nYes. Here's the data from bug 930797 comment 54, scoped to VideoData::Create:\n\nReserve Type, Desired Size (B), Stack, Count, Committed Actual (B), Outstanding Committed (B)\nAIFO, 2097152, xul.dll!mozilla::VideoData::Create, 176, 369098752, 243662848, 369098752, 369098752\nAIFI, , , 66, 162529280, 0, 162529280, 0\n, 2097152, xul.dll!mozilla::VideoData::Create, 43, 90177536, 0, 90177536, 0\n, 3145728, xul.dll!mozilla::VideoData::Create, 23, 72351744, 0, 72351744, 0\n\nHow to read this: At the point of the OOM crash, there were 176 outstanding 2MB allocations from VideoData::Create. That was 369MB originally reserved, of which 243MB is still committed. Drilling down to individual allocations, nearly all were 1348448 committed out of 2097152 reserved -- so about 33% slop.\n\nOver the course of the trace, there were an additional 43 2MB allocations and 23 3MB allocations that were allocated and released before the crash (no data on slop since the blocks are freed).", "time": "2014-01-29T18:20:51Z", "attachment_id": null, "raw_text": "(In reply to Benjamin Smedberg  [:bsmedberg] from comment #20)\n> dmajor, does this bug match up with your previous measurements of HTML\n> <video> decoding causing additional memory usage or fragmentation?\n\nYes. Here's the data from bug 930797 comment 54, scoped to VideoData::Create:\n\nReserve Type, Desired Size (B), Stack, Count, Committed Actual (B), Outstanding Committed (B)\nAIFO, 2097152, xul.dll!mozilla::VideoData::Create, 176, 369098752, 243662848, 369098752, 369098752\nAIFI, , , 66, 162529280, 0, 162529280, 0\n, 2097152, xul.dll!mozilla::VideoData::Create, 43, 90177536, 0, 90177536, 0\n, 3145728, xul.dll!mozilla::VideoData::Create, 23, 72351744, 0, 72351744, 0\n\nHow to read this: At the point of the OOM crash, there were 176 outstanding 2MB allocations from VideoData::Create. That was 369MB originally reserved, of which 243MB is still committed. Drilling down to individual allocations, nearly all were 1348448 committed out of 2097152 reserved -- so about 33% slop.\n\nOver the course of the trace, there were an additional 43 2MB allocations and 23 3MB allocations that were allocated and released before the crash (no data on slop since the blocks are freed).", "creation_time": "2014-01-29T18:20:51Z"}, {"bug_id": 680358, "id": 8348349, "tags": [], "is_private": false, "count": 22, "time": "2014-01-29T19:23:17Z", "attachment_id": null, "text": "A correction and clarification:\n\n(In reply to David Major [:dmajor] from comment #21)\n> That was 369MB originally reserved, of which 243MB is still committed. \nThat wording may be misleading. More like: 396MB *still* reserved, 243MB still committed. So we have lots of ~700KB blocks of VA reserved-but-uncommitted.\n\n> nearly all were 1348448 committed out of 2097152 reserved \nTypo, the committed size should be 1384448 (in case that number has significance to you)", "creation_time": "2014-01-29T19:23:17Z", "raw_text": "A correction and clarification:\n\n(In reply to David Major [:dmajor] from comment #21)\n> That was 369MB originally reserved, of which 243MB is still committed. \nThat wording may be misleading. More like: 396MB *still* reserved, 243MB still committed. So we have lots of ~700KB blocks of VA reserved-but-uncommitted.\n\n> nearly all were 1348448 committed out of 2097152 reserved \nTypo, the committed size should be 1384448 (in case that number has significance to you)", "author": "away@bugmail.cc", "creator": "away@bugmail.cc"}, {"id": 8348376, "tags": [], "count": 23, "is_private": false, "bug_id": 680358, "creation_time": "2014-01-29T19:27:12Z", "raw_text": "> More like: 396MB *still* reserved, 243MB\nSigh. 369MB *still* reserved. Catn tpye todya.", "time": "2014-01-29T19:27:12Z", "attachment_id": null, "text": "> More like: 396MB *still* reserved, 243MB\nSigh. 369MB *still* reserved. Catn tpye todya.", "author": "away@bugmail.cc", "creator": "away@bugmail.cc"}]}}}