{"bugs": {"662134": {"comments": [{"creator": "siarhei.siamashka@gmail.com", "author": "siarhei.siamashka@gmail.com", "bug_id": 662134, "is_private": false, "count": 0, "tags": [], "id": 5513873, "text": "User-Agent:       Mozilla/5.0 (X11; Linux x86_64; rv:2.0.1) Gecko/20110519 Firefox/4.0.1\nBuild Identifier: \n\nThis optimization needs to support two types of rounding:\n    1. \"newcolor = (255 * color) / alpha\"\n    2. \"newcolor = (255 * color + (alpha / 2)) / alpha\"\n\nThe former is needed right now as it's compatible with the existing C code.\n\nThe latter is going to be needed after bug 662130 gets fixed. But it's better to have this optimization readily available right now in order not to block or hinder bug 662130 resolution.\n\nReproducible: Always", "time": "2011-06-05T08:42:33Z", "attachment_id": null, "creation_time": "2011-06-05T08:42:33Z", "raw_text": "User-Agent:       Mozilla/5.0 (X11; Linux x86_64; rv:2.0.1) Gecko/20110519 Firefox/4.0.1\nBuild Identifier: \n\nThis optimization needs to support two types of rounding:\n    1. \"newcolor = (255 * color) / alpha\"\n    2. \"newcolor = (255 * color + (alpha / 2)) / alpha\"\n\nThe former is needed right now as it's compatible with the existing C code.\n\nThe latter is going to be needed after bug 662130 gets fixed. But it's better to have this optimization readily available right now in order not to block or hinder bug 662130 resolution.\n\nReproducible: Always"}, {"creator": "siarhei.siamashka@gmail.com", "author": "siarhei.siamashka@gmail.com", "creation_time": "2011-06-09T07:08:16Z", "raw_text": "", "text": "Created attachment 538198\nunmultiply-tablegen.c - generator for unmultiply lookup tables", "attachment_id": 538198, "time": "2011-06-09T07:08:16Z", "count": 1, "is_private": false, "tags": [], "id": 5521786, "bug_id": 662134}, {"creation_time": "2011-06-09T07:44:48Z", "raw_text": "ARM NEON assembly optimized RGBA unmultiply functions. They support both types of rounding. The functions are generated from a macro template which supports arbitrary color channels layout for source and destination pixel data.\n\nCortex-A8 1GHz (Galaxy Tab):\n    unmultiply_r: small buffer (L1 cache) : 193.82 MPix/s\n    unmultiply_r: large buffer (memory)   : 146.22 MPix/s\n    unmultiply_t: small buffer (L1 cache) : 184.35 MPix/s\n    unmultiply_t: large buffer (memory)   : 143.08 MPix/s\n\nCortex-A9 1GHz (early prototype board with slow RAM):\n    unmultiply_r: small buffer (L1 cache) : 124.46 MPix/s\n    unmultiply_r: large buffer (memory)   : 58.87 MPix/s\n    unmultiply_t: small buffer (L1 cache) : 118.86 MPix/s\n    unmultiply_t: large buffer (memory)   : 59.21 MPix/s\n\nARM Cortex-A9 results are a bit too weak in L1 cached test here. Maybe the primary performance tuning should be done for Cortex-A9. Because it is generally slower than Cortex-A8 per MHz for NEON code and also potentially may have much faster memory bandwidth in newer devices.", "time": "2011-06-09T07:44:48Z", "attachment_id": 538206, "text": "Created attachment 538206\nunmultiply-arm-neon-v1.tar.gz - ARM NEON assembly optimized RGBA unmultiply functions v1\n\nARM NEON assembly optimized RGBA unmultiply functions. They support both types of rounding. The functions are generated from a macro template which supports arbitrary color channels layout for source and destination pixel data.\n\nCortex-A8 1GHz (Galaxy Tab):\n    unmultiply_r: small buffer (L1 cache) : 193.82 MPix/s\n    unmultiply_r: large buffer (memory)   : 146.22 MPix/s\n    unmultiply_t: small buffer (L1 cache) : 184.35 MPix/s\n    unmultiply_t: large buffer (memory)   : 143.08 MPix/s\n\nCortex-A9 1GHz (early prototype board with slow RAM):\n    unmultiply_r: small buffer (L1 cache) : 124.46 MPix/s\n    unmultiply_r: large buffer (memory)   : 58.87 MPix/s\n    unmultiply_t: small buffer (L1 cache) : 118.86 MPix/s\n    unmultiply_t: large buffer (memory)   : 59.21 MPix/s\n\nARM Cortex-A9 results are a bit too weak in L1 cached test here. Maybe the primary performance tuning should be done for Cortex-A9. Because it is generally slower than Cortex-A8 per MHz for NEON code and also potentially may have much faster memory bandwidth in newer devices.", "id": 5521823, "tags": [], "count": 2, "is_private": false, "bug_id": 662134, "author": "siarhei.siamashka@gmail.com", "creator": "siarhei.siamashka@gmail.com"}, {"author": "siarhei.siamashka@gmail.com", "creator": "siarhei.siamashka@gmail.com", "raw_text": "And it's also good to know your enemy :) The current table lookup based C code is in:\n    http://mxr.mozilla.org/mozilla2.0/source/content/canvas/src/nsCanvasRenderingContext2D.cpp#3849\n\nIn the most optimistic scenario and using ARM instructions it needs:\n1 instruction to get alpha channel value in a separate register\n1 instruction to calculate base offset for r/g/b lookups (ARM can't do memory accesses that would involve three registers at once for calculating addresses)\n3 instructions to get r/g/b color components in separate registers\n3 instructions to do table lookups\n4 instructions to write pixel data to the destination, which can be still reduced to 7 instructions per 2 pixels if oring the data and performing 64-bit write with a single instruction\n\nSo in the ideal case, assuming perfect L1 cache hits and even assuming perfect dual issue, that would be no less than ((1 + 1 + 3 + 3 + 3.5) / 2) = 5.75 cycles per pixel.\n\nAnd the practically measured performance for ARM NEON code on ARM Cortex-A8 for L1 cached data already is (1000 MHz / 193.82 MPix/s) = ~5.15 cycles per pixel. So NEON should be faster in all the cases without exception (even when processing some uniform grey color, where the 64K lookup table used by the current C code should cause less problems).", "creation_time": "2011-06-09T08:14:03Z", "attachment_id": null, "time": "2011-06-09T08:14:03Z", "text": "And it's also good to know your enemy :) The current table lookup based C code is in:\n    http://mxr.mozilla.org/mozilla2.0/source/content/canvas/src/nsCanvasRenderingContext2D.cpp#3849\n\nIn the most optimistic scenario and using ARM instructions it needs:\n1 instruction to get alpha channel value in a separate register\n1 instruction to calculate base offset for r/g/b lookups (ARM can't do memory accesses that would involve three registers at once for calculating addresses)\n3 instructions to get r/g/b color components in separate registers\n3 instructions to do table lookups\n4 instructions to write pixel data to the destination, which can be still reduced to 7 instructions per 2 pixels if oring the data and performing 64-bit write with a single instruction\n\nSo in the ideal case, assuming perfect L1 cache hits and even assuming perfect dual issue, that would be no less than ((1 + 1 + 3 + 3 + 3.5) / 2) = 5.75 cycles per pixel.\n\nAnd the practically measured performance for ARM NEON code on ARM Cortex-A8 for L1 cached data already is (1000 MHz / 193.82 MPix/s) = ~5.15 cycles per pixel. So NEON should be faster in all the cases without exception (even when processing some uniform grey color, where the 64K lookup table used by the current C code should cause less problems).", "id": 5521843, "is_private": false, "count": 3, "tags": [], "bug_id": 662134}, {"creator": "jmuizelaar@mozilla.com", "author": "jmuizelaar@mozilla.com", "text": "derf asked me to comment on super-luminescence. Here are my feelings:\n\n- I think the behaviour that makes the most sense is for values to saturate to 255 but pass through when alpha == 0\n- I don't know of anything that depends on particular behaviour so I'm open to changing behaviour for performance reasons. Further, while consistency across platforms would be nice, I'm not even sure that it's critical.", "time": "2011-06-09T17:59:13Z", "attachment_id": null, "raw_text": "derf asked me to comment on super-luminescence. Here are my feelings:\n\n- I think the behaviour that makes the most sense is for values to saturate to 255 but pass through when alpha == 0\n- I don't know of anything that depends on particular behaviour so I'm open to changing behaviour for performance reasons. Further, while consistency across platforms would be nice, I'm not even sure that it's critical.", "creation_time": "2011-06-09T17:59:13Z", "bug_id": 662134, "count": 4, "is_private": false, "tags": [], "id": 5522765}, {"creator": "tterribe@vt.edu", "author": "tterribe@vt.edu", "tags": [], "is_private": false, "count": 5, "id": 5522798, "bug_id": 662134, "raw_text": "(In reply to comment #4)\n> - I don't know of anything that depends on particular behaviour so I'm open\n> to changing behaviour for performance reasons. Further, while consistency\n> across platforms would be nice, I'm not even sure that it's critical.\n\nThe non-NEON code currently uses a lookup table, so I don't see any reason for there to be a difference, since we can put anything we want in the lookup table.\n\nSiarhei, are you up to making a full patch for review?", "creation_time": "2011-06-09T18:09:36Z", "text": "(In reply to comment #4)\n> - I don't know of anything that depends on particular behaviour so I'm open\n> to changing behaviour for performance reasons. Further, while consistency\n> across platforms would be nice, I'm not even sure that it's critical.\n\nThe non-NEON code currently uses a lookup table, so I don't see any reason for there to be a difference, since we can put anything we want in the lookup table.\n\nSiarhei, are you up to making a full patch for review?", "attachment_id": null, "time": "2011-06-09T18:09:36Z"}, {"author": "siarhei.siamashka@gmail.com", "creator": "siarhei.siamashka@gmail.com", "creation_time": "2011-06-09T19:40:42Z", "raw_text": "(In reply to comment #4)\n> derf asked me to comment on super-luminescence. Here are my feelings:\n> \n> - I think the behaviour that makes the most sense is for values to saturate\n> to 255 but pass through when alpha == 0\n\nOK, perfect. This is how it already works. And if any tweaks might be still needed, the tables can be regenerated with the attached 'unmultiply-tablegen.c' program.\n\n(In reply to comment #5)\n> Siarhei, are you up to making a full patch for review?\n\nYes. But I'll still try to do something about the performance on Cortex-A9. Apparently because of the mix of ARM and NEON code here, making it fast for both Cortex-A8 and Cortex-A9 at the same time may be a bit more difficult than usual.", "time": "2011-06-09T19:40:42Z", "attachment_id": null, "text": "(In reply to comment #4)\n> derf asked me to comment on super-luminescence. Here are my feelings:\n> \n> - I think the behaviour that makes the most sense is for values to saturate\n> to 255 but pass through when alpha == 0\n\nOK, perfect. This is how it already works. And if any tweaks might be still needed, the tables can be regenerated with the attached 'unmultiply-tablegen.c' program.\n\n(In reply to comment #5)\n> Siarhei, are you up to making a full patch for review?\n\nYes. But I'll still try to do something about the performance on Cortex-A9. Apparently because of the mix of ARM and NEON code here, making it fast for both Cortex-A8 and Cortex-A9 at the same time may be a bit more difficult than usual.", "id": 5523063, "count": 6, "is_private": false, "tags": [], "bug_id": 662134}, {"count": 7, "is_private": false, "tags": [], "id": 5523123, "bug_id": 662134, "raw_text": "(In reply to comment #6)\n> Yes. But I'll still try to do something about the performance on Cortex-A9.\n> Apparently because of the mix of ARM and NEON code here, making it fast for\n> both Cortex-A8 and Cortex-A9 at the same time may be a bit more difficult\n> than usual.\n\nIt was unclear to me from what documentation I could find if the A9 used a separate instruction decoder for the load/store unit and the NEON unit, which could be a limiting factor. Since basically everything here is a load or a NEON instruction, that would effectively eliminate dual-issuing, and explain the factor of nearly 2 between the A8 and A9 performance.", "creation_time": "2011-06-09T20:02:15Z", "text": "(In reply to comment #6)\n> Yes. But I'll still try to do something about the performance on Cortex-A9.\n> Apparently because of the mix of ARM and NEON code here, making it fast for\n> both Cortex-A8 and Cortex-A9 at the same time may be a bit more difficult\n> than usual.\n\nIt was unclear to me from what documentation I could find if the A9 used a separate instruction decoder for the load/store unit and the NEON unit, which could be a limiting factor. Since basically everything here is a load or a NEON instruction, that would effectively eliminate dual-issuing, and explain the factor of nearly 2 between the A8 and A9 performance.", "time": "2011-06-09T20:02:15Z", "attachment_id": null, "creator": "tterribe@vt.edu", "author": "tterribe@vt.edu"}, {"creator": "Jacob.Bramley@arm.com", "text": "(In reply to comment #7)\n> It was unclear to me from what documentation I could find if the A9\n> used a separate instruction decoder for the load/store unit and the\n> NEON unit, which could be a limiting factor.\n\nA9 can't dual-issue ARM loads or stores with NEON instructions:\nhttp://infocenter.arm.com/help/topic/com.arm.doc.ddi0409f/CHDEDCDC.html\n\n(In reply to comment #7)\n> Since basically everything here is a load or a NEON instruction, that\n> would effectively eliminate dual-issuing, and explain the factor of\n> nearly 2 between the A8 and A9 performance.\n\nSomething to try (though I haven't thought through all the implications)\nis to load blocks of T1-T4 using LDR (rather than several LDRBs), then\nuse bit-field operations to separate the channels. This requires more\ninstructions and requires some scratch registers, but ought to be\nfaster simply by doing away with some loads and allowing ARM/NEON\ndual-issuing:\n\n    LDR         S2, [SRC, #(4 + s_a)]       @ S2 = [off4:off3:off2:off1]\n    ROR         S2, S2, #30                 @ Scale all offsets by 4.\n    UXTB16      S1, S2                      @ S1 = [    off3 :    off1 ]\n    UXTB16      S2, S2, ROR #8              @ S2 = [    off4 :    off2 ]\n\n    @ Some instructions are required to extract the 16-bit offsets, but\n    @ not many. The following could be reasonble:\n\n    ADD         T3, CONSTS_TBL, S1, LSR #16\n    UXTAH       T1, CONSTS_TBL, S1          @ (Add halfword to word.)\n    ADD         T4, CONSTS_TBL, S2, LSR #16\n    UXTAH       T2, CONSTS_TBL, S2          @ (Add halfword to word.)\n\n    VLD1.u32    {d5[0]}, [T3]\n    VLD1.u32    {d4[0]}, [T1]\n    VLD1.u32    {d5[1]}, [T4]\n    VLD1.u32    {d4[1]}, [T2]\n\n(Some pipelining tweaks may be necessary.)\n\nI've also found that A9 benefits from longer-range PLD offsets than A8\ndoes, and tuning those can have surprising results. (In my case, A8\nperformance was not harmed.)", "author": "Jacob.Bramley@arm.com", "attachment_id": null, "time": "2011-06-10T08:32:58Z", "raw_text": "(In reply to comment #7)\n> It was unclear to me from what documentation I could find if the A9\n> used a separate instruction decoder for the load/store unit and the\n> NEON unit, which could be a limiting factor.\n\nA9 can't dual-issue ARM loads or stores with NEON instructions:\nhttp://infocenter.arm.com/help/topic/com.arm.doc.ddi0409f/CHDEDCDC.html\n\n(In reply to comment #7)\n> Since basically everything here is a load or a NEON instruction, that\n> would effectively eliminate dual-issuing, and explain the factor of\n> nearly 2 between the A8 and A9 performance.\n\nSomething to try (though I haven't thought through all the implications)\nis to load blocks of T1-T4 using LDR (rather than several LDRBs), then\nuse bit-field operations to separate the channels. This requires more\ninstructions and requires some scratch registers, but ought to be\nfaster simply by doing away with some loads and allowing ARM/NEON\ndual-issuing:\n\n    LDR         S2, [SRC, #(4 + s_a)]       @ S2 = [off4:off3:off2:off1]\n    ROR         S2, S2, #30                 @ Scale all offsets by 4.\n    UXTB16      S1, S2                      @ S1 = [    off3 :    off1 ]\n    UXTB16      S2, S2, ROR #8              @ S2 = [    off4 :    off2 ]\n\n    @ Some instructions are required to extract the 16-bit offsets, but\n    @ not many. The following could be reasonble:\n\n    ADD         T3, CONSTS_TBL, S1, LSR #16\n    UXTAH       T1, CONSTS_TBL, S1          @ (Add halfword to word.)\n    ADD         T4, CONSTS_TBL, S2, LSR #16\n    UXTAH       T2, CONSTS_TBL, S2          @ (Add halfword to word.)\n\n    VLD1.u32    {d5[0]}, [T3]\n    VLD1.u32    {d4[0]}, [T1]\n    VLD1.u32    {d5[1]}, [T4]\n    VLD1.u32    {d4[1]}, [T2]\n\n(Some pipelining tweaks may be necessary.)\n\nI've also found that A9 benefits from longer-range PLD offsets than A8\ndoes, and tuning those can have surprising results. (In my case, A8\nperformance was not harmed.)", "creation_time": "2011-06-10T08:32:58Z", "bug_id": 662134, "tags": [], "is_private": false, "count": 8, "id": 5524469}, {"creator": "siarhei.siamashka@gmail.com", "author": "siarhei.siamashka@gmail.com", "text": "Created attachment 538470\nunmultiply-arm-neon-v2.tar.gz - ARM NEON assembly optimized RGBA unmultiply functions v2\n\nThis is the current work-in-progress code, it it shows the following results (an improvement for Cortex-A9, some slowdown for Cortex-A8):\n\nARM Cortex-A8 1GHz (Galaxy Tab):\n    unmultiply_r: small buffer (L1 cache) : 184.25 MPix/s\n    unmultiply_r: large buffer (memory)   : 157.46 MPix/s\n    unmultiply_t: small buffer (L1 cache) : 176.78 MPix/s\n    unmultiply_t: large buffer (memory)   : 150.35 MPix/s\n\nARM Cortex-A9 1GHz (early prototype board with slow RAM):\n    unmultiply_r: small buffer (L1 cache) : 152.17 MPix/s\n    unmultiply_r: large buffer (memory)   : 65.84 MPix/s\n    unmultiply_t: small buffer (L1 cache) : 143.95 MPix/s\n    unmultiply_t: large buffer (memory)   : 64.97 MPix/s\n\nI have also added the missing \".baling\" directive for the tables.", "attachment_id": 538470, "time": "2011-06-10T09:37:41Z", "raw_text": "This is the current work-in-progress code, it it shows the following results (an improvement for Cortex-A9, some slowdown for Cortex-A8):\n\nARM Cortex-A8 1GHz (Galaxy Tab):\n    unmultiply_r: small buffer (L1 cache) : 184.25 MPix/s\n    unmultiply_r: large buffer (memory)   : 157.46 MPix/s\n    unmultiply_t: small buffer (L1 cache) : 176.78 MPix/s\n    unmultiply_t: large buffer (memory)   : 150.35 MPix/s\n\nARM Cortex-A9 1GHz (early prototype board with slow RAM):\n    unmultiply_r: small buffer (L1 cache) : 152.17 MPix/s\n    unmultiply_r: large buffer (memory)   : 65.84 MPix/s\n    unmultiply_t: small buffer (L1 cache) : 143.95 MPix/s\n    unmultiply_t: large buffer (memory)   : 64.97 MPix/s\n\nI have also added the missing \".baling\" directive for the tables.", "creation_time": "2011-06-10T09:37:41Z", "bug_id": 662134, "tags": [], "is_private": false, "count": 9, "id": 5524527}, {"creator": "siarhei.siamashka@gmail.com", "author": "siarhei.siamashka@gmail.com", "text": "(In reply to comment #8)\n> (In reply to comment #7)\n> > It was unclear to me from what documentation I could find if the A9\n> > used a separate instruction decoder for the load/store unit and the\n> > NEON unit, which could be a limiting factor.\n> \n> A9 can't dual-issue ARM loads or stores with NEON instructions:\n> http://infocenter.arm.com/help/topic/com.arm.doc.ddi0409f/CHDEDCDC.html\n\nThis is a bit vague. One could interpret the statement \"with the exception of simultaneous loads and stores, the processor can execute VFP and Advanced SIMD instructions in parallel with ARM or Thumb instructions\" as \"ARM load/store instructions can't dual issue with NEON load/store instructions\".\n\nBut mixing ARM and NEON instructions (even without taking load/store instructions into account!) seems to be kind of weird on A9 and I still don't have a complete understanding how it works. A8 is surely a lot more simple and predictable.\n\n> (In reply to comment #7)\n> Something to try (though I haven't thought through all the implications)\n> is to load blocks of T1-T4 using LDR (rather than several LDRBs), then\n> use bit-field operations to separate the channels.\n\nWe only need one 8-bit alpha value from each 32-bit pixel. So this would not work directly. If reducing the number of ARM load instruction is needed, probably using LDM is the way to go.\n\n> I've also found that A9 benefits from longer-range PLD offsets than A8\n> does, and tuning those can have surprising results. (In my case, A8\n> performance was not harmed.)\n\nOptimal prefetch distance is likely memory controller dependent. I have only a preproduction pandaboard EA1, which has known problems with the memory controller and is probably not the best piece of hardware for developing/tuning memory bandwidth sensitive code. But I hope to get http://www.origenboard.org/ soon :)\n\nAlso I noticed that there are probably some cache associativity/aliasing related issues. The performance of this function run over large buffer seems to also depend on the relative alignment of the all 3 memory buffers being accessed (source, destination, lookup table). Maybe modifying the benchmark to use random offsets and averaging the result is the way to more reliable/reproducible benchmark numbers.", "time": "2011-06-10T10:01:38Z", "attachment_id": null, "raw_text": "(In reply to comment #8)\n> (In reply to comment #7)\n> > It was unclear to me from what documentation I could find if the A9\n> > used a separate instruction decoder for the load/store unit and the\n> > NEON unit, which could be a limiting factor.\n> \n> A9 can't dual-issue ARM loads or stores with NEON instructions:\n> http://infocenter.arm.com/help/topic/com.arm.doc.ddi0409f/CHDEDCDC.html\n\nThis is a bit vague. One could interpret the statement \"with the exception of simultaneous loads and stores, the processor can execute VFP and Advanced SIMD instructions in parallel with ARM or Thumb instructions\" as \"ARM load/store instructions can't dual issue with NEON load/store instructions\".\n\nBut mixing ARM and NEON instructions (even without taking load/store instructions into account!) seems to be kind of weird on A9 and I still don't have a complete understanding how it works. A8 is surely a lot more simple and predictable.\n\n> (In reply to comment #7)\n> Something to try (though I haven't thought through all the implications)\n> is to load blocks of T1-T4 using LDR (rather than several LDRBs), then\n> use bit-field operations to separate the channels.\n\nWe only need one 8-bit alpha value from each 32-bit pixel. So this would not work directly. If reducing the number of ARM load instruction is needed, probably using LDM is the way to go.\n\n> I've also found that A9 benefits from longer-range PLD offsets than A8\n> does, and tuning those can have surprising results. (In my case, A8\n> performance was not harmed.)\n\nOptimal prefetch distance is likely memory controller dependent. I have only a preproduction pandaboard EA1, which has known problems with the memory controller and is probably not the best piece of hardware for developing/tuning memory bandwidth sensitive code. But I hope to get http://www.origenboard.org/ soon :)\n\nAlso I noticed that there are probably some cache associativity/aliasing related issues. The performance of this function run over large buffer seems to also depend on the relative alignment of the all 3 memory buffers being accessed (source, destination, lookup table). Maybe modifying the benchmark to use random offsets and averaging the result is the way to more reliable/reproducible benchmark numbers.", "creation_time": "2011-06-10T10:01:38Z", "bug_id": 662134, "tags": [], "is_private": false, "count": 10, "id": 5524558}, {"text": "(In reply to comment #10)\n> > (In reply to comment #7)\n> > Something to try (though I haven't thought through all the implications)\n> > is to load blocks of T1-T4 using LDR (rather than several LDRBs), then\n> > use bit-field operations to separate the channels.\n> \n> We only need one 8-bit alpha value from each 32-bit pixel. So this would not\n> work directly. If reducing the number of ARM load instruction is needed,\n> probably using LDM is the way to go.\n\nAh, sorry, I mis-read your source. It's a shame that we want to index a table with it because we could easily load that pattern using a VLD4 (and there's too much data there for VTBL). A NEON divide (VRECPE/VRECPS) routine is an option and saves having 1KB of look-up table, but it's probably slower than the table solution.\n\n> Also I noticed that there are probably some cache associativity/aliasing\n> related issues. The performance of this function run over large buffer seems\n> to also depend on the relative alignment of the all 3 memory buffers being\n> accessed (source, destination, lookup table). Maybe modifying the benchmark\n> to use random offsets and averaging the result is the way to more\n> reliable/reproducible benchmark numbers.\n\nAgreed. It's tricky to do, though, and emulating the realistic usage is probably not that simple. Some buffers may tend to be aligned, for example, and the existing contents of cache (including evictions at run-time other active threads etc) will differ greatly.", "creator": "Jacob.Bramley@arm.com", "author": "Jacob.Bramley@arm.com", "attachment_id": null, "time": "2011-06-10T10:41:02Z", "raw_text": "(In reply to comment #10)\n> > (In reply to comment #7)\n> > Something to try (though I haven't thought through all the implications)\n> > is to load blocks of T1-T4 using LDR (rather than several LDRBs), then\n> > use bit-field operations to separate the channels.\n> \n> We only need one 8-bit alpha value from each 32-bit pixel. So this would not\n> work directly. If reducing the number of ARM load instruction is needed,\n> probably using LDM is the way to go.\n\nAh, sorry, I mis-read your source. It's a shame that we want to index a table with it because we could easily load that pattern using a VLD4 (and there's too much data there for VTBL). A NEON divide (VRECPE/VRECPS) routine is an option and saves having 1KB of look-up table, but it's probably slower than the table solution.\n\n> Also I noticed that there are probably some cache associativity/aliasing\n> related issues. The performance of this function run over large buffer seems\n> to also depend on the relative alignment of the all 3 memory buffers being\n> accessed (source, destination, lookup table). Maybe modifying the benchmark\n> to use random offsets and averaging the result is the way to more\n> reliable/reproducible benchmark numbers.\n\nAgreed. It's tricky to do, though, and emulating the realistic usage is probably not that simple. Some buffers may tend to be aligned, for example, and the existing contents of cache (including evictions at run-time other active threads etc) will differ greatly.", "creation_time": "2011-06-10T10:41:02Z", "bug_id": 662134, "tags": [], "is_private": false, "count": 11, "id": 5524590}, {"author": "siarhei.siamashka@gmail.com", "creator": "siarhei.siamashka@gmail.com", "time": "2011-06-13T08:06:27Z", "attachment_id": 538826, "text": "Created attachment 538826\nunmultiply-arm-neon-v3.tar.gz - ARM NEON assembly optimized RGBA unmultiply functions v3\n\nChanges since v2:\n1. NEON code optimized to use LDM instruction - better performance\n2. added ARMv6 optimized function implementing 64KB table lookup\n3. big endian compatibility test\n4. \"large buffer\" benchmark now uses random buffer offsets in order to get more or less averaged result and avoid hitting favourable or unfavourable relative buffers alignment corner cases\n\nARM Cortex-A8 1GHz (Galaxy Tab):\n    unmultiply small zero filled buffer (L1 cache, ARMv6)   : 126.43 MPix/s\n    unmultiply small random filled buffer (L1 cache, ARMv6) : 45.56 MPix/s\n    unmultiply small random filled buffer (L1 cache, NEON)  : 197.66 MPix/s\n    unmultiply large zero filled buffer (memory, ARMv6)     : 111.30 MPix/s\n    unmultiply large random filled buffer (memory, NEON)    : 151.14 MPix/s\n\nARM Cortex-A9 1GHz (early prototype board with slow RAM):\n    unmultiply small zero filled buffer (L1 cache, ARMv6)   : 109.98 MPix/s\n    unmultiply small random filled buffer (L1 cache, ARMv6) : 36.85 MPix/s\n    unmultiply small random filled buffer (L1 cache, NEON)  : 151.98 MPix/s\n    unmultiply large zero filled buffer (memory, ARMv6)     : 65.55 MPix/s\n    unmultiply large random filled buffer (memory, NEON)    : 61.68 MPix/s", "creation_time": "2011-06-13T08:06:27Z", "raw_text": "Changes since v2:\n1. NEON code optimized to use LDM instruction - better performance\n2. added ARMv6 optimized function implementing 64KB table lookup\n3. big endian compatibility test\n4. \"large buffer\" benchmark now uses random buffer offsets in order to get more or less averaged result and avoid hitting favourable or unfavourable relative buffers alignment corner cases\n\nARM Cortex-A8 1GHz (Galaxy Tab):\n    unmultiply small zero filled buffer (L1 cache, ARMv6)   : 126.43 MPix/s\n    unmultiply small random filled buffer (L1 cache, ARMv6) : 45.56 MPix/s\n    unmultiply small random filled buffer (L1 cache, NEON)  : 197.66 MPix/s\n    unmultiply large zero filled buffer (memory, ARMv6)     : 111.30 MPix/s\n    unmultiply large random filled buffer (memory, NEON)    : 151.14 MPix/s\n\nARM Cortex-A9 1GHz (early prototype board with slow RAM):\n    unmultiply small zero filled buffer (L1 cache, ARMv6)   : 109.98 MPix/s\n    unmultiply small random filled buffer (L1 cache, ARMv6) : 36.85 MPix/s\n    unmultiply small random filled buffer (L1 cache, NEON)  : 151.98 MPix/s\n    unmultiply large zero filled buffer (memory, ARMv6)     : 65.55 MPix/s\n    unmultiply large random filled buffer (memory, NEON)    : 61.68 MPix/s", "bug_id": 662134, "id": 5528273, "tags": [], "count": 12, "is_private": false}, {"creator": "siarhei.siamashka@gmail.com", "author": "siarhei.siamashka@gmail.com", "bug_id": 662134, "count": 13, "is_private": false, "tags": [], "id": 5529082, "text": "(In reply to comment #11)\n> A NEON divide (VRECPE/VRECPS) routine\n> is an option and saves having 1KB of look-up table, but it's probably slower\n> than the table solution.\n\nYes, VRECPE likely much slower because of the extra int<->float conversion and only 2 operations per cycle throughput. And I updated the code in v3 variant to replace LDRB instructions with LDM/UXTB with some other performance tweaks. A think it's the last update from my side, and even if getting more performance is still possible, it does not look like a low hanging fruit.\n\n1KB table is already better than 64KB. And the use of fast multiplications with a wide 128-bit NEON unit in Cortex-A8/A9 allows to replace 3 table lookups per pixel with just 1, which is an overall good performance win. But this is likely not going to be true for ARM Cortex-A5. And even ARM Cortex-A9 has some performance issue with NEON code.\n\nAppears that Cortex-A9 has a significant penalty if trying to load data from any memory location soon after any large NEON store. It looks like LS unit is busy in the background, processing something like only 32-bits (!) of data per cycle.\n\nJust as an experiment I tried to implement ARMv6 variant of the unmultiply function which uses 64KB table lookup. Appears that A9 also has some unpleasant surprises even for ARM code. First of all, UXTAB instruction is totally useless on A9 because it is really slow (A9 can only execute one UXTAB every ~2 cycles, while A8 can execute two of them in one cycle). The following table contains some data, even though it is not very informative (it's not clear what these numbers actually mean there):\n  http://infocenter.arm.com/help/topic/com.arm.doc.ddi0388f/Chdgjcci.html\nAny instruction using shifted operands are also supposed to be slow (2 cycles of something in the table), but I guess in fact they just have higher latencies internally or require shifted operand earlier in the pipeline. In fact Cortex-A9 can execute up to two UXTB or any other two ALU instructions with shifted operands per cycle, which is the same peak throughput as A8. Still A9 is slower than A8 when running the code from 'table_lookup_alpha_bgra_to_rgba_armv6' function no matter what I tried.\n\n> > Also I noticed that there are probably some cache associativity/aliasing\n> > related issues. The performance of this function run over large buffer seems\n> > to also depend on the relative alignment of the all 3 memory buffers being\n> > accessed (source, destination, lookup table). Maybe modifying the benchmark\n> > to use random offsets and averaging the result is the way to more\n> > reliable/reproducible benchmark numbers.\n> \n> Agreed. It's tricky to do, though, and emulating the realistic usage is\n> probably not that simple. Some buffers may tend to be aligned, for example,\n> and the existing contents of cache (including evictions at run-time other\n> active threads etc) will differ greatly.\n\nWe can't control realistic usage, especially considering that I suspect that  *physical* memory addresses are actually playing some role because the data caches are physically tagged in Cortex-A9:\n  http://infocenter.arm.com/help/topic/com.arm.doc.ddi0388f/Caccifbd.html\nSo I guess the performance may be affected by the level of fragmentation of the memory buffers in physical memory. But in any case, right now I'm more interested in getting just the reproducible benchmark numbers, which do not drift unexpectedly across different runs. Just processing data chunks from random places and averaging the results seems to help a bit.", "time": "2011-06-13T17:20:11Z", "attachment_id": null, "raw_text": "(In reply to comment #11)\n> A NEON divide (VRECPE/VRECPS) routine\n> is an option and saves having 1KB of look-up table, but it's probably slower\n> than the table solution.\n\nYes, VRECPE likely much slower because of the extra int<->float conversion and only 2 operations per cycle throughput. And I updated the code in v3 variant to replace LDRB instructions with LDM/UXTB with some other performance tweaks. A think it's the last update from my side, and even if getting more performance is still possible, it does not look like a low hanging fruit.\n\n1KB table is already better than 64KB. And the use of fast multiplications with a wide 128-bit NEON unit in Cortex-A8/A9 allows to replace 3 table lookups per pixel with just 1, which is an overall good performance win. But this is likely not going to be true for ARM Cortex-A5. And even ARM Cortex-A9 has some performance issue with NEON code.\n\nAppears that Cortex-A9 has a significant penalty if trying to load data from any memory location soon after any large NEON store. It looks like LS unit is busy in the background, processing something like only 32-bits (!) of data per cycle.\n\nJust as an experiment I tried to implement ARMv6 variant of the unmultiply function which uses 64KB table lookup. Appears that A9 also has some unpleasant surprises even for ARM code. First of all, UXTAB instruction is totally useless on A9 because it is really slow (A9 can only execute one UXTAB every ~2 cycles, while A8 can execute two of them in one cycle). The following table contains some data, even though it is not very informative (it's not clear what these numbers actually mean there):\n  http://infocenter.arm.com/help/topic/com.arm.doc.ddi0388f/Chdgjcci.html\nAny instruction using shifted operands are also supposed to be slow (2 cycles of something in the table), but I guess in fact they just have higher latencies internally or require shifted operand earlier in the pipeline. In fact Cortex-A9 can execute up to two UXTB or any other two ALU instructions with shifted operands per cycle, which is the same peak throughput as A8. Still A9 is slower than A8 when running the code from 'table_lookup_alpha_bgra_to_rgba_armv6' function no matter what I tried.\n\n> > Also I noticed that there are probably some cache associativity/aliasing\n> > related issues. The performance of this function run over large buffer seems\n> > to also depend on the relative alignment of the all 3 memory buffers being\n> > accessed (source, destination, lookup table). Maybe modifying the benchmark\n> > to use random offsets and averaging the result is the way to more\n> > reliable/reproducible benchmark numbers.\n> \n> Agreed. It's tricky to do, though, and emulating the realistic usage is\n> probably not that simple. Some buffers may tend to be aligned, for example,\n> and the existing contents of cache (including evictions at run-time other\n> active threads etc) will differ greatly.\n\nWe can't control realistic usage, especially considering that I suspect that  *physical* memory addresses are actually playing some role because the data caches are physically tagged in Cortex-A9:\n  http://infocenter.arm.com/help/topic/com.arm.doc.ddi0388f/Caccifbd.html\nSo I guess the performance may be affected by the level of fragmentation of the memory buffers in physical memory. But in any case, right now I'm more interested in getting just the reproducible benchmark numbers, which do not drift unexpectedly across different runs. Just processing data chunks from random places and averaging the results seems to help a bit.", "creation_time": "2011-06-13T17:20:11Z"}, {"id": 5562372, "tags": [], "is_private": false, "count": 14, "bug_id": 662134, "raw_text": "So, there's less than a week until the next Aurora merge. Any plans on making a reviewable patch out of this proof-of-concept before then?", "creation_time": "2011-06-28T18:55:42Z", "time": "2011-06-28T18:55:42Z", "attachment_id": null, "text": "So, there's less than a week until the next Aurora merge. Any plans on making a reviewable patch out of this proof-of-concept before then?", "author": "tterribe@vt.edu", "creator": "tterribe@vt.edu"}, {"raw_text": "(In reply to comment #14)\n> So, there's less than a week until the next Aurora merge. Any plans on\n> making a reviewable patch out of this proof-of-concept before then?\n\nNo plans. And most likely I will not have any free time until the weekend to work on this, sorry.", "creation_time": "2011-06-28T19:16:19Z", "author": "siarhei.siamashka@gmail.com", "time": "2011-06-28T19:16:19Z", "attachment_id": null, "text": "(In reply to comment #14)\n> So, there's less than a week until the next Aurora merge. Any plans on\n> making a reviewable patch out of this proof-of-concept before then?\n\nNo plans. And most likely I will not have any free time until the weekend to work on this, sorry.", "creator": "siarhei.siamashka@gmail.com", "id": 5562445, "tags": [], "count": 15, "is_private": false, "bug_id": 662134}, {"creator": "george.carstoiu@gmail.com", "author": "george.carstoiu@gmail.com", "text": " Mozilla/5.0 (X11; Linux i686; rv:8.0a1) Gecko/20110707 Firefox/8.0a1\n\nSetting status to New.", "time": "2011-07-08T08:28:30Z", "attachment_id": null, "raw_text": " Mozilla/5.0 (X11; Linux i686; rv:8.0a1) Gecko/20110707 Firefox/8.0a1\n\nSetting status to New.", "creation_time": "2011-07-08T08:28:30Z", "bug_id": 662134, "is_private": false, "count": 16, "tags": [], "id": 5580485}]}}, "comments": {}}