{"comments": {}, "bugs": {"328525": {"comments": [{"count": 0, "creation_time": "2006-02-24T22:52:42Z", "id": 2786857, "time": "2006-02-24T22:52:42Z", "raw_text": "User-Agent:       Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8) Gecko/20051107 Firefox/1.5\nBuild Identifier: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8) Gecko/20051107 Firefox/1.5\n\nThe example page here shows how one can circumvent the cross-domain security checks and load JavaScript code by assigning a URL from another domain to the 'src' attribute of a newly created <script> node. This creates script which \n\nThis seems like a huge security hole to me, but I'm not sure if there are other implications here that would prevent this from being fixed.\n\nThanks!\n\nCheers,\n\n- Bill\n\nReproducible: Always\n\nSteps to Reproduce:\n1. Run the page. It clearly discusses and demonstrates the issue.\n2.\n3.\n\nActual Results:  \nIt works.\n\nExpected Results:  \nI would have expected it not to work.", "creator": "bedney@technicalpursuit.com", "attachment_id": null, "is_private": false, "tags": [], "bug_id": 328525, "author": "bedney@technicalpursuit.com", "text": "User-Agent:       Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8) Gecko/20051107 Firefox/1.5\nBuild Identifier: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8) Gecko/20051107 Firefox/1.5\n\nThe example page here shows how one can circumvent the cross-domain security checks and load JavaScript code by assigning a URL from another domain to the 'src' attribute of a newly created <script> node. This creates script which \n\nThis seems like a huge security hole to me, but I'm not sure if there are other implications here that would prevent this from being fixed.\n\nThanks!\n\nCheers,\n\n- Bill\n\nReproducible: Always\n\nSteps to Reproduce:\n1. Run the page. It clearly discusses and demonstrates the issue.\n2.\n3.\n\nActual Results:  \nIt works.\n\nExpected Results:  \nI would have expected it not to work."}, {"raw_text": "Sorry, my one comment was trunacted:\n\nThis creates script which can then call back to the originator of the call.", "creator": "bedney@technicalpursuit.com", "attachment_id": null, "creation_time": "2006-02-24T22:55:58Z", "time": "2006-02-24T22:55:58Z", "is_private": false, "count": 1, "id": 2786861, "author": "bedney@technicalpursuit.com", "text": "Sorry, my one comment was trunacted:\n\nThis creates script which can then call back to the originator of the call.", "tags": [], "bug_id": 328525}, {"id": 2786895, "count": 2, "bug_id": 328525, "tags": [], "text": "<script src> (dynamic or not) is a well-known and commonly-used exception to the same-origin policy.  Google AdSense, for example, relies on it.  So this bug doesn't need to be confidential.\n\nI agree with some of what Douglas Crockford says on that page about XMLHttpRequest vs. <script src>.  I disagree with some of his strong language -- XMLHttpRequest's *default* same-host restriction is not a \"defective security mechanism\", and the *ability* of sites to use <script src> is not a \"big security hole in browsers\".  But he's right about the need for a way for domains to share data without sharing privileges.  I'd like to see browsers implement an opt-in alternate security model for XMLHttpRequest (this seems to be happening), a way to load scripts containing JSON objects without giving them other abilities, or both.", "author": "jruderman@gmail.com", "time": "2006-02-24T23:24:58Z", "creation_time": "2006-02-24T23:24:58Z", "attachment_id": null, "creator": "jruderman@gmail.com", "raw_text": "<script src> (dynamic or not) is a well-known and commonly-used exception to the same-origin policy.  Google AdSense, for example, relies on it.  So this bug doesn't need to be confidential.\n\nI agree with some of what Douglas Crockford says on that page about XMLHttpRequest vs. <script src>.  I disagree with some of his strong language -- XMLHttpRequest's *default* same-host restriction is not a \"defective security mechanism\", and the *ability* of sites to use <script src> is not a \"big security hole in browsers\".  But he's right about the need for a way for domains to share data without sharing privileges.  I'd like to see browsers implement an opt-in alternate security model for XMLHttpRequest (this seems to be happening), a way to load scripts containing JSON objects without giving them other abilities, or both.", "is_private": false}, {"is_private": false, "raw_text": "Jesse -\n\nNot that I disagree with you on the 'opt-in' policy, in fact given my product line, I'd wholeheartedly endorse it. It's something I've been wanting for years.\n\nBut, it seems really weird to me that we're restricting XMLHttpRequest() down to a gnat's ass, but there's a huge security hole around executing arbitrarily complex chunks of JavaScript?\n\nWow... sorry, I just don't get it.\n\nCheers,\n\n- Bill", "creator": "bedney@technicalpursuit.com", "attachment_id": null, "creation_time": "2006-02-24T23:38:16Z", "time": "2006-02-24T23:38:16Z", "author": "bedney@technicalpursuit.com", "text": "Jesse -\n\nNot that I disagree with you on the 'opt-in' policy, in fact given my product line, I'd wholeheartedly endorse it. It's something I've been wanting for years.\n\nBut, it seems really weird to me that we're restricting XMLHttpRequest() down to a gnat's ass, but there's a huge security hole around executing arbitrarily complex chunks of JavaScript?\n\nWow... sorry, I just don't get it.\n\nCheers,\n\n- Bill", "tags": [], "bug_id": 328525, "count": 3, "id": 2786913}, {"count": 4, "id": 2786923, "author": "dveditz@mozilla.com", "text": "Clearing the confidential flag because the dynodes explanation is already public (but thanks, it got our attention).\n\nThe example link didn't work for me because it's currently trying to load an XHTML document as the script src (crashed IE, gave errors in Firefox 1.5.0.1), but it looks like it should work.\n\nThe warnings from Douglas Crockford apply to varying degrees to any content a webpage loads from another server -- it's got to be a trusted partner. But the danger is to the user's data on the including webpage's domain, so it's in the interest of the webpage to get it right if it wants to keep customers. The XMLHttpRequest restrictions, on the other hand, are there to protect the *user* from a malicious webpage that tries to slurp up data from unrelated sources where the user has already logged in.\n\nWhat we need to make XMLHttpRequest useful is to standardize a way for a webserver to announce \"My data is safe from any domain\" for public data, or \"My data can be retrieved from pages on domains X,Y,Z\" for cooperating sites (Mozilla's SOAP implementation has a similar scheme). The WHAT-WG is working on this.\n\nBut that's off-topic. I don't think there's anyway to prevent this without breaking the web. Too many sites already dynamically document.write() <script> tags for ads and there's little difference in danger between that and adding the nodes through the DOM. In XHTML pages you can't document.write(), adding DOM nodes is all you've got.\n\nSo is this WONTFIX, or have I underestimated the danger?", "tags": [], "bug_id": 328525, "raw_text": "Clearing the confidential flag because the dynodes explanation is already public (but thanks, it got our attention).\n\nThe example link didn't work for me because it's currently trying to load an XHTML document as the script src (crashed IE, gave errors in Firefox 1.5.0.1), but it looks like it should work.\n\nThe warnings from Douglas Crockford apply to varying degrees to any content a webpage loads from another server -- it's got to be a trusted partner. But the danger is to the user's data on the including webpage's domain, so it's in the interest of the webpage to get it right if it wants to keep customers. The XMLHttpRequest restrictions, on the other hand, are there to protect the *user* from a malicious webpage that tries to slurp up data from unrelated sources where the user has already logged in.\n\nWhat we need to make XMLHttpRequest useful is to standardize a way for a webserver to announce \"My data is safe from any domain\" for public data, or \"My data can be retrieved from pages on domains X,Y,Z\" for cooperating sites (Mozilla's SOAP implementation has a similar scheme). The WHAT-WG is working on this.\n\nBut that's off-topic. I don't think there's anyway to prevent this without breaking the web. Too many sites already dynamically document.write() <script> tags for ads and there's little difference in danger between that and adding the nodes through the DOM. In XHTML pages you can't document.write(), adding DOM nodes is all you've got.\n\nSo is this WONTFIX, or have I underestimated the danger?", "creator": "dveditz@mozilla.com", "attachment_id": null, "creation_time": "2006-02-24T23:47:24Z", "time": "2006-02-24T23:47:24Z", "is_private": false}, {"is_private": false, "creation_time": "2006-02-24T23:52:38Z", "time": "2006-02-24T23:52:38Z", "raw_text": "Comment 4 paragraph 3 is key.  And yes, this should be wontfix (or dup; I bet someone's \"found\" this before).", "creator": "bzbarsky@mit.edu", "attachment_id": null, "tags": [], "bug_id": 328525, "author": "bzbarsky@mit.edu", "text": "Comment 4 paragraph 3 is key.  And yes, this should be wontfix (or dup; I bet someone's \"found\" this before).", "count": 5, "id": 2786930}, {"raw_text": "I should add that if a site routinely served up sensitive data as predigested javascript objects at predictable URLs then it would indeed be at risk from this. But responsible sites don't do this, because they know <script src=> can be from anywhere. Like it or not, that came about too early in the WWW to change now.\n\nXMLHttpRequest examples where it's too restrictive: 3rd party pages can't create \"mashups\" of safe data such as Amazon book listings, or Bugzilla bug data. Example of why we have the restrictions: I don't want any random site to be able to read my GMail.", "attachment_id": null, "creator": "dveditz@mozilla.com", "count": 6, "time": "2006-02-24T23:56:10Z", "creation_time": "2006-02-24T23:56:10Z", "id": 2786931, "author": "dveditz@mozilla.com", "text": "I should add that if a site routinely served up sensitive data as predigested javascript objects at predictable URLs then it would indeed be at risk from this. But responsible sites don't do this, because they know <script src=> can be from anywhere. Like it or not, that came about too early in the WWW to change now.\n\nXMLHttpRequest examples where it's too restrictive: 3rd party pages can't create \"mashups\" of safe data such as Amazon book listings, or Bugzilla bug data. Example of why we have the restrictions: I don't want any random site to be able to read my GMail.", "is_private": false, "tags": [], "bug_id": 328525}, {"tags": [], "bug_id": 328525, "author": "dveditz@mozilla.com", "text": "Can't find a dupe", "count": 7, "id": 2786936, "is_private": false, "time": "2006-02-25T00:02:16Z", "creation_time": "2006-02-25T00:02:16Z", "raw_text": "Can't find a dupe", "attachment_id": null, "creator": "dveditz@mozilla.com"}, {"is_private": false, "time": "2006-02-25T00:07:22Z", "creation_time": "2006-02-25T00:07:22Z", "attachment_id": null, "creator": "bedney@technicalpursuit.com", "raw_text": "Guys -\n\nOk. Thanks for the review. Glad you didn't find a dupe as I looked on Bugzilla for quite a while first. That would have been embarrassing :-).\n\nCheers,\n\n- Bill", "bug_id": 328525, "tags": [], "text": "Guys -\n\nOk. Thanks for the review. Glad you didn't find a dupe as I looked on Bugzilla for quite a while first. That would have been embarrassing :-).\n\nCheers,\n\n- Bill", "author": "bedney@technicalpursuit.com", "id": 2786944, "count": 8}]}}}