{"bugs": {"1630665": {"comments": [{"id": 14759432, "text": "This bug is for determining a standard for passing data from the test environment to the metrics environment. I propose we use this format, using jsonschema to validate that the results given agree with it:\n```\n## Full result view - it's a \"partial\" perfherder data blob\n\n[\n\t{ ## Suite 1\n\t\tRequired(\"results\"): <filepath>/<see below - dictionary of values with keys naming the subtest, and values are replicates>,\n\t\tOptional(\"shouldAlert\"): True/False or dict containing a mapping of metrics to shouldAlert values,\n\t\tOptional(\"alertThreshold\"): 2.0,\n\t\tOptional(\"name\"): \"browsertime\",\n\t\tOptional(\"extraOptions\"): [],\n\t\tOptional(\"lowerIsBetter\"): True/False,\n\t\tOptional(\"transformer\"): \"SingleJsonRetriever\"\n\t},\n\t{ ## Suite 2\n\t\t...\n\t}\n]\n\n## \"results\" entry as a dictionary\n\nMeasurement name and replicates are required in this format, all other fields are optional.\n\n{\n\t\"results\": {\n\t\t\"measurement-name-1\": {\n\t\t\tRequired(\"replicates\"): [0,1,2,...],\n\t\t\tOptional(\"unit\"): \"ms\",\n\t\t\tOptional(<more-subtest-perfherder-settings>)...\n\t\t},\n\t\t\"measurement-name-2\": {\n\t\t\t...\n\t\t}\n\t}\n}\n```\n\n:tarek, what do you think about this format? Essentially, the test environment would be passing the metrics env some \"partial\" perfherder data blobs which may possibly contain settings for perfherder. If there are no special settings we use the default ones we have.\n\nWith this, it'll make it possible for test environments to build perfherder data blobs that is specific to them. Furthermore, you'll notice that I added a `transformer` field. This will be used to determine how the results should be opened, parsed, and merged. The SingleJsonRetriever will be able to handle the majority of these transforms, but for cases like the ADB logcat parsing, we will need to use a different one: https://dxr.mozilla.org/mozilla-central/source/python/mozperftest/mozperftest/metrics/notebook/customtransforms/custom_single_json_retriever.py", "creation_time": "2020-04-16T16:19:05Z", "creator": "gmierz2@outlook.com", "attachment_id": null, "author": "gmierz2@outlook.com", "count": 0, "is_private": false, "tags": [], "bug_id": 1630665, "raw_text": "This bug is for determining a standard for passing data from the test environment to the metrics environment. I propose we use this format, using jsonschema to validate that the results given agree with it:\n```\n## Full result view - it's a \"partial\" perfherder data blob\n\n[\n\t{ ## Suite 1\n\t\tRequired(\"results\"): <filepath>/<see below - dictionary of values with keys naming the subtest, and values are replicates>,\n\t\tOptional(\"shouldAlert\"): True/False or dict containing a mapping of metrics to shouldAlert values,\n\t\tOptional(\"alertThreshold\"): 2.0,\n\t\tOptional(\"name\"): \"browsertime\",\n\t\tOptional(\"extraOptions\"): [],\n\t\tOptional(\"lowerIsBetter\"): True/False,\n\t\tOptional(\"transformer\"): \"SingleJsonRetriever\"\n\t},\n\t{ ## Suite 2\n\t\t...\n\t}\n]\n\n## \"results\" entry as a dictionary\n\nMeasurement name and replicates are required in this format, all other fields are optional.\n\n{\n\t\"results\": {\n\t\t\"measurement-name-1\": {\n\t\t\tRequired(\"replicates\"): [0,1,2,...],\n\t\t\tOptional(\"unit\"): \"ms\",\n\t\t\tOptional(<more-subtest-perfherder-settings>)...\n\t\t},\n\t\t\"measurement-name-2\": {\n\t\t\t...\n\t\t}\n\t}\n}\n```\n\n:tarek, what do you think about this format? Essentially, the test environment would be passing the metrics env some \"partial\" perfherder data blobs which may possibly contain settings for perfherder. If there are no special settings we use the default ones we have.\n\nWith this, it'll make it possible for test environments to build perfherder data blobs that is specific to them. Furthermore, you'll notice that I added a `transformer` field. This will be used to determine how the results should be opened, parsed, and merged. The SingleJsonRetriever will be able to handle the majority of these transforms, but for cases like the ADB logcat parsing, we will need to use a different one: https://dxr.mozilla.org/mozilla-central/source/python/mozperftest/mozperftest/metrics/notebook/customtransforms/custom_single_json_retriever.py", "time": "2020-04-16T16:19:05Z"}, {"text": "This looks great! We should also make sure that we don't restrict the results to perfherder metrics, with all the notions it carries around, like replicates, alert threshold.\n\nIn other words, each metrics module should be able to consume a data set that follows a given standard, and the browser env should be able to produce those data sets. Which makes me think that maybe the browser module could be automatically paired with the right metrics modules, given the datasets it produces.\n\nTo summarize, I think there's a lot of value to have schemas declared separately from implementations, with a unique name from them.\nand have the metrics module validate the data against the schema before it uses.\n\nSo the flow :\n\n```\nbrowser_test() => browsertime data => perfherder_converter() => dataset for perfherder => perfherder metrics()\nbrowser_test() => adb logcat data => perfherder_converter() => dataset for perfherder => perfherder metrics()\nbrowser_test() => browsertime data=> r_converter() => dataset for \"R\" => R_metrics()\n```\nNow the question I have is, where do we want the converters to live? in the browser env or the metrics env ?\nOr maybe that's a new layer called ETL ?", "id": 14759643, "creator": "tarek@mozilla.com", "attachment_id": null, "creation_time": "2020-04-16T17:59:29Z", "bug_id": 1630665, "tags": [], "is_private": false, "count": 1, "author": "tarek@mozilla.com", "time": "2020-04-16T17:59:29Z", "raw_text": "This looks great! We should also make sure that we don't restrict the results to perfherder metrics, with all the notions it carries around, like replicates, alert threshold.\n\nIn other words, each metrics module should be able to consume a data set that follows a given standard, and the browser env should be able to produce those data sets. Which makes me think that maybe the browser module could be automatically paired with the right metrics modules, given the datasets it produces.\n\nTo summarize, I think there's a lot of value to have schemas declared separately from implementations, with a unique name from them.\nand have the metrics module validate the data against the schema before it uses.\n\nSo the flow :\n\n```\nbrowser_test() => browsertime data => perfherder_converter() => dataset for perfherder => perfherder metrics()\nbrowser_test() => adb logcat data => perfherder_converter() => dataset for perfherder => perfherder metrics()\nbrowser_test() => browsertime data=> r_converter() => dataset for \"R\" => R_metrics()\n```\nNow the question I have is, where do we want the converters to live? in the browser env or the metrics env ?\nOr maybe that's a new layer called ETL ?"}, {"tags": [], "bug_id": 1630665, "is_private": false, "count": 2, "id": 14759778, "text": "I find that the notebook code actually provides all of this already - it more or less uses an ETL process. It extracts data from some given files/directories, and then transforms them into the standardized data, and finally saves and returns that standardized data for further processing.\n\nThe converter's that you're talking about are actually the transformers in the notebook. Using the `transformer` field we can tell the notebook how to extract data (which is essentially what the converter will do). You'll notice that the only field that is required in the schema above is the `results` field, they don't need to provide the perfherder specific stuff (we can provide defaults if they need it).\n\nThis is the flow I'm thinking of:\n```\nbrowser_test()\n     => data: [browsertime_data: SingleJsonRetrieverTransform, logcat_data: LogcatTransform]\n     => any_metrics_module()\n     => call CommonMetricsSingleton.get_standardized_data()  - here, notebook uses the specified transformers to do the conversion of what's stored in \"results\"\n     => metrics module does something with the data (Perfherder, ConsoleOutput, RStudioNotebook, IodideNotebook)\n```\n\nI think this flow is a bit more contained, and would lead to less duplicated code. It's a bit more restrictive, but that could be a bonus for us if we're trying to prevent \"garbage\" from making it's way into the tool. I agree with changing the names we use for some of the fields like using \"values\" instead of \"replicates\".", "author": "gmierz2@outlook.com", "time": "2020-04-16T19:09:23Z", "creator": "gmierz2@outlook.com", "attachment_id": null, "creation_time": "2020-04-16T19:09:23Z", "raw_text": "I find that the notebook code actually provides all of this already - it more or less uses an ETL process. It extracts data from some given files/directories, and then transforms them into the standardized data, and finally saves and returns that standardized data for further processing.\n\nThe converter's that you're talking about are actually the transformers in the notebook. Using the `transformer` field we can tell the notebook how to extract data (which is essentially what the converter will do). You'll notice that the only field that is required in the schema above is the `results` field, they don't need to provide the perfherder specific stuff (we can provide defaults if they need it).\n\nThis is the flow I'm thinking of:\n```\nbrowser_test()\n     => data: [browsertime_data: SingleJsonRetrieverTransform, logcat_data: LogcatTransform]\n     => any_metrics_module()\n     => call CommonMetricsSingleton.get_standardized_data()  - here, notebook uses the specified transformers to do the conversion of what's stored in \"results\"\n     => metrics module does something with the data (Perfherder, ConsoleOutput, RStudioNotebook, IodideNotebook)\n```\n\nI think this flow is a bit more contained, and would lead to less duplicated code. It's a bit more restrictive, but that could be a bonus for us if we're trying to prevent \"garbage\" from making it's way into the tool. I agree with changing the names we use for some of the fields like using \"values\" instead of \"replicates\"."}, {"creator": "tarek@mozilla.com", "attachment_id": null, "creation_time": "2020-04-21T16:57:00Z", "text": "I like it. I think we should have the mechansim extracted off the notebook/ package though.", "id": 14769550, "time": "2020-04-21T16:57:00Z", "raw_text": "I like it. I think we should have the mechansim extracted off the notebook/ package though.", "is_private": false, "count": 3, "bug_id": 1630665, "tags": [], "author": "tarek@mozilla.com"}, {"author": "gmierz2@outlook.com", "count": 4, "is_private": false, "tags": [], "bug_id": 1630665, "raw_text": "We discussed this further over Riot, and everything requested is already in place. The standards in the description comment are agreed upon and I'll start implementing the mechanisms for us to handle this format. A TODO for this will be to document all of this in a wiki.", "time": "2020-04-21T17:07:31Z", "text": "We discussed this further over Riot, and everything requested is already in place. The standards in the description comment are agreed upon and I'll start implementing the mechanisms for us to handle this format. A TODO for this will be to document all of this in a wiki.", "id": 14769763, "creation_time": "2020-04-21T17:07:31Z", "creator": "gmierz2@outlook.com", "attachment_id": null}, {"bug_id": 1630665, "tags": [], "count": 5, "is_private": false, "author": "gmierz2@outlook.com", "time": "2020-04-22T22:27:25Z", "raw_text": "\nThis patch implements the new intermediate results standard and adds the mechanisms required to handle it. Results validation is done with jsonschema and some manual validation (because of some unfortunate issues with jsonschema) and some tests were implemented to ensure that we fail/pass where expected. The metrics modules were modified to handle multiple suites.\n\nOne thing that is disabled in this patch is the subtest/single-metric specifications through the \"results\" field. We'll do one thing at a time here and we also have no use for subtests yet (although we definitely will).", "text": "Created attachment 9142570\nBug 1630665 - Implement new browser intermediate results standard. r?tarek\n\n\nThis patch implements the new intermediate results standard and adds the mechanisms required to handle it. Results validation is done with jsonschema and some manual validation (because of some unfortunate issues with jsonschema) and some tests were implemented to ensure that we fail/pass where expected. The metrics modules were modified to handle multiple suites.\n\nOne thing that is disabled in this patch is the subtest/single-metric specifications through the \"results\" field. We'll do one thing at a time here and we also have no use for subtests yet (although we definitely will).", "id": 14772869, "creator": "gmierz2@outlook.com", "attachment_id": 9142570, "creation_time": "2020-04-22T22:27:25Z"}, {"author": "pulsebot@bots.tld", "count": 6, "is_private": false, "bug_id": 1630665, "tags": [], "raw_text": "Pushed by gmierz2@outlook.com:\nhttps://hg.mozilla.org/integration/autoland/rev/bb84825b3299\nImplement new browser intermediate results standard. r=tarek", "time": "2020-05-04T16:46:19Z", "text": "Pushed by gmierz2@outlook.com:\nhttps://hg.mozilla.org/integration/autoland/rev/bb84825b3299\nImplement new browser intermediate results standard. r=tarek", "id": 14796117, "creation_time": "2020-05-04T16:46:19Z", "attachment_id": null, "creator": "pulsebot@bots.tld"}, {"time": "2020-05-04T20:56:18Z", "raw_text": "https://hg.mozilla.org/mozilla-central/rev/bb84825b3299", "count": 7, "is_private": false, "bug_id": 1630665, "tags": ["bugherder"], "author": "ccoroiu@mozilla.com", "attachment_id": null, "creator": "ccoroiu@mozilla.com", "creation_time": "2020-05-04T20:56:18Z", "text": "https://hg.mozilla.org/mozilla-central/rev/bb84825b3299", "id": 14796584}]}}, "comments": {}}