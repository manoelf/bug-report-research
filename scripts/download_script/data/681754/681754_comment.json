{"comments": {}, "bugs": {"681754": {"comments": [{"bug_id": 681754, "id": 5676012, "creation_time": "2011-08-24T20:55:33Z", "is_private": false, "creator": "jonathan.protzenko@gmail.com", "count": 0, "text": "(Spin-off of bug 554033 where we ended up implementing the cheap solution, namely bumping the minimum token length from two to three (unless the token is a cjk bigram).)\n\nOur tokenizer does not currently have any stopword support, so we index extremely common words like \"the\", \"in\", \"of\", etc.  Yes, that's right, we also currently emit 2-letter tokens for non-CJK stuff too.\n\nThat the tokenizer will be presented with multiple languages but have no idea what language it is looking at complicates things.  The user's locale could provide some insight into the probability of certain things being stopwords.\n\nMitigation could fall into two cases:\n1) Full stopword consumption.  Does not get emitted at all.\n2) Escalation to bi-gram.  We don't emit the term, but we do emit the term and what follows it.  For example, \"mit\" is a german stopword but is also the commonly used acronym for a college (let's ignore that it would be upper-cased in most cases in that context for now).  Assuming \"mit\" gets the bi-gram escalation flag and we see the phrase \"mit campus\", we would literally emit that as our token.  (We would also emit campus separately.)\n\nThe other major complication is that the introduction of stop-words complicates our query building somewhat.  As in bug 549594, if the tokenizer is eating tokens, it may cause the boolean logic we are using to end up (vacuously) false because of a gobbled token.  This demands that we expose the tokenizer in some manner to XPCOM.\n\n(Note: bi-gram escalation would likely also require some explicit support in the XPCOM exposure.  For example, we would want to know that \"mit\" is an escalating stop-word so that we could include permutations of it and the other terms.  As per the 'MIT campus' example above, we might want 'campus MIT' to also get results which would require that parameterization.\n\nAny improvements to this problem are better than no improvements, so even if we can't address the multilingual case out of the gate, removing ridiculously common english stop-words as well as eliminating 2-character non-CJK tokens (to help out other languages too) is probably a great way to go.", "author": "jonathan.protzenko@gmail.com", "tags": [], "time": "2011-08-24T20:55:33Z", "raw_text": "(Spin-off of bug 554033 where we ended up implementing the cheap solution, namely bumping the minimum token length from two to three (unless the token is a cjk bigram).)\n\nOur tokenizer does not currently have any stopword support, so we index extremely common words like \"the\", \"in\", \"of\", etc.  Yes, that's right, we also currently emit 2-letter tokens for non-CJK stuff too.\n\nThat the tokenizer will be presented with multiple languages but have no idea what language it is looking at complicates things.  The user's locale could provide some insight into the probability of certain things being stopwords.\n\nMitigation could fall into two cases:\n1) Full stopword consumption.  Does not get emitted at all.\n2) Escalation to bi-gram.  We don't emit the term, but we do emit the term and what follows it.  For example, \"mit\" is a german stopword but is also the commonly used acronym for a college (let's ignore that it would be upper-cased in most cases in that context for now).  Assuming \"mit\" gets the bi-gram escalation flag and we see the phrase \"mit campus\", we would literally emit that as our token.  (We would also emit campus separately.)\n\nThe other major complication is that the introduction of stop-words complicates our query building somewhat.  As in bug 549594, if the tokenizer is eating tokens, it may cause the boolean logic we are using to end up (vacuously) false because of a gobbled token.  This demands that we expose the tokenizer in some manner to XPCOM.\n\n(Note: bi-gram escalation would likely also require some explicit support in the XPCOM exposure.  For example, we would want to know that \"mit\" is an escalating stop-word so that we could include permutations of it and the other terms.  As per the 'MIT campus' example above, we might want 'campus MIT' to also get results which would require that parameterization.\n\nAny improvements to this problem are better than no improvements, so even if we can't address the multilingual case out of the gate, removing ridiculously common english stop-words as well as eliminating 2-character non-CJK tokens (to help out other languages too) is probably a great way to go.", "attachment_id": null}, {"creator": "vseerror@lehigh.edu", "creation_time": "2012-01-23T17:30:36Z", "is_private": false, "attachment_id": null, "tags": [], "author": "vseerror@lehigh.edu", "count": 1, "text": "protz, anyone other than you who might easily jump on this and bug 549594?\nthis could help older laptops be more performant. (including mine)", "bug_id": 681754, "id": 6001052, "raw_text": "protz, anyone other than you who might easily jump on this and bug 549594?\nthis could help older laptops be more performant. (including mine)", "time": "2012-01-23T17:30:36Z"}, {"attachment_id": null, "raw_text": "Well if you could get an intern to work on this, I could most definitely provide guidance. Asuth could fix it, but I'm pretty sure he's got better things to do. Apart from that, no, I can't see anyone else who's proficient with the gloda code. Squib could do it for sure, he's very good, but I'm not sure that's his area of interest...\n\n(hope that answers your question)", "time": "2012-01-23T18:40:42Z", "tags": [], "author": "jonathan.protzenko@gmail.com", "count": 2, "text": "Well if you could get an intern to work on this, I could most definitely provide guidance. Asuth could fix it, but I'm pretty sure he's got better things to do. Apart from that, no, I can't see anyone else who's proficient with the gloda code. Squib could do it for sure, he's very good, but I'm not sure that's his area of interest...\n\n(hope that answers your question)", "creator": "jonathan.protzenko@gmail.com", "is_private": false, "creation_time": "2012-01-23T18:40:42Z", "id": 6001311, "bug_id": 681754}, {"tags": [], "attachment_id": null, "raw_text": "any crude estimate range of what this might buy us?  \n15% improvement in indexing speed or reduced index space?", "time": "2012-07-16T20:41:47Z", "creation_time": "2012-07-16T20:41:47Z", "is_private": false, "bug_id": 681754, "id": 6477386, "author": "vseerror@lehigh.edu", "count": 3, "text": "any crude estimate range of what this might buy us?  \n15% improvement in indexing speed or reduced index space?", "creator": "vseerror@lehigh.edu"}, {"tags": [], "time": "2012-07-16T20:49:53Z", "attachment_id": null, "raw_text": "This would definitely buy us some index space, a few percents maybe.", "bug_id": 681754, "id": 6477424, "is_private": false, "creation_time": "2012-07-16T20:49:53Z", "creator": "jonathan.protzenko@gmail.com", "author": "jonathan.protzenko@gmail.com", "count": 4, "text": "This would definitely buy us some index space, a few percents maybe."}]}}}