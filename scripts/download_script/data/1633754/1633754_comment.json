{"bugs": {"1633754": {"comments": [{"bug_id": 1633754, "attachment_id": null, "is_private": false, "text": "The pipeline that creates the data for the public anomaly table currently costs about $160 a day in BigQuery costs.  Most of this could be saved if the method was made incremental, especially caching the forecast models rather than recomputing them using historical MAU each day.  This will add complexity beyond what I feel it is reasonable for Data Science to maintain, so would recommend that Data Engineering take ownership of the pipeline.  I'm happy to assist of course.\n\nThe pipeline is here: https://dbc-caf9527b-e073.cloud.databricks.com/#notebook/271173/  It contains a service account key so will only make it available as-needed.  Please contact me for access.\n\nMostly it just calls code in here: https://github.com/mozilla/dscontrib/tree/master/src/dscontrib/jmccrosky/anomdtct", "raw_text": "The pipeline that creates the data for the public anomaly table currently costs about $160 a day in BigQuery costs.  Most of this could be saved if the method was made incremental, especially caching the forecast models rather than recomputing them using historical MAU each day.  This will add complexity beyond what I feel it is reasonable for Data Science to maintain, so would recommend that Data Engineering take ownership of the pipeline.  I'm happy to assist of course.\n\nThe pipeline is here: https://dbc-caf9527b-e073.cloud.databricks.com/#notebook/271173/  It contains a service account key so will only make it available as-needed.  Please contact me for access.\n\nMostly it just calls code in here: https://github.com/mozilla/dscontrib/tree/master/src/dscontrib/jmccrosky/anomdtct", "author": "jmccrosky@mozilla.com", "id": 14783963, "tags": [], "count": 0, "time": "2020-04-28T14:30:06Z", "creation_time": "2020-04-28T14:30:06Z", "creator": "jmccrosky@mozilla.com"}, {"is_private": false, "text": "Jesse can I get access to the notebook? \nLooking at the github repo I think the setup for the productionized pipeline will end up being similar to the forecasting models one [1]. I'd create a separate repository for the pipeline that'll also provide a docker configuration for an image that can easily be scheduled via Airflow and run the query incrementally.\n\n[1] https://github.com/mozilla/forecasting/tree/master/simpleprophet", "raw_text": "Jesse can I get access to the notebook? \nLooking at the github repo I think the setup for the productionized pipeline will end up being similar to the forecasting models one [1]. I'd create a separate repository for the pipeline that'll also provide a docker configuration for an image that can easily be scheduled via Airflow and run the query incrementally.\n\n[1] https://github.com/mozilla/forecasting/tree/master/simpleprophet", "id": 14787228, "author": "ascholtz@mozilla.com", "bug_id": 1633754, "attachment_id": null, "creation_time": "2020-04-29T21:50:11Z", "creator": "ascholtz@mozilla.com", "tags": [], "count": 1, "time": "2020-04-29T21:50:11Z"}, {"creation_time": "2020-04-30T04:37:47Z", "creator": "jmccrosky@mozilla.com", "time": "2020-04-30T04:37:47Z", "tags": [], "count": 2, "id": 14787734, "author": "jmccrosky@mozilla.com", "is_private": false, "text": "Hi Anna, I've granted the access.  Thanks for looking at this.\n\nI'll note that the code for generating the forecasting models is currently interleaved with the rest and will need to be extracted in order to just fit the models once (and read the entire history of our metrics once) rather than every day.  I think this should be quite trivial, but feel free to reach out if you want me to help.\n\nAnd the notebook currently writes to the Google Sheet endpoint as well.  If possible, I'd like to maintain this as some users of the data may be using it.  If it's difficult to move this to AirFlow, I can maintain a notebook that just reads the public BQ table and writes it to the Google Sheet.", "raw_text": "Hi Anna, I've granted the access.  Thanks for looking at this.\n\nI'll note that the code for generating the forecasting models is currently interleaved with the rest and will need to be extracted in order to just fit the models once (and read the entire history of our metrics once) rather than every day.  I think this should be quite trivial, but feel free to reach out if you want me to help.\n\nAnd the notebook currently writes to the Google Sheet endpoint as well.  If possible, I'd like to maintain this as some users of the data may be using it.  If it's difficult to move this to AirFlow, I can maintain a notebook that just reads the public BQ table and writes it to the Google Sheet.", "attachment_id": null, "bug_id": 1633754}, {"tags": [], "count": 3, "time": "2020-05-01T21:57:09Z", "creation_time": "2020-05-01T21:57:09Z", "creator": "pulgasaur@mozilla.bugs", "bug_id": 1633754, "attachment_id": 9145124, "is_private": false, "text": "Created attachment 9145124\nLink to GitHub pull-request: https://github.com/mozilla/bigquery-etl/pull/949", "raw_text": "", "author": "pulgasaur@mozilla.bugs", "id": 14792046}, {"id": 14803233, "author": "ascholtz@mozilla.com", "is_private": false, "text": "Created attachment 9146547\nGitHub Pull Request", "raw_text": "", "attachment_id": 9146547, "bug_id": 1633754, "creation_time": "2020-05-07T18:58:20Z", "creator": "ascholtz@mozilla.com", "time": "2020-05-07T18:58:20Z", "tags": [], "count": 4}, {"tags": [], "count": 5, "time": "2020-05-07T19:00:11Z", "creation_time": "2020-05-07T19:00:11Z", "creator": "ascholtz@mozilla.com", "bug_id": 1633754, "attachment_id": 9146548, "is_private": false, "raw_text": "", "text": "Created attachment 9146548\nGitHub Pull Request", "author": "ascholtz@mozilla.com", "id": 14803239}]}}, "comments": {}}