{"comments": {}, "bugs": {"708436": {"comments": [{"author": "jduell.mcbugs@gmail.com", "creator": "jduell.mcbugs@gmail.com", "time": "2011-12-07T23:11:41Z", "attachment_id": null, "text": "The idea is to get telemetry on what the average working set size for the HTTP cache is, for some definition(s) of \"working set\".   For instance, if we define working set as \"items read from cache within the last week\", we'd get some telemetry to indicate that for user N that took up 120 MB of cache (everything else in their cache has been accessed less recently than a week).  The numbers will obviously vary widely between users, but getting stats on this would be useful.  If 95% of users aren't using more than 500 MB of cache in their working set, that's a good argument for not gobbling up 1 GB.  \n\nIdeally we could get stats for several different working sets (last 24 hours, last week: or perhaps some metric based on browser uptime, i.e. \"accessed within last 5 hours browser has been running.\"  I'm not sure how to get stats on uptime.\n\nLogically, we have the information we need here (it's just cache items sorted by LRU), but I'm not so confident we have an efficient way to get it.  I'm hoping that Michal or Bjarne or Nick have some idea of when we could gather this info without incurrent too much cost.\n\nOne idea is that we could get a statistical sample for this info from accumulating LRU data as we scan cache items when we're doing evictions: that might be a good way to free-ride on existing computation (though it would mean we only get data from users who have full caches.  That's probably fine).  I'm assuming that when we scan for evictions, we're getting random sample of items in the cache (i.e. they're not already sorted by LRU in any way)--true?", "raw_text": "The idea is to get telemetry on what the average working set size for the HTTP cache is, for some definition(s) of \"working set\".   For instance, if we define working set as \"items read from cache within the last week\", we'd get some telemetry to indicate that for user N that took up 120 MB of cache (everything else in their cache has been accessed less recently than a week).  The numbers will obviously vary widely between users, but getting stats on this would be useful.  If 95% of users aren't using more than 500 MB of cache in their working set, that's a good argument for not gobbling up 1 GB.  \n\nIdeally we could get stats for several different working sets (last 24 hours, last week: or perhaps some metric based on browser uptime, i.e. \"accessed within last 5 hours browser has been running.\"  I'm not sure how to get stats on uptime.\n\nLogically, we have the information we need here (it's just cache items sorted by LRU), but I'm not so confident we have an efficient way to get it.  I'm hoping that Michal or Bjarne or Nick have some idea of when we could gather this info without incurrent too much cost.\n\nOne idea is that we could get a statistical sample for this info from accumulating LRU data as we scan cache items when we're doing evictions: that might be a good way to free-ride on existing computation (though it would mean we only get data from users who have full caches.  That's probably fine).  I'm assuming that when we scan for evictions, we're getting random sample of items in the cache (i.e. they're not already sorted by LRU in any way)--true?", "creation_time": "2011-12-07T23:11:41Z", "bug_id": 708436, "id": 5898872, "tags": [], "is_private": false, "count": 0}, {"creator": "bug-husbandry-bot@mozilla.bugs", "author": "bug-husbandry-bot@mozilla.bugs", "tags": [], "is_private": false, "count": 1, "id": 12659070, "bug_id": 708436, "raw_text": "Bulk change to priority: https://bugzilla.mozilla.org/show_bug.cgi?id=1399258", "creation_time": "2017-09-13T18:41:15Z", "text": "Bulk change to priority: https://bugzilla.mozilla.org/show_bug.cgi?id=1399258", "attachment_id": null, "time": "2017-09-13T18:41:15Z"}]}}}