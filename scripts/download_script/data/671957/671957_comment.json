{"bugs": {"671957": {"comments": [{"is_private": false, "id": 5594235, "count": 0, "author": "dmandelin@mozilla.com", "creator": "dmandelin@mozilla.com", "time": "2011-07-15T21:07:03Z", "creation_time": "2011-07-15T21:07:03Z", "raw_text": "Stuart told me it would be awesome if websites could work on mobile just as well as on desktop. For JS, this basically means performance. I'm not sure yet how to think about time and space tradeoffs, but for now maybe we can think of it as how much speed we can get with an acceptable space cost.\n\nThere are 3 main questions here:\n\n1. What is the current state of JS perf on mobile ARM devices compared to desktop?\n2. How close can we get JS perf on ARM to desktop?\n3. What do we have to do to get that close?", "attachment_id": null, "text": "Stuart told me it would be awesome if websites could work on mobile just as well as on desktop. For JS, this basically means performance. I'm not sure yet how to think about time and space tradeoffs, but for now maybe we can think of it as how much speed we can get with an acceptable space cost.\n\nThere are 3 main questions here:\n\n1. What is the current state of JS perf on mobile ARM devices compared to desktop?\n2. How close can we get JS perf on ARM to desktop?\n3. What do we have to do to get that close?", "bug_id": 671957, "tags": []}, {"raw_text": "So I decided to see how various aspects of the hardware compare between x86 and arm.  With a rather simplistic test, I got these numbers (in seconds for the run)\n800 mhz arm:\ndouble:              73.965850\nsingle:              64.780529\ninteger:             20.124182\nrandom memory access: 5.966641\n2300 mhz core i7: (about 4 times the clock speed):\n\ndouble:               5.851761\nsingle:               5.861845\nint:                  4.758156\nrandom memory access: 1.510734\n\nthese tests were written in C, compiled with -O3, and turning on vfp on arm.  It seems like the arm floating point code is running at less than a tenth of the x86 floating point code.  Arm's integer and the memory accesses both seem to be keeping up with x86.  Unfortunately, this seems to indicate that code that is floating point heavy will have lots of trouble keeping up.  When I get some newer hardware, I'll test with that.", "count": 1, "id": 5603127, "attachment_id": null, "is_private": false, "bug_id": 671957, "text": "So I decided to see how various aspects of the hardware compare between x86 and arm.  With a rather simplistic test, I got these numbers (in seconds for the run)\n800 mhz arm:\ndouble:              73.965850\nsingle:              64.780529\ninteger:             20.124182\nrandom memory access: 5.966641\n2300 mhz core i7: (about 4 times the clock speed):\n\ndouble:               5.851761\nsingle:               5.861845\nint:                  4.758156\nrandom memory access: 1.510734\n\nthese tests were written in C, compiled with -O3, and turning on vfp on arm.  It seems like the arm floating point code is running at less than a tenth of the x86 floating point code.  Arm's integer and the memory accesses both seem to be keeping up with x86.  Unfortunately, this seems to indicate that code that is floating point heavy will have lots of trouble keeping up.  When I get some newer hardware, I'll test with that.", "tags": [], "author": "marty.rosenberg@gmail.com", "time": "2011-07-20T22:57:14Z", "creation_time": "2011-07-20T22:57:14Z", "creator": "marty.rosenberg@gmail.com"}, {"id": 5603228, "count": 2, "is_private": false, "creation_time": "2011-07-20T23:36:06Z", "time": "2011-07-20T23:36:06Z", "creator": "cdleary@acm.org", "author": "cdleary@acm.org", "attachment_id": null, "raw_text": "(In reply to comment #1)\n\nI don't know what the C is, exactly, but I would guess that this test implicitly assumes that the compilation from x86 GCC (presumably with SSE enabled? or is it just x87?) and ARM-with-VFP GCC is at parity, which I seriously doubt.\n\nIn order to get a solid comparison here I think comparing some FPU kernels that can't be easily reordered (since all the ARM uprocs we'll have are in-order) is a better test. Especially because in the JIT we hand craft the code! :-)", "tags": [], "text": "(In reply to comment #1)\n\nI don't know what the C is, exactly, but I would guess that this test implicitly assumes that the compilation from x86 GCC (presumably with SSE enabled? or is it just x87?) and ARM-with-VFP GCC is at parity, which I seriously doubt.\n\nIn order to get a solid comparison here I think comparing some FPU kernels that can't be easily reordered (since all the ARM uprocs we'll have are in-order) is a better test. Especially because in the JIT we hand craft the code! :-)", "bug_id": 671957}, {"count": 3, "id": 5603278, "is_private": false, "author": "dmandelin@mozilla.com", "time": "2011-07-20T23:55:57Z", "creation_time": "2011-07-20T23:55:57Z", "creator": "dmandelin@mozilla.com", "raw_text": "(In reply to comment #1)\n> It seems like the arm floating point code is running at less than a tenth of\n> the x86 floating point code.  Arm's integer and the memory accesses both\n> seem to be keeping up with x86.  Unfortunately, this seems to indicate that\n> code that is floating point heavy will have lots of trouble keeping up. \n> When I get some newer hardware, I'll test with that.\n\nNice! This is exactly the kind of basic info we need.\n\nI think the i7 is only 3x the clock speed in this case, right? For int and memory, you measured a 4x difference. It seems like that could be due to things like 'width' (is that the right term? I mean things like fetch bandwidth and number of functional units), reordering, and/or memory bus speed.\n\n- I'd be curious about a variant on Chris's idea: make a little integer kernel in C, test that, and then see if hand-hacking the ARM assembly can improve it. Maybe get help from Jacob on that.\n\n- Another key question is: Are the benchmark scores in line with the results above? I.e., do we tend to run 10x slower on the fp ones and 4x slower on the others? That would be great to know.\n\n(Note: I intended for this to be a meta bug, and each experiment on this general topic would be a separate bug linking to it. See bug 642003 for a prior example of this bug organization. Don't worry about it here, I'll make a new meta bug and morph this one.)", "attachment_id": null, "bug_id": 671957, "text": "(In reply to comment #1)\n> It seems like the arm floating point code is running at less than a tenth of\n> the x86 floating point code.  Arm's integer and the memory accesses both\n> seem to be keeping up with x86.  Unfortunately, this seems to indicate that\n> code that is floating point heavy will have lots of trouble keeping up. \n> When I get some newer hardware, I'll test with that.\n\nNice! This is exactly the kind of basic info we need.\n\nI think the i7 is only 3x the clock speed in this case, right? For int and memory, you measured a 4x difference. It seems like that could be due to things like 'width' (is that the right term? I mean things like fetch bandwidth and number of functional units), reordering, and/or memory bus speed.\n\n- I'd be curious about a variant on Chris's idea: make a little integer kernel in C, test that, and then see if hand-hacking the ARM assembly can improve it. Maybe get help from Jacob on that.\n\n- Another key question is: Are the benchmark scores in line with the results above? I.e., do we tend to run 10x slower on the fp ones and 4x slower on the others? That would be great to know.\n\n(Note: I intended for this to be a meta bug, and each experiment on this general topic would be a separate bug linking to it. See bug 642003 for a prior example of this bug organization. Don't worry about it here, I'll make a new meta bug and morph this one.)", "tags": []}, {"is_private": false, "count": 4, "id": 5603298, "author": "cdleary@acm.org", "creator": "cdleary@acm.org", "creation_time": "2011-07-21T00:07:42Z", "time": "2011-07-21T00:07:42Z", "raw_text": "(In reply to comment #3)\n> - I'd be curious about a variant on Chris's idea: make a little integer\n> kernel in C, test that, and then see if hand-hacking the ARM assembly can\n> improve it. Maybe get help from Jacob on that.\n\nMarty and I also chatted about this a little bit IRL -- if you make a micro-kernel with a single long (loop carried) dependency chain the issue width of your hardware won't matter and your reordering window won't help you at all. You can scale the number of simultaneous dependency chains up to see where the issue width chokes on the \"less super\" scalar pipelines.", "attachment_id": null, "bug_id": 671957, "text": "(In reply to comment #3)\n> - I'd be curious about a variant on Chris's idea: make a little integer\n> kernel in C, test that, and then see if hand-hacking the ARM assembly can\n> improve it. Maybe get help from Jacob on that.\n\nMarty and I also chatted about this a little bit IRL -- if you make a micro-kernel with a single long (loop carried) dependency chain the issue width of your hardware won't matter and your reordering window won't help you at all. You can scale the number of simultaneous dependency chains up to see where the issue width chokes on the \"less super\" scalar pipelines.", "tags": []}, {"is_private": false, "count": 5, "id": 5603718, "creator": "Jacob.Bramley@arm.com", "creation_time": "2011-07-21T07:47:33Z", "time": "2011-07-21T07:47:33Z", "author": "Jacob.Bramley@arm.com", "attachment_id": null, "raw_text": "(In reply to comment #3)\n> - I'd be curious about a variant on Chris's idea: make a little integer\n> kernel in C, test that, and then see if hand-hacking the ARM assembly can\n> improve it. Maybe get help from Jacob on that.\n\nGCC does a pretty good job on common cases, i.e. if you aren't doing\nanything crazy and on the boundaries of normal C. Still, it's always\nworth having a look to check.\n\nConsider A8 the lowest common denominator in terms of ARM floating-point\nperformance. A8's VFP is not pipelined. From its TRM:\n\n\"The VFP coprocessor is a nonpipelined floating-point execution engine\nthat can execute any VFPv3 data-processing instruction. Each instruction\nruns to completion before the next instruction can issue, and there is\nno forwarding of VFP results to other instructions.\"\n\nYou can find this at infocenter.arm.com (\"Cortex-A series processors\" ->\n\"Cortex-A8\" -> \"r3p2\" -> \"Instruction Cycle Timing\" -> \"VFP instructions\").\n\nA9's VFP is entirely different, and it's very fast. I suspect that the\nlikes of Qualcomm's Snapdragon (in several HTC phones) also have fast\nVFP, though I've not measured them.\n\nAlso, bear in mind that even on FP-heavy JavaScript, the overall\npercentage of VFP instructions involved is tiny compared to the loads\nand other overheads that we have. VFP performance still has an impact,\nbut not as much as it would for C code.", "tags": [], "bug_id": 671957, "text": "(In reply to comment #3)\n> - I'd be curious about a variant on Chris's idea: make a little integer\n> kernel in C, test that, and then see if hand-hacking the ARM assembly can\n> improve it. Maybe get help from Jacob on that.\n\nGCC does a pretty good job on common cases, i.e. if you aren't doing\nanything crazy and on the boundaries of normal C. Still, it's always\nworth having a look to check.\n\nConsider A8 the lowest common denominator in terms of ARM floating-point\nperformance. A8's VFP is not pipelined. From its TRM:\n\n\"The VFP coprocessor is a nonpipelined floating-point execution engine\nthat can execute any VFPv3 data-processing instruction. Each instruction\nruns to completion before the next instruction can issue, and there is\nno forwarding of VFP results to other instructions.\"\n\nYou can find this at infocenter.arm.com (\"Cortex-A series processors\" ->\n\"Cortex-A8\" -> \"r3p2\" -> \"Instruction Cycle Timing\" -> \"VFP instructions\").\n\nA9's VFP is entirely different, and it's very fast. I suspect that the\nlikes of Qualcomm's Snapdragon (in several HTC phones) also have fast\nVFP, though I've not measured them.\n\nAlso, bear in mind that even on FP-heavy JavaScript, the overall\npercentage of VFP instructions involved is tiny compared to the loads\nand other overheads that we have. VFP performance still has an impact,\nbut not as much as it would for C code."}, {"count": 6, "id": 5605431, "is_private": false, "creation_time": "2011-07-21T21:10:45Z", "time": "2011-07-21T21:10:45Z", "creator": "marty.rosenberg@gmail.com", "author": "marty.rosenberg@gmail.com", "attachment_id": null, "raw_text": "(In reply to comment #5)\n> (In reply to comment #3)\n> > - I'd be curious about a variant on Chris's idea: make a little integer\n> > kernel in C, test that, and then see if hand-hacking the ARM assembly can\n> > improve it. Maybe get help from Jacob on that.\n> \n> GCC does a pretty good job on common cases, i.e. if you aren't doing\n> anything crazy and on the boundaries of normal C. Still, it's always\n> worth having a look to check.\n> \nTo be fair, the version of gcc that I am using is kind of old (gcc-4.4), which\niirc, does not have their amazing loop optimization library.\n\n> Consider A8 the lowest common denominator in terms of ARM floating-point\n> performance. A8's VFP is not pipelined. From its TRM:\nThis is the board that I've been using (Freescale MX51 Lange5.1 Board).\n\n> Also, bear in mind that even on FP-heavy JavaScript, the overall\n> percentage of VFP instructions involved is tiny compared to the loads\n> and other overheads that we have. VFP performance still has an impact,\n> but not as much as it would for C code.\n\nThis would be my guess, I'm going to take a look at what we generate for some numerically heavy code.\n\n> I don't know what the C is, exactly, but I would guess that this test\n> implicitly assumes that the compilation from x86 GCC (presumably with SSE \n> enabled? or is it just x87?) and ARM-with-VFP GCC is at parity, which I \n> seriously doubt.\n\nI'll attach the C code in a bit.  I'm now compiling with -march=core2, which enables sse2, sse3 and ssse3.\n\n> In order to get a solid comparison here I think comparing some FPU kernels \n> that can't be easily reordered (since all the ARM uprocs we'll have are \n> in-order) is a better test. Especially because in the JIT we hand craft the \n> code! :-)\n\nI've added in two new tests, one that is basically a linear floating point test, and one that is basically a linear integer test.  The new results are:\nx86:\ndouble/mand:                     2.2143\ndouble/logistic:                 2.0211\nfloat/mand:                      2.2091\nfloat/logistic:                  2.9499\nint/fermat:                      1.3415\nint/rand:                        2.0532\nrandom_memory/merge_list_sort:   0.7088\narm:\ndouble/mand:                    73.2609\ndouble/logistic:                27.2753\nfloat/mand:                     64.2513\nfloat/logistic:                 29.5893\nint/fermat:                     19.9106\nint/rand:                       14.2950\nrandom_memory/merge_list_sort:   5.9424\n\nIn this case, mand and logistic take about the same amount of time on x86, but mand is about twice as slow on arm, so as we expected, on x86 code with fewer dependencies is much faster.", "text": "(In reply to comment #5)\n> (In reply to comment #3)\n> > - I'd be curious about a variant on Chris's idea: make a little integer\n> > kernel in C, test that, and then see if hand-hacking the ARM assembly can\n> > improve it. Maybe get help from Jacob on that.\n> \n> GCC does a pretty good job on common cases, i.e. if you aren't doing\n> anything crazy and on the boundaries of normal C. Still, it's always\n> worth having a look to check.\n> \nTo be fair, the version of gcc that I am using is kind of old (gcc-4.4), which\niirc, does not have their amazing loop optimization library.\n\n> Consider A8 the lowest common denominator in terms of ARM floating-point\n> performance. A8's VFP is not pipelined. From its TRM:\nThis is the board that I've been using (Freescale MX51 Lange5.1 Board).\n\n> Also, bear in mind that even on FP-heavy JavaScript, the overall\n> percentage of VFP instructions involved is tiny compared to the loads\n> and other overheads that we have. VFP performance still has an impact,\n> but not as much as it would for C code.\n\nThis would be my guess, I'm going to take a look at what we generate for some numerically heavy code.\n\n> I don't know what the C is, exactly, but I would guess that this test\n> implicitly assumes that the compilation from x86 GCC (presumably with SSE \n> enabled? or is it just x87?) and ARM-with-VFP GCC is at parity, which I \n> seriously doubt.\n\nI'll attach the C code in a bit.  I'm now compiling with -march=core2, which enables sse2, sse3 and ssse3.\n\n> In order to get a solid comparison here I think comparing some FPU kernels \n> that can't be easily reordered (since all the ARM uprocs we'll have are \n> in-order) is a better test. Especially because in the JIT we hand craft the \n> code! :-)\n\nI've added in two new tests, one that is basically a linear floating point test, and one that is basically a linear integer test.  The new results are:\nx86:\ndouble/mand:                     2.2143\ndouble/logistic:                 2.0211\nfloat/mand:                      2.2091\nfloat/logistic:                  2.9499\nint/fermat:                      1.3415\nint/rand:                        2.0532\nrandom_memory/merge_list_sort:   0.7088\narm:\ndouble/mand:                    73.2609\ndouble/logistic:                27.2753\nfloat/mand:                     64.2513\nfloat/logistic:                 29.5893\nint/fermat:                     19.9106\nint/rand:                       14.2950\nrandom_memory/merge_list_sort:   5.9424\n\nIn this case, mand and logistic take about the same amount of time on x86, but mand is about twice as slow on arm, so as we expected, on x86 code with fewer dependencies is much faster.", "bug_id": 671957, "tags": []}, {"bug_id": 671957, "text": "Created attachment 547514\n.tar.gz of the miniature benchmarking suite that I wrote\n\nextract with tar xzf foo.tgz\nthen \ncd hw; ./doit\n\nIf other people with working arm devices running linux could test it out, and\ngive numbers for comparison, it would probably be good.", "tags": [], "attachment_id": 547514, "raw_text": "extract with tar xzf foo.tgz\nthen \ncd hw; ./doit\n\nIf other people with working arm devices running linux could test it out, and\ngive numbers for comparison, it would probably be good.", "creator": "marty.rosenberg@gmail.com", "creation_time": "2011-07-21T21:18:40Z", "time": "2011-07-21T21:18:40Z", "author": "marty.rosenberg@gmail.com", "is_private": false, "id": 5605466, "count": 7}, {"is_private": false, "count": 8, "id": 5605792, "author": "evilpies@gmail.com", "creator": "evilpies@gmail.com", "creation_time": "2011-07-21T22:40:17Z", "time": "2011-07-21T22:40:17Z", "raw_text": "># uname -a\n>Darwin Toms-iPod-Touch 11.0.0 Darwin Kernel Version 11.0.0: Thu Feb 10 >21:45:19 PST 2011; root:xnu-1735.46~2/RELEASE_ARM_S5L8922X iPod3,1 arm N18AP >Darwin\n># gcc --version\n>gcc (GCC) 4.2.1 (Based on Apple Inc. build 5555)\n>Copyright (C) 2007 Free Software Foundation, Inc.\n>This is free software; see the source for copying conditions.  There is NO\n>warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nrandom_memory/merge_list_sort had build failures so i removed it.\n\ndouble/mand:     104.6216\ndouble/logistic:  36.9583\nfloat/mand:       69.9893\nfloat/logistic:   41.0557\nint/fermat:       61.7327\nint/rand:         20.3462", "attachment_id": null, "tags": [], "bug_id": 671957, "text": "># uname -a\n>Darwin Toms-iPod-Touch 11.0.0 Darwin Kernel Version 11.0.0: Thu Feb 10 >21:45:19 PST 2011; root:xnu-1735.46~2/RELEASE_ARM_S5L8922X iPod3,1 arm N18AP >Darwin\n># gcc --version\n>gcc (GCC) 4.2.1 (Based on Apple Inc. build 5555)\n>Copyright (C) 2007 Free Software Foundation, Inc.\n>This is free software; see the source for copying conditions.  There is NO\n>warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nrandom_memory/merge_list_sort had build failures so i removed it.\n\ndouble/mand:     104.6216\ndouble/logistic:  36.9583\nfloat/mand:       69.9893\nfloat/logistic:   41.0557\nint/fermat:       61.7327\nint/rand:         20.3462"}]}}, "comments": {}}