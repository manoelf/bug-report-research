{"comments": {}, "bugs": {"650284": {"comments": [{"author": "daniel.baulig@gmx.de", "tags": [], "count": 0, "creator": "daniel.baulig@gmx.de", "bug_id": 650284, "creation_time": "2011-04-15T15:07:13Z", "text": "User-Agent:       Mozilla/5.0 (Windows NT 6.1; WOW64; rv:2.0) Gecko/20100101 Firefox/4.0\nBuild Identifier: 4.0\n\nThe above linked jsPerf test case indicates that accessing an objects own member is considerable slower than if the same member is inherited by the objects prototype chain. Considering how ECMAScript works/should work? this is unlogical and should not be the case. For that reason I suppose there is a bug in the ES implementation.\n\nReproducible: Always\n\nSteps to Reproduce:\n1. Go to http://jsperf.com/prototype-chain/4\n2. Run Test\n3.\nActual Results:  \nAccessing Proto.member and Proto.method is considerable slower than accessing Level1Child or Level2Child members.\n\nExpected Results:  \nAccessing Proto.method and Proto.member should be at least as fast as accessing Level1Child and Level2Child members.\n\nAs you can see the other browsers give the expected results. I also considered that JIT might cause the difference, since Proto members are accessed first and then later again through the prototype chain. If the first test causes JIT to trigger then maybe the codepath is already compiled when the other two tests hit. This however is not the case. You can verify this by relaoding the page and then running the test cases one by one manually in reverse order. The effect is the same.", "id": 5411872, "raw_text": "User-Agent:       Mozilla/5.0 (Windows NT 6.1; WOW64; rv:2.0) Gecko/20100101 Firefox/4.0\nBuild Identifier: 4.0\n\nThe above linked jsPerf test case indicates that accessing an objects own member is considerable slower than if the same member is inherited by the objects prototype chain. Considering how ECMAScript works/should work? this is unlogical and should not be the case. For that reason I suppose there is a bug in the ES implementation.\n\nReproducible: Always\n\nSteps to Reproduce:\n1. Go to http://jsperf.com/prototype-chain/4\n2. Run Test\n3.\nActual Results:  \nAccessing Proto.member and Proto.method is considerable slower than accessing Level1Child or Level2Child members.\n\nExpected Results:  \nAccessing Proto.method and Proto.member should be at least as fast as accessing Level1Child and Level2Child members.\n\nAs you can see the other browsers give the expected results. I also considered that JIT might cause the difference, since Proto members are accessed first and then later again through the prototype chain. If the first test causes JIT to trigger then maybe the codepath is already compiled when the other two tests hit. This however is not the case. You can verify this by relaoding the page and then running the test cases one by one manually in reverse order. The effect is the same.", "attachment_id": null, "time": "2011-04-15T15:07:13Z", "is_private": false}, {"creator": "bzbarsky@mit.edu", "tags": [], "count": 1, "bug_id": 650284, "creation_time": "2011-04-15T16:16:53Z", "text": "Created attachment 526278\nShell testcase\n\nThis is only an issue in tracemonkey.  The numbers I see reported for this testcase look like this:\n\nInterp: 11871 11183\nJM: 1246 1305\nTM: 808 305\n\nThe difference for TM is that the first number includes a large number of samples under js::MethodWriteBarrier which are not present in the second number.", "id": 5412025, "raw_text": "This is only an issue in tracemonkey.  The numbers I see reported for this testcase look like this:\n\nInterp: 11871 11183\nJM: 1246 1305\nTM: 808 305\n\nThe difference for TM is that the first number includes a large number of samples under js::MethodWriteBarrier which are not present in the second number.", "author": "bzbarsky@mit.edu", "time": "2011-04-15T16:16:53Z", "is_private": false, "attachment_id": 526278}, {"tags": [], "creator": "bzbarsky@mit.edu", "count": 2, "time": "2011-04-15T16:28:32Z", "bug_id": 650284, "is_private": false, "creation_time": "2011-04-15T16:28:32Z", "text": "In fact, you don't need the method() calls at all to reproduce this\n\nWhat's happening is that every write to the \"member\" own property of an object that is branded or has a method barrier (which Proto is in this case) needs to call methodWriteBarrier on the object.  The relevant comments in TraceRecorder::nativeSet are:\n\n       // Setting a function-valued property might need to rebrand the\n       // object. Call the method write barrier. Note that even if the\n       // property is not function-valued now, it might be on trace.\n\nI wonder whether checking for \"function-valued\" on trace would be cheaper than unconditionally calling the write barrier.\n\nI also wonder whether this is something type inference would just help.\n\nDaniel, this isn't exactly a bug.  The two objects you're setting properties on have very different behavior on set after optimizations, since one of them has function-valued properties and the other does not and therefore the former can't optimize property writes as well as the latter.  If you add a function-valued property directly on Level1Child in my testcase, you will see the times become equal.  Note that after jitting what the browser actually does is quite different from what the spec says to do, because it short-circuits a bunch of the steps the spec calls for.  You can see that by comparing the \"TM\" times above to the \"Interp\" times (which _also_ short-circuit the prototype chain lookup, by the way).", "id": 5412049, "raw_text": "In fact, you don't need the method() calls at all to reproduce this\n\nWhat's happening is that every write to the \"member\" own property of an object that is branded or has a method barrier (which Proto is in this case) needs to call methodWriteBarrier on the object.  The relevant comments in TraceRecorder::nativeSet are:\n\n       // Setting a function-valued property might need to rebrand the\n       // object. Call the method write barrier. Note that even if the\n       // property is not function-valued now, it might be on trace.\n\nI wonder whether checking for \"function-valued\" on trace would be cheaper than unconditionally calling the write barrier.\n\nI also wonder whether this is something type inference would just help.\n\nDaniel, this isn't exactly a bug.  The two objects you're setting properties on have very different behavior on set after optimizations, since one of them has function-valued properties and the other does not and therefore the former can't optimize property writes as well as the latter.  If you add a function-valued property directly on Level1Child in my testcase, you will see the times become equal.  Note that after jitting what the browser actually does is quite different from what the spec says to do, because it short-circuits a bunch of the steps the spec calls for.  You can see that by comparing the \"TM\" times above to the \"Interp\" times (which _also_ short-circuit the prototype chain lookup, by the way).", "attachment_id": null, "author": "bzbarsky@mit.edu"}, {"raw_text": "(In reply to comment #2)\n> I also wonder whether this is something type inference would just help.\n\nThe idea here for type inference is that we should know call targets statically (simple in this case for the method() calls) and not need branding to cut the number of memory ops needed to resolve call targets dynamically.\n\nbranding has the dual problem that, as shown in the comment you quoted, it can slow down things which aren't involved with method properties at all.  I hit this when optimizing SETGNAME in JM+TI (we know the slot we're accessing and the value we're writing, but in the future the property may be method-valued and with branding we can't overwrite it without a shape change), and resolved this by disabling branding when inference is enabled.  Going forward I think we will want to remove branding entirely.\n\nFWIW, the times I get from JM+TI:\n\njs -m -n  499 1121\njs -j     1276 410\njs -m     1104 1147\n\nThe Level1Child case is comparable to plain -m because it is made with Object.create, whose result currently has totally unknown properties (should get fixed sometime).  If I doctor the testcase to use the more typical scripted 'new':\n\nfunction Foo() {}\nFoo.prototype = Proto;\nvar Level1Child = new Foo();\n\nI get these times (-j and -m are unchanged):\n\njs -m -n  488 290", "id": 5412098, "text": "(In reply to comment #2)\n> I also wonder whether this is something type inference would just help.\n\nThe idea here for type inference is that we should know call targets statically (simple in this case for the method() calls) and not need branding to cut the number of memory ops needed to resolve call targets dynamically.\n\nbranding has the dual problem that, as shown in the comment you quoted, it can slow down things which aren't involved with method properties at all.  I hit this when optimizing SETGNAME in JM+TI (we know the slot we're accessing and the value we're writing, but in the future the property may be method-valued and with branding we can't overwrite it without a shape change), and resolved this by disabling branding when inference is enabled.  Going forward I think we will want to remove branding entirely.\n\nFWIW, the times I get from JM+TI:\n\njs -m -n  499 1121\njs -j     1276 410\njs -m     1104 1147\n\nThe Level1Child case is comparable to plain -m because it is made with Object.create, whose result currently has totally unknown properties (should get fixed sometime).  If I doctor the testcase to use the more typical scripted 'new':\n\nfunction Foo() {}\nFoo.prototype = Proto;\nvar Level1Child = new Foo();\n\nI get these times (-j and -m are unchanged):\n\njs -m -n  488 290", "creation_time": "2011-04-15T16:49:26Z", "bug_id": 650284, "count": 3, "tags": [], "creator": "bhackett1024@gmail.com", "author": "bhackett1024@gmail.com", "is_private": false, "time": "2011-04-15T16:49:26Z", "attachment_id": null}, {"author": "ryanvm@gmail.com", "id": 5854192, "raw_text": "Current js shell numbers for the attached testcase:\nInterp: 34312 28926\n-j:     34478 29039\n-m:     1543  1510\n-m -n:  610   1600\n\nGiven that TM is obsolete at this point, I'm changing the title to reflect the future JM+TI work discussed in comment #3.", "creation_time": "2011-11-17T01:14:31Z", "text": "Current js shell numbers for the attached testcase:\nInterp: 34312 28926\n-j:     34478 29039\n-m:     1543  1510\n-m -n:  610   1600\n\nGiven that TM is obsolete at this point, I'm changing the title to reflect the future JM+TI work discussed in comment #3.", "tags": [], "count": 4, "creator": "ryanvm@gmail.com", "bug_id": 650284, "attachment_id": null, "is_private": false, "time": "2011-11-17T01:14:31Z"}, {"bug_id": 650284, "time": "2012-11-25T02:09:00Z", "tags": [], "creator": "guijoselito@gmail.com", "count": 5, "text": "On Nightly\n\nIon off\nLocal 49,022,280 \u00b10.16% fastest\nMember 28,845,034 \u00b137.94% 57% slower\nChild 5,813,390 \u00b14.10% 89% slower\nGrandChild 5,889,825 \u00b11.29% 88% slower\n\nIon on\nLocal 69,643,123 \u00b10.33% fastest\nMember 53,768,355 \u00b15.40%27% slower\nChild 1,413,836 \u00b10.16%98% slower\nGrandChild 1,420,589 \u00b10.15%98% slower\n\nA lot slower on the last 2. Expected? Not important?", "creation_time": "2012-11-25T02:09:00Z", "is_private": false, "raw_text": "On Nightly\n\nIon off\nLocal 49,022,280 \u00b10.16% fastest\nMember 28,845,034 \u00b137.94% 57% slower\nChild 5,813,390 \u00b14.10% 89% slower\nGrandChild 5,889,825 \u00b11.29% 88% slower\n\nIon on\nLocal 69,643,123 \u00b10.33% fastest\nMember 53,768,355 \u00b15.40%27% slower\nChild 1,413,836 \u00b10.16%98% slower\nGrandChild 1,420,589 \u00b10.15%98% slower\n\nA lot slower on the last 2. Expected? Not important?", "id": 6849582, "author": "guijoselito@gmail.com", "attachment_id": null}, {"attachment_id": 763294, "author": "till@tillschneidereit.net", "is_private": false, "creation_time": "2013-06-16T23:02:13Z", "text": "Created attachment 763294\nShell testcase for property accesses\n\n> A lot slower on the last 2. Expected? Not important?\n\nMaybe to both. There certainly are some weird things going on, here.\n\nFirst the good news: accessing own members of an object isn't slower, anymore. Well, it when running in the interpreter, but not in Baseline or Ion. However, it is *much* slower than in v8. For the attached testcase, I get these timings:\n\nSpiderMonkey (with IM):\ndirect: 1514\n__proto__: 1586\nObject.create: 1700\nfunction instance with prototype: 1560\n\nd8:\ndirect: 162\n__proto__: 176\nObject.create: 191\nfunction instance with prototype: 177\n\nSo where ~9x as slow for all these scenarios as v8 is.\n\nNote also that the Object.create version is ~10% slower than the others. That might nor might not be expected, I don't know.\n\nSo far, so bad. Now for the really weird part. The jsperf test paints a *very* different picture. There, accessing the inherited properties is much slower than the own property - almost 50x as slow. I modified the test in http://jsperf.com/prototype-chain/5 to reflect the scenarios in the attached shell testcase, with results resembling those of the previous version.\n\nFrom the results in http://jsperf.com/prototype-chain/4, it looks like we regressed this somewhere between FF6 and FF9.\n\nAt least, we're ~3x as fast as Chromium on the local access part in the jsperf harness - again going completely against the shell results.\n\nI guess that leaves us with three questions:\n- Why are we that much slower than v8 in the shell testcase?\n- What on earth is going on in that jsperf test?\n- When and why exactly did those performance numbers in the jsperf test change?", "id": 7540312, "raw_text": "> A lot slower on the last 2. Expected? Not important?\n\nMaybe to both. There certainly are some weird things going on, here.\n\nFirst the good news: accessing own members of an object isn't slower, anymore. Well, it when running in the interpreter, but not in Baseline or Ion. However, it is *much* slower than in v8. For the attached testcase, I get these timings:\n\nSpiderMonkey (with IM):\ndirect: 1514\n__proto__: 1586\nObject.create: 1700\nfunction instance with prototype: 1560\n\nd8:\ndirect: 162\n__proto__: 176\nObject.create: 191\nfunction instance with prototype: 177\n\nSo where ~9x as slow for all these scenarios as v8 is.\n\nNote also that the Object.create version is ~10% slower than the others. That might nor might not be expected, I don't know.\n\nSo far, so bad. Now for the really weird part. The jsperf test paints a *very* different picture. There, accessing the inherited properties is much slower than the own property - almost 50x as slow. I modified the test in http://jsperf.com/prototype-chain/5 to reflect the scenarios in the attached shell testcase, with results resembling those of the previous version.\n\nFrom the results in http://jsperf.com/prototype-chain/4, it looks like we regressed this somewhere between FF6 and FF9.\n\nAt least, we're ~3x as fast as Chromium on the local access part in the jsperf harness - again going completely against the shell results.\n\nI guess that leaves us with three questions:\n- Why are we that much slower than v8 in the shell testcase?\n- What on earth is going on in that jsperf test?\n- When and why exactly did those performance numbers in the jsperf test change?", "tags": [], "time": "2013-06-16T23:02:13Z", "creator": "till@tillschneidereit.net", "count": 6, "bug_id": 650284}, {"time": "2013-06-16T23:03:38Z", "is_private": false, "attachment_id": null, "count": 7, "creator": "till@tillschneidereit.net", "tags": [], "bug_id": 650284, "creation_time": "2013-06-16T23:03:38Z", "text": "In case that wasn't clear, this bug has now officially morphed into something entirely different. Sorry about that.", "id": 7540313, "raw_text": "In case that wasn't clear, this bug has now officially morphed into something entirely different. Sorry about that.", "author": "till@tillschneidereit.net"}, {"attachment_id": null, "time": "2013-06-17T01:14:11Z", "is_private": false, "author": "bzbarsky@mit.edu", "bug_id": 650284, "count": 8, "creator": "bzbarsky@mit.edu", "tags": [], "text": "jsperf, unlike the shell testcase:\n\n1)  Runs in a browser.\n2)  Runs inside a |new Function()|, not at global scope.\n\nSo sometimes it will measure things quite differently from a near-equivalent shell testcase...", "creation_time": "2013-06-17T01:14:11Z", "raw_text": "jsperf, unlike the shell testcase:\n\n1)  Runs in a browser.\n2)  Runs inside a |new Function()|, not at global scope.\n\nSo sometimes it will measure things quite differently from a near-equivalent shell testcase...", "id": 7540423}, {"attachment_id": null, "time": "2013-06-17T07:14:34Z", "is_private": false, "author": "till@tillschneidereit.net", "tags": [], "count": 9, "creator": "till@tillschneidereit.net", "bug_id": 650284, "creation_time": "2013-06-17T07:14:34Z", "text": "(In reply to Boris Zbarsky (:bz) from comment #8)\n> jsperf, unlike the shell testcase:\n> \n> 1)  Runs in a browser.\n\nOne of these days, we should show measurements on awfy that are actually relevant to our users ...\n\n> 2)  Runs inside a |new Function()|, not at global scope.\n\nAh, that's a good hint. Since bug 646597, that should not matter anymore, but it probably does.\n\n> \n> So sometimes it will measure things quite differently from a near-equivalent\n> shell testcase...\n\nSure. It's just that this changed behavior in interesting ways that seem worthwile to investigate.", "id": 7540959, "raw_text": "(In reply to Boris Zbarsky (:bz) from comment #8)\n> jsperf, unlike the shell testcase:\n> \n> 1)  Runs in a browser.\n\nOne of these days, we should show measurements on awfy that are actually relevant to our users ...\n\n> 2)  Runs inside a |new Function()|, not at global scope.\n\nAh, that's a good hint. Since bug 646597, that should not matter anymore, but it probably does.\n\n> \n> So sometimes it will measure things quite differently from a near-equivalent\n> shell testcase...\n\nSure. It's just that this changed behavior in interesting ways that seem worthwile to investigate."}, {"id": 7541655, "raw_text": "Right.  I was just pointing out possible starting avenues for investigation.  ;)", "is_private": false, "creation_time": "2013-06-17T12:07:40Z", "text": "Right.  I was just pointing out possible starting avenues for investigation.  ;)", "tags": [], "count": 10, "creator": "bzbarsky@mit.edu", "time": "2013-06-17T12:07:40Z", "bug_id": 650284, "attachment_id": null, "author": "bzbarsky@mit.edu"}, {"raw_text": "Just re-discovered this bug. Current numbers follow.\n\nFirst test case:\nv8:\n211\n178\n\nSpiderMonkey:\n3426\n3436\n\n\nSecond test case:\n\nv8:\ndirect: 163\n__proto__: 185\nObject.create: 184\nfunction instance with prototype: 181\n\nSpiderMonkey:\ndirect: 1497\n__proto__: 1493\nObject.create: 1526\nfunction instance with prototype: 1554\n\nSo, we're still between 8x and 15x slower than v8 here.\n\nefaust, this looks like it might interest you.", "id": 8424006, "text": "Just re-discovered this bug. Current numbers follow.\n\nFirst test case:\nv8:\n211\n178\n\nSpiderMonkey:\n3426\n3436\n\n\nSecond test case:\n\nv8:\ndirect: 163\n__proto__: 185\nObject.create: 184\nfunction instance with prototype: 181\n\nSpiderMonkey:\ndirect: 1497\n__proto__: 1493\nObject.create: 1526\nfunction instance with prototype: 1554\n\nSo, we're still between 8x and 15x slower than v8 here.\n\nefaust, this looks like it might interest you.", "creation_time": "2014-02-15T00:06:58Z", "bug_id": 650284, "count": 11, "creator": "till@tillschneidereit.net", "tags": [], "author": "till@tillschneidereit.net", "is_private": false, "time": "2014-02-15T00:06:58Z", "attachment_id": null}, {"attachment_id": null, "is_private": false, "time": "2014-02-15T03:13:05Z", "author": "bzbarsky@mit.edu", "text": "> So, we're still between 8x and 15x slower than v8 here.\n\nYou should try replacing this bit:\n\n  const LOOPCOUNT = 10000000;\n\nwith:\n\n  var LOOPCOUNT = 10000000;\n\nbecause afaict we just fail to ion-compile a getgname for a const for some reason (or at least JIT inspector claims no ion code).  I'm not sure why it fails, exactly.  I thought of bug 956072, but that's fixed...\n\nAlternately, wrap the code in a function so that we're not doing a gname.  That also makes it faster (and the const/var difference goes away)...", "creation_time": "2014-02-15T03:13:05Z", "raw_text": "> So, we're still between 8x and 15x slower than v8 here.\n\nYou should try replacing this bit:\n\n  const LOOPCOUNT = 10000000;\n\nwith:\n\n  var LOOPCOUNT = 10000000;\n\nbecause afaict we just fail to ion-compile a getgname for a const for some reason (or at least JIT inspector claims no ion code).  I'm not sure why it fails, exactly.  I thought of bug 956072, but that's fixed...\n\nAlternately, wrap the code in a function so that we're not doing a gname.  That also makes it faster (and the const/var difference goes away)...", "id": 8424419, "bug_id": 650284, "count": 12, "creator": "bzbarsky@mit.edu", "tags": []}, {"time": "2014-02-17T09:42:48Z", "count": 13, "tags": [], "creator": "jdemooij@mozilla.com", "bug_id": 650284, "is_private": false, "creation_time": "2014-02-17T09:42:48Z", "text": "NI myself for the const vs var issue; that seems silly.", "id": 8427650, "raw_text": "NI myself for the const vs var issue; that seems silly.", "attachment_id": null, "author": "jdemooij@mozilla.com"}, {"raw_text": "Filed bug 973526 for the const issue, Ion should compile JSOP_SETCONST.", "id": 8427714, "text": "Filed bug 973526 for the const issue, Ion should compile JSOP_SETCONST.", "is_private": false, "creation_time": "2014-02-17T10:02:34Z", "bug_id": 650284, "tags": [], "count": 14, "time": "2014-02-17T10:02:34Z", "creator": "jdemooij@mozilla.com", "author": "jdemooij@mozilla.com", "attachment_id": null}, {"bug_id": 650284, "creator": "till@tillschneidereit.net", "tags": [], "count": 15, "text": "(In reply to Boris Zbarsky [:bz] from comment #12)\n> > So, we're still between 8x and 15x slower than v8 here.\n> \n> You should try replacing this bit:\n> \n>   const LOOPCOUNT = 10000000;\n> \n> with:\n> \n>   var LOOPCOUNT = 10000000;\n> \n> because afaict we just fail to ion-compile a getgname for a const for some\n> reason (or at least JIT inspector claims no ion code).  I'm not sure why it\n> fails, exactly.  I thought of bug 956072, but that's fixed...\n\nOh, right - I should've seen the const.\n\nWith that fixed, the numbers are:\n\nSpiderMonkey:\ndirect: 106\n__proto__: 540\nObject.create: 130\nfunction instance with prototype: 123\n\nv8:\ndirect: 167\n__proto__: 197\nObject.create: 185\nfunction instance with prototype: 180\n\njsc:\ndirect: 35\n__proto__: 36\nObject.create: 45\nfunction instance with prototype: 73\n\n\nTwo observations:\n- JSC's get/setprop is *fast*\n- the competition can apparently deal with __proto__ being set at least once without deoptimizing.\nWaldo, you worked on the __proto__ stuff and recently added a warning about it being slow. Is there any chance to get the set-once-in-the-beginning case fast?", "creation_time": "2014-02-19T02:10:38Z", "raw_text": "(In reply to Boris Zbarsky [:bz] from comment #12)\n> > So, we're still between 8x and 15x slower than v8 here.\n> \n> You should try replacing this bit:\n> \n>   const LOOPCOUNT = 10000000;\n> \n> with:\n> \n>   var LOOPCOUNT = 10000000;\n> \n> because afaict we just fail to ion-compile a getgname for a const for some\n> reason (or at least JIT inspector claims no ion code).  I'm not sure why it\n> fails, exactly.  I thought of bug 956072, but that's fixed...\n\nOh, right - I should've seen the const.\n\nWith that fixed, the numbers are:\n\nSpiderMonkey:\ndirect: 106\n__proto__: 540\nObject.create: 130\nfunction instance with prototype: 123\n\nv8:\ndirect: 167\n__proto__: 197\nObject.create: 185\nfunction instance with prototype: 180\n\njsc:\ndirect: 35\n__proto__: 36\nObject.create: 45\nfunction instance with prototype: 73\n\n\nTwo observations:\n- JSC's get/setprop is *fast*\n- the competition can apparently deal with __proto__ being set at least once without deoptimizing.\nWaldo, you worked on the __proto__ stuff and recently added a warning about it being slow. Is there any chance to get the set-once-in-the-beginning case fast?", "id": 8435389, "author": "till@tillschneidereit.net", "time": "2014-02-19T02:10:38Z", "is_private": false, "attachment_id": null}, {"text": "(In reply to Till Schneidereit [:till] from comment #15)\n> Waldo, you worked on the __proto__ stuff and recently added a warning about\n> it being slow. Is there any chance to get the set-once-in-the-beginning case\n> fast?\n\nIt seems unlikely. setting __proto__ almost certianly nukes TI for a whole bunch of things, making later optimizations hard.", "creation_time": "2014-02-19T02:13:19Z", "raw_text": "(In reply to Till Schneidereit [:till] from comment #15)\n> Waldo, you worked on the __proto__ stuff and recently added a warning about\n> it being slow. Is there any chance to get the set-once-in-the-beginning case\n> fast?\n\nIt seems unlikely. setting __proto__ almost certianly nukes TI for a whole bunch of things, making later optimizations hard.", "id": 8435400, "bug_id": 650284, "tags": [], "count": 16, "creator": "efaustbmo@gmail.com", "author": "efaustbmo@gmail.com", "is_private": false, "time": "2014-02-19T02:13:19Z", "attachment_id": null}, {"creator": "bzbarsky@mit.edu", "count": 17, "time": "2014-02-19T02:56:38Z", "tags": [], "bug_id": 650284, "is_private": false, "creation_time": "2014-02-19T02:56:38Z", "text": "> - JSC's get/setprop is *fast*\n\nSort of.  If you actually look at how much time it's taking per iteration, and compare to a empty loop, you discover that JSC is doing 10 get/sets in about 3 instructions on my hardware.\n\nIn other words, a sufficiently smart compiler optimized away your microbenchmark.  Simply moving the \"Proto.member = 0;\" line (and equivalent for other loops) out of the loop makes JSC take a lot more time.  Now it's _still_ faster at that point than we are, but not by nearly as much...", "id": 8435546, "raw_text": "> - JSC's get/setprop is *fast*\n\nSort of.  If you actually look at how much time it's taking per iteration, and compare to a empty loop, you discover that JSC is doing 10 get/sets in about 3 instructions on my hardware.\n\nIn other words, a sufficiently smart compiler optimized away your microbenchmark.  Simply moving the \"Proto.member = 0;\" line (and equivalent for other loops) out of the loop makes JSC take a lot more time.  Now it's _still_ faster at that point than we are, but not by nearly as much...", "attachment_id": null, "author": "bzbarsky@mit.edu"}, {"author": "till@tillschneidereit.net", "text": "(In reply to Boris Zbarsky [:bz] from comment #17)\n> > - JSC's get/setprop is *fast*\n> \n> Sort of.  If you actually look at how much time it's taking per iteration,\n> and compare to a empty loop, you discover that JSC is doing 10 get/sets in\n> about 3 instructions on my hardware.\n\nHum, I thought I had checked for that by removing single lines from the 10 `+= 1` part. Doing that reduced the time JSC takes for the changed test by 1.5ms/removed line.", "creation_time": "2014-02-19T03:10:30Z", "raw_text": "(In reply to Boris Zbarsky [:bz] from comment #17)\n> > - JSC's get/setprop is *fast*\n> \n> Sort of.  If you actually look at how much time it's taking per iteration,\n> and compare to a empty loop, you discover that JSC is doing 10 get/sets in\n> about 3 instructions on my hardware.\n\nHum, I thought I had checked for that by removing single lines from the 10 `+= 1` part. Doing that reduced the time JSC takes for the changed test by 1.5ms/removed line.", "id": 8435593, "bug_id": 650284, "tags": [], "count": 18, "creator": "till@tillschneidereit.net", "attachment_id": null, "is_private": false, "time": "2014-02-19T03:10:30Z"}, {"is_private": false, "time": "2014-02-19T03:27:02Z", "attachment_id": null, "creation_time": "2014-02-19T03:27:02Z", "text": "LOOPCOUNT is 10000000 = 1e7.  1.5ms/LOOPCOUNT is therefore 0.15ns.  I suspect your hardware is clocked in the 2-3 GHz range, so 0.3-0.5ns per cycle.  Since you're updating the same memory location, it's unlikely you're getting funky multiple-instructions-per-cycle benefits from out of order execution and whatnot.  More likely, you're spending 1.5ms/line in JSC's JIT warmup period...\n\nYou could check by increasing LOOPCOUNT by a factor of 10 and seeing whether you then win 15ms per removed line.", "id": 8435635, "raw_text": "LOOPCOUNT is 10000000 = 1e7.  1.5ms/LOOPCOUNT is therefore 0.15ns.  I suspect your hardware is clocked in the 2-3 GHz range, so 0.3-0.5ns per cycle.  Since you're updating the same memory location, it's unlikely you're getting funky multiple-instructions-per-cycle benefits from out of order execution and whatnot.  More likely, you're spending 1.5ms/line in JSC's JIT warmup period...\n\nYou could check by increasing LOOPCOUNT by a factor of 10 and seeing whether you then win 15ms per removed line.", "tags": [], "creator": "bzbarsky@mit.edu", "count": 19, "bug_id": 650284, "author": "bzbarsky@mit.edu"}, {"attachment_id": null, "time": "2014-02-19T03:41:05Z", "is_private": false, "author": "till@tillschneidereit.net", "tags": [], "count": 20, "creator": "till@tillschneidereit.net", "bug_id": 650284, "id": 8435687, "raw_text": "(In reply to Boris Zbarsky [:bz] from comment #19)\n> You could check by increasing LOOPCOUNT by a factor of 10 and seeing whether\n> you then win 15ms per removed line.\n\nPretty exactly 20ms/cycle, in fact. Compared to 110ms for us. But in any case, you're right: moving the `member = 0` out of the loop reduces they speed advantage to about 33%. Stupid microbenchmark is stupid. As was to be expected.", "creation_time": "2014-02-19T03:41:05Z", "text": "(In reply to Boris Zbarsky [:bz] from comment #19)\n> You could check by increasing LOOPCOUNT by a factor of 10 and seeing whether\n> you then win 15ms per removed line.\n\nPretty exactly 20ms/cycle, in fact. Compared to 110ms for us. But in any case, you're right: moving the `member = 0` out of the loop reduces they speed advantage to about 33%. Stupid microbenchmark is stupid. As was to be expected."}, {"creation_time": "2014-03-12T22:58:19Z", "text": "(In reply to Till Schneidereit [:till] (on vacation March 1st to 23rd) from comment #15)\n> Waldo, you worked on the __proto__ stuff and recently added a warning about\n> it being slow. Is there any chance to get the set-once-in-the-beginning case\n> fast?\n\nI'm not sure __proto__ has much of anything to do with this, it's really the underlying |SetClassAndProto| function at issue.\n\nIt looks like the issue might be that when we have the object whose [[Prototype]] is being mutated, we walk its [[Prototype]] chain and generate new shapes, or setUncacheableProto, on each -- because any object with any of those on its [[Prototype]] chain suddenly will look different to some property accesses.  That's perfectly reasonable.\n\nWhat's not so reasonable is that we include the starting object in that loop.  But the starting object *doesn't* necessarily have any objects whose lookup behavior will change, if the starting object changes.  If the starting object's a singleton, no biggie, we just generate a new shape, which probably doesn't hurt much.  But if it's not, and I don't see why |Proto| would be in your shell testcase, then we |setUncacheableProto|, which probably makes JITs go sadfaces.\n\nSo probably that loop should start with |obj->getProto()|.  And as far as |obj| is concerned, we probably should only futz with it if |obj->isDelegate()|.", "id": 8526409, "raw_text": "(In reply to Till Schneidereit [:till] (on vacation March 1st to 23rd) from comment #15)\n> Waldo, you worked on the __proto__ stuff and recently added a warning about\n> it being slow. Is there any chance to get the set-once-in-the-beginning case\n> fast?\n\nI'm not sure __proto__ has much of anything to do with this, it's really the underlying |SetClassAndProto| function at issue.\n\nIt looks like the issue might be that when we have the object whose [[Prototype]] is being mutated, we walk its [[Prototype]] chain and generate new shapes, or setUncacheableProto, on each -- because any object with any of those on its [[Prototype]] chain suddenly will look different to some property accesses.  That's perfectly reasonable.\n\nWhat's not so reasonable is that we include the starting object in that loop.  But the starting object *doesn't* necessarily have any objects whose lookup behavior will change, if the starting object changes.  If the starting object's a singleton, no biggie, we just generate a new shape, which probably doesn't hurt much.  But if it's not, and I don't see why |Proto| would be in your shell testcase, then we |setUncacheableProto|, which probably makes JITs go sadfaces.\n\nSo probably that loop should start with |obj->getProto()|.  And as far as |obj| is concerned, we probably should only futz with it if |obj->isDelegate()|.", "tags": [], "count": 21, "creator": "jwalden@mit.edu", "bug_id": 650284, "author": "jwalden@mit.edu", "is_private": false, "time": "2014-03-12T22:58:19Z", "attachment_id": null}]}}}