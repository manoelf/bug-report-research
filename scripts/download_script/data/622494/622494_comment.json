{"comments": {}, "bugs": {"622494": {"comments": [{"count": 0, "time": "2011-01-03T07:27:09Z", "raw_text": "I claimed that JavaScript will never be as fast as native code, thus the web needs Native Client or something like it.  Shaver called my bluff \"Take a performance-sensitive algorithm that matters to you, write a benchmark, and submit it as a bug.\"\n\nI took the SSE-accelerated Cal3D skinning loop and implemented four versions of it.  I benchmarked them on a MacBook Pro, 2.5 GHz Core i5.  C++ is compiled with gcc -O2, and JavaScript is run with Firefox 4 beta 8.  Results are in millions of vertices skinned per second.\n\nC++ w/ SSE intrinsics (near theoretical maximum on a single core):  98.3\nC++ w/ scalars:  61.2\nJavaScript:  5.1\nJavaScript w/ typed arrays:  8.4\n\nThe code is available at https://github.com/chadaustin/Web-Benchmarks\n\nvlad did some investigation on IRC:\n\n<vlad>\tare we really guarding that these things are still functions?\n<vlad>\tis that what's going on here?\n<vlad>\twell, got a 15% win by removing all the inner calls to TransformPoint/TransformVector/ScaleMatrix/AddScaledMatrix and inlining those\n<vlad>\twhich is stupid\n\nThe JS testcases are trivial to turn into shell testcases by changing dump() to print().  (I think vlad meant alert() there.)", "id": 5170759, "text": "I claimed that JavaScript will never be as fast as native code, thus the web needs Native Client or something like it.  Shaver called my bluff \"Take a performance-sensitive algorithm that matters to you, write a benchmark, and submit it as a bug.\"\n\nI took the SSE-accelerated Cal3D skinning loop and implemented four versions of it.  I benchmarked them on a MacBook Pro, 2.5 GHz Core i5.  C++ is compiled with gcc -O2, and JavaScript is run with Firefox 4 beta 8.  Results are in millions of vertices skinned per second.\n\nC++ w/ SSE intrinsics (near theoretical maximum on a single core):  98.3\nC++ w/ scalars:  61.2\nJavaScript:  5.1\nJavaScript w/ typed arrays:  8.4\n\nThe code is available at https://github.com/chadaustin/Web-Benchmarks\n\nvlad did some investigation on IRC:\n\n<vlad>\tare we really guarding that these things are still functions?\n<vlad>\tis that what's going on here?\n<vlad>\twell, got a 15% win by removing all the inner calls to TransformPoint/TransformVector/ScaleMatrix/AddScaledMatrix and inlining those\n<vlad>\twhich is stupid\n\nThe JS testcases are trivial to turn into shell testcases by changing dump() to print().  (I think vlad meant alert() there.)", "is_private": false, "bug_id": 622494, "attachment_id": null, "creation_time": "2011-01-03T07:27:09Z", "author": "caustin@gmail.com", "tags": [], "creator": "caustin@gmail.com"}, {"creator": "bzbarsky@mit.edu", "creation_time": "2011-01-03T08:19:48Z", "time": "2011-01-03T08:19:48Z", "text": "No, he probably meant dump(), which writes to stdout in the browser if the right pref is enabled.\n\nI'll attach a shell version of the typed array JS, with profiling hooks added but commented out; running a shark profile there shows pretty much all of the time spent in tracejit-generated code.\n\nGiven that tjit inlines method calls, obviously, the fact that inlining them manually helped does seem to point to function guards of some sort.\n\nI wonder how this testcase does with the LICM patches...", "id": 5170779, "is_private": false, "author": "bzbarsky@mit.edu", "tags": [], "bug_id": 622494, "attachment_id": null, "raw_text": "No, he probably meant dump(), which writes to stdout in the browser if the right pref is enabled.\n\nI'll attach a shell version of the typed array JS, with profiling hooks added but commented out; running a shark profile there shows pretty much all of the time spent in tracejit-generated code.\n\nGiven that tjit inlines method calls, obviously, the fact that inlining them manually helped does seem to point to function guards of some sort.\n\nI wonder how this testcase does with the LICM patches...", "count": 1}, {"author": "bzbarsky@mit.edu", "creator": "bzbarsky@mit.edu", "tags": [], "creation_time": "2011-01-03T08:20:17Z", "bug_id": 622494, "attachment_id": 500754, "is_private": false, "time": "2011-01-03T08:20:17Z", "text": "Created attachment 500754\nTyped array shell testcase", "id": 5170780, "raw_text": "", "count": 2}, {"tags": [], "author": "bzbarsky@mit.edu", "attachment_id": null, "bug_id": 622494, "raw_text": "Fwiw, v8 can't run the typed array testcase, but their tip (with crankshaft) gets about 8e6 vertices per second on the non-typed-array version; we get about 5.5e6 (with -j or -m -j -p; the results are about the same; -m alone is at 3e6).  So there's definite headroom here!", "count": 3, "creator": "bzbarsky@mit.edu", "creation_time": "2011-01-03T08:22:10Z", "is_private": false, "text": "Fwiw, v8 can't run the typed array testcase, but their tip (with crankshaft) gets about 8e6 vertices per second on the non-typed-array version; we get about 5.5e6 (with -j or -m -j -p; the results are about the same; -m alone is at 3e6).  So there's definite headroom here!", "id": 5170781, "time": "2011-01-03T08:22:10Z"}, {"author": "bzbarsky@mit.edu", "tags": [], "bug_id": 622494, "attachment_id": null, "raw_text": "Also fwiw, I can't seem to reproduce vlad's 15% speedup here, though maybe my lazy way of inlining (copy-paste, with some var decls to assign the things that got passed to the function before) is responsible... but that seems like the right way to inline in this case anyway, so we only compute the somewhat-complicated arguments once.\n\nOne other note: we seem to compile two copies of the hot loop here; it looks like something is type-unstable (a bunch of I and O types the first time through; all U the second time through).\n\nAnd if I inline the ScaleMatrix/AddScaledMatrix calls (not like the latter is called anyway, in my testing), and then re-roll those loops, performance goes down by a factor of 2, and the number of traces compiled goes from 4 to 10 (presumably due to more type instability?).", "count": 4, "creator": "bzbarsky@mit.edu", "creation_time": "2011-01-03T08:39:34Z", "is_private": false, "time": "2011-01-03T08:39:34Z", "id": 5170786, "text": "Also fwiw, I can't seem to reproduce vlad's 15% speedup here, though maybe my lazy way of inlining (copy-paste, with some var decls to assign the things that got passed to the function before) is responsible... but that seems like the right way to inline in this case anyway, so we only compute the somewhat-complicated arguments once.\n\nOne other note: we seem to compile two copies of the hot loop here; it looks like something is type-unstable (a bunch of I and O types the first time through; all U the second time through).\n\nAnd if I inline the ScaleMatrix/AddScaledMatrix calls (not like the latter is called anyway, in my testing), and then re-roll those loops, performance goes down by a factor of 2, and the number of traces compiled goes from 4 to 10 (presumably due to more type instability?)."}, {"raw_text": "(In reply to comment #4)\n> Also fwiw, I can't seem to reproduce vlad's 15% speedup here, though maybe my\n> lazy way of inlining (copy-paste, with some var decls to assign the things that\n> got passed to the function before) is responsible... but that seems like the\n> right way to inline in this case anyway, so we only compute the\n> somewhat-complicated arguments once.\n\nYeah, that's what I did.  After a bunch more runs, the win wasn't consistent, but it showed up.\n\n> One other note: we seem to compile two copies of the hot loop here; it looks\n> like something is type-unstable (a bunch of I and O types the first time\n> through; all U the second time through).\n> \n> And if I inline the ScaleMatrix/AddScaledMatrix calls (not like the latter is\n> called anyway, in my testing), and then re-roll those loops, performance goes\n> down by a factor of 2, and the number of traces compiled goes from 4 to 10\n> (presumably due to more type instability?).\n\nOne thing that I did is change the timing loop:\n\n    const start = new Date();\n    const now = new Date();\n    while ((new Date() - start) < 1000) {\n\tcalculateVerticesAndNormals(bt, v, i, output);\n\tvertices_skinned += N;\n    }\n\nto\n\n    var start = Date.now();\n    while (true) {\n\tcalculateVerticesAndNormals(bt, v, i, output);\n\tvertices_skinned += N;\n\n\tif ((vertices_skinned % 100000) == 0 &&\n\t    (Date.now() - start) >= 1000)\n\t{\n\t    break;\n\t}\n    }\n\nto avoid constant Date object creation.  It made a difference, but not a huge one; it made me feel better, though!  (Also the testcase has some odd const usage; replacing most of those with var didn't change the results, but I think we largely ignore const for this?)", "count": 5, "author": "vladimir@pobox.com", "tags": [], "bug_id": 622494, "attachment_id": null, "time": "2011-01-03T20:26:04Z", "text": "(In reply to comment #4)\n> Also fwiw, I can't seem to reproduce vlad's 15% speedup here, though maybe my\n> lazy way of inlining (copy-paste, with some var decls to assign the things that\n> got passed to the function before) is responsible... but that seems like the\n> right way to inline in this case anyway, so we only compute the\n> somewhat-complicated arguments once.\n\nYeah, that's what I did.  After a bunch more runs, the win wasn't consistent, but it showed up.\n\n> One other note: we seem to compile two copies of the hot loop here; it looks\n> like something is type-unstable (a bunch of I and O types the first time\n> through; all U the second time through).\n> \n> And if I inline the ScaleMatrix/AddScaledMatrix calls (not like the latter is\n> called anyway, in my testing), and then re-roll those loops, performance goes\n> down by a factor of 2, and the number of traces compiled goes from 4 to 10\n> (presumably due to more type instability?).\n\nOne thing that I did is change the timing loop:\n\n    const start = new Date();\n    const now = new Date();\n    while ((new Date() - start) < 1000) {\n\tcalculateVerticesAndNormals(bt, v, i, output);\n\tvertices_skinned += N;\n    }\n\nto\n\n    var start = Date.now();\n    while (true) {\n\tcalculateVerticesAndNormals(bt, v, i, output);\n\tvertices_skinned += N;\n\n\tif ((vertices_skinned % 100000) == 0 &&\n\t    (Date.now() - start) >= 1000)\n\t{\n\t    break;\n\t}\n    }\n\nto avoid constant Date object creation.  It made a difference, but not a huge one; it made me feel better, though!  (Also the testcase has some odd const usage; replacing most of those with var didn't change the results, but I think we largely ignore const for this?)", "id": 5171920, "is_private": false, "creator": "vladimir@pobox.com", "creation_time": "2011-01-03T20:26:04Z"}, {"raw_text": "(In reply to comment #0)\n> \n> <vlad>    are we really guarding that these things are still functions?\n\nSounds like bug 606892 would help?\n\nAnd I bet type inference (bug 608741) will help a *lot* with this program.", "count": 6, "author": "n.nethercote@gmail.com", "tags": [], "bug_id": 622494, "attachment_id": null, "time": "2011-01-04T00:09:22Z", "id": 5172499, "text": "(In reply to comment #0)\n> \n> <vlad>    are we really guarding that these things are still functions?\n\nSounds like bug 606892 would help?\n\nAnd I bet type inference (bug 608741) will help a *lot* with this program.", "is_private": false, "creator": "n.nethercote@gmail.com", "creation_time": "2011-01-04T00:09:22Z"}, {"count": 7, "is_private": false, "time": "2011-01-04T00:20:49Z", "raw_text": "Yeah, the date thing isn't too bad, esp on non-Windows.  And yes, const is pretty much ignored at runtime for optimization purposes.\n\nMarking dependent on the obvious bugs that can help here.  Chances are, it'll be a few months before we get far enough on type inference to see an effect here, right?", "text": "Yeah, the date thing isn't too bad, esp on non-Windows.  And yes, const is pretty much ignored at runtime for optimization purposes.\n\nMarking dependent on the obvious bugs that can help here.  Chances are, it'll be a few months before we get far enough on type inference to see an effect here, right?", "id": 5172530, "creation_time": "2011-01-04T00:20:49Z", "bug_id": 622494, "attachment_id": null, "author": "bzbarsky@mit.edu", "creator": "bzbarsky@mit.edu", "tags": []}, {"is_private": false, "time": "2011-01-04T00:26:53Z", "text": "(In reply to comment #7)\n> \n> Marking dependent on the obvious bugs that can help here.  Chances are, it'll\n> be a few months before we get far enough on type inference to see an effect\n> here, right?\n\nYep.\n\nType inference will cannibalize some of LICM's potential benefits too.  Eg. LICM often can hoist the \"is this an array?\" test that currently occur on every array access;  type inference will typically eliminate that test outright.  IMO, type inference is crucial if JS is to have a chance of getting remotely close to C++ on computationally-intensive programs like this.", "id": 5172546, "creation_time": "2011-01-04T00:26:53Z", "creator": "n.nethercote@gmail.com", "count": 8, "raw_text": "(In reply to comment #7)\n> \n> Marking dependent on the obvious bugs that can help here.  Chances are, it'll\n> be a few months before we get far enough on type inference to see an effect\n> here, right?\n\nYep.\n\nType inference will cannibalize some of LICM's potential benefits too.  Eg. LICM often can hoist the \"is this an array?\" test that currently occur on every array access;  type inference will typically eliminate that test outright.  IMO, type inference is crucial if JS is to have a chance of getting remotely close to C++ on computationally-intensive programs like this.", "bug_id": 622494, "attachment_id": null, "author": "n.nethercote@gmail.com", "tags": []}, {"is_private": false, "id": 5172630, "text": "I'm not as convinced, but maybe my methodology is flawed.  I went in and stubbed out guardClass to be a no-op -- it only gave back a few precent of perf.  Might even have been noise.  Given that I don't see any side exits other than the normal loop termination, it should be safe to stub out -all- guards here and see what the perf is like, right?  I suspect we'll still be a long ways off.", "time": "2011-01-04T01:04:17Z", "creator": "vladimir@pobox.com", "creation_time": "2011-01-04T01:04:17Z", "raw_text": "I'm not as convinced, but maybe my methodology is flawed.  I went in and stubbed out guardClass to be a no-op -- it only gave back a few precent of perf.  Might even have been noise.  Given that I don't see any side exits other than the normal loop termination, it should be safe to stub out -all- guards here and see what the perf is like, right?  I suspect we'll still be a long ways off.", "count": 9, "tags": [], "author": "vladimir@pobox.com", "attachment_id": null, "bug_id": 622494}, {"count": 10, "raw_text": "I wonder how we're doing on this now that type inference is enabled.", "bug_id": 622494, "attachment_id": null, "author": "n.nethercote@gmail.com", "tags": [], "is_private": false, "time": "2011-09-29T16:22:43Z", "text": "I wonder how we're doing on this now that type inference is enabled.", "id": 5746449, "creation_time": "2011-09-29T16:22:43Z", "creator": "n.nethercote@gmail.com"}, {"creator": "azakai@mozilla.com", "creation_time": "2013-09-22T16:24:10Z", "time": "2013-09-22T16:24:10Z", "id": 7887806, "text": "Almost 2 years passed here. The non-sse version of this benchmark is tested on\n\nhttp://arewefastyet.com/#machine=12&view=breakdown&suite=asmjs-ubench\n\nand it runs at about half the speed of native with odin. Not quite native yet, but getting close.", "is_private": false, "author": "azakai@mozilla.com", "tags": [], "bug_id": 622494, "attachment_id": null, "raw_text": "Almost 2 years passed here. The non-sse version of this benchmark is tested on\n\nhttp://arewefastyet.com/#machine=12&view=breakdown&suite=asmjs-ubench\n\nand it runs at about half the speed of native with odin. Not quite native yet, but getting close.", "count": 11}]}}}