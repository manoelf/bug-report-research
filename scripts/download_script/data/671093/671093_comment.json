{"comments": {}, "bugs": {"671093": {"comments": [{"text": "jst mentioned this to me earlier today.\n\nThe main use-case is for javascript libraries.  A site could say \"I want the script with this SHA-256 hash, and if you can't find it in your cache, here's where you can download it from.\"\n\nHere are my initial thoughts about this:\n\n* I think we'd probably want sites to opt in to sharing a script.  Otherwise, evil.com might be able to tell if I've visited bank.com recently by asking for a script whose hash matches one on bank.com.\n\nThis does limit the immediate usefulness of this feature, since I can't specify a hash on my one site and expect to go faster due to scripts downloaded from other sites, but I don't see a way around this.\n\n* You'd want the opt-in mechanism to be something more flexible than \"add a same-origin script and specify its hash\", because some scripts (e.g. Google Analytics) are rarely or never retrieved from a same-origin page.  I'm not sure what's the right way to do this.\n\n* It's not clear if this would be useful for resources other than scripts.\n\n* I don't think we should hard-code in a specific hash algorithm.  Some years ago, we would have chosen MD5 or SHA-1, but neither of those is a secure choice now.  Sites should have to explicitly state what algorithm they used, and we should reserve the right to deprecate (and resultantly ignore) algorithms in the future.\n\n* We should also keep the list of accepted algorithms short (length 1?), since the more algorithms we accept, the more likely it is that two sites will specify the same resource but use different hash algorithms, causing an unnecessary miss.\n\nJonas, Chris, and Brendan, jst mentioned you'd already thought about this some.  What are your ideas?", "bug_id": 671093, "tags": [], "raw_text": "jst mentioned this to me earlier today.\n\nThe main use-case is for javascript libraries.  A site could say \"I want the script with this SHA-256 hash, and if you can't find it in your cache, here's where you can download it from.\"\n\nHere are my initial thoughts about this:\n\n* I think we'd probably want sites to opt in to sharing a script.  Otherwise, evil.com might be able to tell if I've visited bank.com recently by asking for a script whose hash matches one on bank.com.\n\nThis does limit the immediate usefulness of this feature, since I can't specify a hash on my one site and expect to go faster due to scripts downloaded from other sites, but I don't see a way around this.\n\n* You'd want the opt-in mechanism to be something more flexible than \"add a same-origin script and specify its hash\", because some scripts (e.g. Google Analytics) are rarely or never retrieved from a same-origin page.  I'm not sure what's the right way to do this.\n\n* It's not clear if this would be useful for resources other than scripts.\n\n* I don't think we should hard-code in a specific hash algorithm.  Some years ago, we would have chosen MD5 or SHA-1, but neither of those is a secure choice now.  Sites should have to explicitly state what algorithm they used, and we should reserve the right to deprecate (and resultantly ignore) algorithms in the future.\n\n* We should also keep the list of accepted algorithms short (length 1?), since the more algorithms we accept, the more likely it is that two sites will specify the same resource but use different hash algorithms, causing an unnecessary miss.\n\nJonas, Chris, and Brendan, jst mentioned you'd already thought about this some.  What are your ideas?", "attachment_id": null, "author": "justin.lebar+bug@gmail.com", "creator": "justin.lebar+bug@gmail.com", "creation_time": "2011-07-12T21:51:39Z", "time": "2011-07-12T21:51:39Z", "is_private": false, "count": 0, "id": 5587707}, {"is_private": false, "id": 5587747, "count": 1, "author": "jonas@sicking.cc", "creator": "jonas@sicking.cc", "time": "2011-07-12T22:05:46Z", "creation_time": "2011-07-12T22:05:46Z", "raw_text": "In addition to the concerns you've raised, I'm concerned about cache poisoning. I.e. evil.com crafting a script with the same hash as one used by a specificc version jQuery.\n\nHash collisions has happened in the past, though I think only when the attacker has been able to control both pieces of content. And possibly only on weaker algorithms.", "attachment_id": null, "bug_id": 671093, "text": "In addition to the concerns you've raised, I'm concerned about cache poisoning. I.e. evil.com crafting a script with the same hash as one used by a specificc version jQuery.\n\nHash collisions has happened in the past, though I think only when the attacker has been able to control both pieces of content. And possibly only on weaker algorithms.", "tags": []}, {"raw_text": "(In reply to comment #1)\n> In addition to the concerns you've raised, I'm concerned about cache\n> poisoning. I.e. evil.com crafting a script with the same hash as one used by\n> a specificc version jQuery.\n> \n> Hash collisions has happened in the past, though I think only when the\n> attacker has been able to control both pieces of content. And possibly only\n> on weaker algorithms.\n\nI think we'll just have to pick a strong hash algorithm.  If you can generate collisions on secure hash functions, I think you can forge signatures on SSL certs, so we're kind of screwed anyway.\n\nIn the past, there's been plenty of warning before a collision is found in a hash algorithm.  If we were to pick SHA-256, I imagine we'd be safe for at least a few years after SHA-3 is done.", "attachment_id": null, "tags": [], "bug_id": 671093, "text": "(In reply to comment #1)\n> In addition to the concerns you've raised, I'm concerned about cache\n> poisoning. I.e. evil.com crafting a script with the same hash as one used by\n> a specificc version jQuery.\n> \n> Hash collisions has happened in the past, though I think only when the\n> attacker has been able to control both pieces of content. And possibly only\n> on weaker algorithms.\n\nI think we'll just have to pick a strong hash algorithm.  If you can generate collisions on secure hash functions, I think you can forge signatures on SSL certs, so we're kind of screwed anyway.\n\nIn the past, there's been plenty of warning before a collision is found in a hash algorithm.  If we were to pick SHA-256, I imagine we'd be safe for at least a few years after SHA-3 is done.", "is_private": false, "count": 2, "id": 5587802, "author": "justin.lebar+bug@gmail.com", "creator": "justin.lebar+bug@gmail.com", "creation_time": "2011-07-12T22:26:11Z", "time": "2011-07-12T22:26:11Z"}, {"tags": [], "text": "cc'ing security folks to get their thoughts on this.", "bug_id": 671093, "attachment_id": null, "raw_text": "cc'ing security folks to get their thoughts on this.", "creator": "justin.lebar+bug@gmail.com", "creation_time": "2011-07-15T14:07:14Z", "time": "2011-07-15T14:07:14Z", "author": "justin.lebar+bug@gmail.com", "is_private": false, "id": 5593407, "count": 3}, {"is_private": false, "count": 4, "id": 5594105, "author": "jruderman@gmail.com", "creator": "jruderman@gmail.com", "creation_time": "2011-07-15T20:05:57Z", "time": "2011-07-15T20:05:57Z", "raw_text": "> * I think we'd probably want sites to opt in to sharing a script. \n> Otherwise, evil.com might be able to tell if I've visited bank.com recently\n> by asking for a script whose hash matches one on bank.com.\n\nThere are plenty of ways an attacker can use the cache to find out whether you've visited bank.com recently. (Maybe we'll solve those eventually, I don't know.)\n\nWhat's different here is that the attacker can query based on the *contents* of the resource, not just whether it is cached.  I guess that's a problem with link fingerprints (bug 292481) too!\n\n> * You'd want the opt-in mechanism to be something more flexible than \"add a\n> same-origin script and specify its hash\", because some scripts (e.g. Google\n> Analytics) are rarely or never retrieved from a same-origin page.  I'm not\n> sure what's the right way to do this.\n\nPerhaps \"Cache-control: public\" or \"Cache-control: public hash\".\n\nAnother possibility is to use a special URL scheme. Then a site \"opts in\" by responding with 200 OK and content that matches the hash.  And we don't need a new HTML attribute to specify the hash, because it's part of the URL.\n\nhttp://example.com/.well-known/sha-256/da39a3ee5e6b4b0d3255bfef95601890afd80709\n\nWhen we request such a URL, we could omit our Accept and Accept-Language headers, saving a few bytes.\n\nCertain headers, such as content-type and content-disposition, would be considered part of the \"content\" that gets hashed.\n\n> * It's not clear if this would be useful for resources other than scripts.\n\nI could imagine it being used for fonts.  Fonts are referenced from CSS, rather than HTML elements you can stick an attribute on, so they would be happy with the hash-in-URL scheme.", "attachment_id": null, "bug_id": 671093, "text": "> * I think we'd probably want sites to opt in to sharing a script. \n> Otherwise, evil.com might be able to tell if I've visited bank.com recently\n> by asking for a script whose hash matches one on bank.com.\n\nThere are plenty of ways an attacker can use the cache to find out whether you've visited bank.com recently. (Maybe we'll solve those eventually, I don't know.)\n\nWhat's different here is that the attacker can query based on the *contents* of the resource, not just whether it is cached.  I guess that's a problem with link fingerprints (bug 292481) too!\n\n> * You'd want the opt-in mechanism to be something more flexible than \"add a\n> same-origin script and specify its hash\", because some scripts (e.g. Google\n> Analytics) are rarely or never retrieved from a same-origin page.  I'm not\n> sure what's the right way to do this.\n\nPerhaps \"Cache-control: public\" or \"Cache-control: public hash\".\n\nAnother possibility is to use a special URL scheme. Then a site \"opts in\" by responding with 200 OK and content that matches the hash.  And we don't need a new HTML attribute to specify the hash, because it's part of the URL.\n\nhttp://example.com/.well-known/sha-256/da39a3ee5e6b4b0d3255bfef95601890afd80709\n\nWhen we request such a URL, we could omit our Accept and Accept-Language headers, saving a few bytes.\n\nCertain headers, such as content-type and content-disposition, would be considered part of the \"content\" that gets hashed.\n\n> * It's not clear if this would be useful for resources other than scripts.\n\nI could imagine it being used for fonts.  Fonts are referenced from CSS, rather than HTML elements you can stick an attribute on, so they would be happy with the hash-in-URL scheme.", "tags": []}, {"raw_text": "(In reply to comment #4)\n> What's different here is that the attacker can query based on the *contents*\n> of the resource, not just whether it is cached.  I guess that's a problem\n> with link fingerprints (bug 292481) too!\n\nYes, this is the main reason that it must be opted into by the hoster of the file.\n\n> http://example.com/.well-known/sha-256/da39a3ee5e6b4b0d3255bfef95601890afd80709\n\nI think security people would like this more than real web content authors. I imagine content authors would like to be able to tell this is JQuery from the URL.\n\n> I could imagine it being used for fonts.  Fonts are referenced from CSS,\n> rather than HTML elements you can stick an attribute on, so they would be\n> happy with the hash-in-URL scheme.\n\nAlso images like the RSS icon.", "attachment_id": null, "tags": [], "text": "(In reply to comment #4)\n> What's different here is that the attacker can query based on the *contents*\n> of the resource, not just whether it is cached.  I guess that's a problem\n> with link fingerprints (bug 292481) too!\n\nYes, this is the main reason that it must be opted into by the hoster of the file.\n\n> http://example.com/.well-known/sha-256/da39a3ee5e6b4b0d3255bfef95601890afd80709\n\nI think security people would like this more than real web content authors. I imagine content authors would like to be able to tell this is JQuery from the URL.\n\n> I could imagine it being used for fonts.  Fonts are referenced from CSS,\n> rather than HTML elements you can stick an attribute on, so they would be\n> happy with the hash-in-URL scheme.\n\nAlso images like the RSS icon.", "bug_id": 671093, "is_private": false, "count": 5, "id": 5594272, "author": "brian@briansmith.org", "creator": "brian@briansmith.org", "time": "2011-07-15T21:24:21Z", "creation_time": "2011-07-15T21:24:21Z"}, {"attachment_id": null, "raw_text": "> http://example.com/.well-known/sha-256/da39a3ee5e6b4b0d3255bfef95601890afd80709\n\nIt also seems easier to modify Apache to send a new header than to get it to understand this scheme.", "tags": [], "text": "> http://example.com/.well-known/sha-256/da39a3ee5e6b4b0d3255bfef95601890afd80709\n\nIt also seems easier to modify Apache to send a new header than to get it to understand this scheme.", "bug_id": 671093, "id": 5594280, "count": 6, "is_private": false, "creation_time": "2011-07-15T21:28:18Z", "time": "2011-07-15T21:28:18Z", "creator": "justin.lebar+bug@gmail.com", "author": "justin.lebar+bug@gmail.com"}, {"author": "mozilla@elie.im", "creator": "mozilla@elie.im", "creation_time": "2011-07-15T21:48:38Z", "time": "2011-07-15T21:48:38Z", "is_private": false, "count": 7, "id": 5594320, "tags": [], "bug_id": 671093, "text": "With Sid Stamm and Ben Adida we already started to work in this direction and have a short description of the feature here : https://wiki.mozilla.org/Security/Features/Content_Hashing\n\nIf the project can get more traction that will be great :)", "raw_text": "With Sid Stamm and Ben Adida we already started to work in this direction and have a short description of the feature here : https://wiki.mozilla.org/Security/Features/Content_Hashing\n\nIf the project can get more traction that will be great :)", "attachment_id": null}, {"is_private": false, "id": 5594378, "count": 8, "author": "justin.lebar+bug@gmail.com", "creator": "justin.lebar+bug@gmail.com", "creation_time": "2011-07-15T22:23:26Z", "time": "2011-07-15T22:23:26Z", "raw_text": "The feature page currently has two use-cases.  The first is the one we all agree on, for speeding up downloads of your favorite JS library.  The second is: \n\n> Untrusted CDN - a site may use a content distribution network (CDN) to serve \n> the majority of their site (images, stylesheets, scripts) but may not want to \n> rely on them to serve the right files. The site can serve the document and \n> reference CDN-hosted content with hashed references to provide an integrity \n> check.\n\nI'm curious how important we think this is, in comparison to the first use case.  There's likely to be at least some complexity involved in supporting it, since we'd have to leave the downloaded content unused until we finish downloading it and can compute its hash.  Normally we try to use bits we've downloaded as soon as they arrive (for instance, we'll incrementally display an image).", "attachment_id": null, "tags": [], "bug_id": 671093, "text": "The feature page currently has two use-cases.  The first is the one we all agree on, for speeding up downloads of your favorite JS library.  The second is: \n\n> Untrusted CDN - a site may use a content distribution network (CDN) to serve \n> the majority of their site (images, stylesheets, scripts) but may not want to \n> rely on them to serve the right files. The site can serve the document and \n> reference CDN-hosted content with hashed references to provide an integrity \n> check.\n\nI'm curious how important we think this is, in comparison to the first use case.  There's likely to be at least some complexity involved in supporting it, since we'd have to leave the downloaded content unused until we finish downloading it and can compute its hash.  Normally we try to use bits we've downloaded as soon as they arrive (for instance, we'll incrementally display an image)."}, {"bug_id": 671093, "text": "For caching what I think a key problem is that if it is okay when a hash is specified to re-use the data that was cached on another website that have the same hash and dis regard the same origin policy ? This can potentially lead to privacy leak.\n\nThe untrusted CDN case is a very important: it will allows website such as eBay to defeat image swapping attack (the image is replaced by another after the approval) . Gravatar already use some sort of hashing to fight this kind of problem.\n\nThis is also important to fight malware distribution via Ads network because it will allows ads company  to \"freeze\" the content of the ads they are serving while leaving their clients in control", "tags": [], "attachment_id": null, "raw_text": "For caching what I think a key problem is that if it is okay when a hash is specified to re-use the data that was cached on another website that have the same hash and dis regard the same origin policy ? This can potentially lead to privacy leak.\n\nThe untrusted CDN case is a very important: it will allows website such as eBay to defeat image swapping attack (the image is replaced by another after the approval) . Gravatar already use some sort of hashing to fight this kind of problem.\n\nThis is also important to fight malware distribution via Ads network because it will allows ads company  to \"freeze\" the content of the ads they are serving while leaving their clients in control", "creator": "mozilla@elie.im", "time": "2011-07-15T22:36:43Z", "creation_time": "2011-07-15T22:36:43Z", "author": "mozilla@elie.im", "is_private": false, "count": 9, "id": 5594402}, {"count": 10, "id": 5594802, "is_private": false, "creation_time": "2011-07-16T11:01:48Z", "time": "2011-07-16T11:01:48Z", "creator": "jruderman@gmail.com", "author": "jruderman@gmail.com", "attachment_id": null, "raw_text": "The \"second use case\" is similar to bug 292481.  I think it's more important, since it would allow sites like mozilla.com to make software downloads secure.  The other one just lets sites make things faster ;)", "text": "The \"second use case\" is similar to bug 292481.  I think it's more important, since it would allow sites like mozilla.com to make software downloads secure.  The other one just lets sites make things faster ;)", "bug_id": 671093, "tags": []}, {"creator": "mozilla@elie.im", "creation_time": "2011-09-03T07:21:25Z", "time": "2011-09-03T07:21:25Z", "author": "mozilla@elie.im", "is_private": false, "id": 5698112, "count": 11, "bug_id": 671093, "text": "After digging around it seems that we have a bug duplicate. Bug 292481 - (link-fingerprints) Support link fingerprints for downloads (file checksum/hash in href attribute) \n\nIs someone able to merge the two ?  \n\nA good news on our side: we have a working prototype of Firefox which support hash as an attribute :) We will run performance tests in the upcoming weeks", "tags": [], "attachment_id": null, "raw_text": "After digging around it seems that we have a bug duplicate. Bug 292481 - (link-fingerprints) Support link fingerprints for downloads (file checksum/hash in href attribute) \n\nIs someone able to merge the two ?  \n\nA good news on our side: we have a working prototype of Firefox which support hash as an attribute :) We will run performance tests in the upcoming weeks"}, {"text": "This may be the key to finally getting Amazon.com, etc to \nprovide trustable security against firesheep, etc. - \n\nIF this can be used to validate mixed content as secure - \n\nIf ALL images, js, etc loaded from http://images.amazon.com \nhave been validated by hashes on the main page from https://amazon.com,\n\n_and_ ALL cookies from https://amazon.com have the secure bit set,\n\nsupress the mixed-content warning! \n\nWe will need a new lock or ~ to indicate \"secure but NOT totally private\".\nAny ideas?\n(maybe also add a pref for a notice-bar that your privacy is not guaranteed)\n\n1. No needless encrypting of content 10% of the planet already has in their cache\n     (like all static, non-stock images on etrade.com)\n2. Easy deduplication of content from images[0-9].site.com\n3. User can choose privacy or speed - \n     If you value your privacy, use https-everywhere or ~ to get secure images.\n     If you cache secure content, \n       you still get secure CDN deduplication by the hash - AND faster cache - \n       no secure-conn setup, etc. to the CDN to check for changes!\n-or - \n     Choose speed - \n        We never need to burden a server with setting up more than 1 \n        secure connection per session - Secure pages can get static content \n        (90%+?) from server-side or CDN caches.\n        (Image/CDN Servers could even be set to prioritize http over https)\n\nWe definitely need to save the origin with the hash - \nperhaps we need something like CSP or Origin-headers that list servers that\ncan serve content for this page, cache the list, and save a link to that list\nwith the hashes - so evil.com's link to images.amazon.com never sees anything\nfetched by any amazon.com page as cached, much less hashed.\n\nBut we already need that, ever since the add-on safecache was abandoned.\n( https://addons.mozilla.org/en-US/firefox/addon/safecache/?src=search )\n\nHashes: \nUnless you are Amazon or ~ fortune 500 .com, \nsha1 is usually secure enough - for 5 years or so (set the expire).\nIf one could steal $10M from your customers w/o them noticing, use sha256.\nIf you have secret gov't contracts, or serve your IP crown jewels, \ndon't use hashes.", "bug_id": 671093, "tags": [], "attachment_id": null, "raw_text": "This may be the key to finally getting Amazon.com, etc to \nprovide trustable security against firesheep, etc. - \n\nIF this can be used to validate mixed content as secure - \n\nIf ALL images, js, etc loaded from http://images.amazon.com \nhave been validated by hashes on the main page from https://amazon.com,\n\n_and_ ALL cookies from https://amazon.com have the secure bit set,\n\nsupress the mixed-content warning! \n\nWe will need a new lock or ~ to indicate \"secure but NOT totally private\".\nAny ideas?\n(maybe also add a pref for a notice-bar that your privacy is not guaranteed)\n\n1. No needless encrypting of content 10% of the planet already has in their cache\n     (like all static, non-stock images on etrade.com)\n2. Easy deduplication of content from images[0-9].site.com\n3. User can choose privacy or speed - \n     If you value your privacy, use https-everywhere or ~ to get secure images.\n     If you cache secure content, \n       you still get secure CDN deduplication by the hash - AND faster cache - \n       no secure-conn setup, etc. to the CDN to check for changes!\n-or - \n     Choose speed - \n        We never need to burden a server with setting up more than 1 \n        secure connection per session - Secure pages can get static content \n        (90%+?) from server-side or CDN caches.\n        (Image/CDN Servers could even be set to prioritize http over https)\n\nWe definitely need to save the origin with the hash - \nperhaps we need something like CSP or Origin-headers that list servers that\ncan serve content for this page, cache the list, and save a link to that list\nwith the hashes - so evil.com's link to images.amazon.com never sees anything\nfetched by any amazon.com page as cached, much less hashed.\n\nBut we already need that, ever since the add-on safecache was abandoned.\n( https://addons.mozilla.org/en-US/firefox/addon/safecache/?src=search )\n\nHashes: \nUnless you are Amazon or ~ fortune 500 .com, \nsha1 is usually secure enough - for 5 years or so (set the expire).\nIf one could steal $10M from your customers w/o them noticing, use sha256.\nIf you have secret gov't contracts, or serve your IP crown jewels, \ndon't use hashes.", "creator": "cvevans@users.sourceforge.net", "creation_time": "2012-03-31T01:31:33Z", "time": "2012-03-31T01:31:33Z", "author": "cvevans@users.sourceforge.net", "is_private": false, "count": 12, "id": 6186869}, {"text": "https://bugzilla.mozilla.org/show_bug.cgi?id=1472046\n\nMove all DOM bugs that haven\u2019t been updated in more than 3 years and has no one currently assigned to P5.\n\nIf you have questions, please contact :mdaly.", "bug_id": 671093, "tags": [], "raw_text": "https://bugzilla.mozilla.org/show_bug.cgi?id=1472046\n\nMove all DOM bugs that haven\u2019t been updated in more than 3 years and has no one currently assigned to P5.\n\nIf you have questions, please contact :mdaly.", "attachment_id": null, "author": "bug-husbandry-bot@mozilla.bugs", "creator": "bug-husbandry-bot@mozilla.bugs", "creation_time": "2018-06-29T04:48:47Z", "time": "2018-06-29T04:48:47Z", "is_private": false, "id": 13435948, "count": 13}]}}}